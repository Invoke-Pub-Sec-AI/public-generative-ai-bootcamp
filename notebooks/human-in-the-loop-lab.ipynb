{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Human-in-the-Loop & Acceptance Tests Lab\n",
        "\n",
        "## 🎯 Lab Objective\n",
        "**Add an approval step that blocks a risky tool call; capture reason.**\n",
        "\n",
        "In this lab, you'll implement a complete human-in-the-loop system that:\n",
        "1. Detects risky tool calls\n",
        "2. Requires human approval before execution\n",
        "3. Logs all approval decisions with reasons\n",
        "4. Captures screenshots of blocked actions\n",
        "\n",
        "**Deliverable**: blocked-action screenshot + approval log\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Google Colab\n",
        "!pip install requests pandas matplotlib seaborn plotly ipywidgets\n",
        "\n",
        "# All required imports for this lab notebook\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Callable\n",
        "from dataclasses import dataclass, asdict\n",
        "from enum import Enum\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# TODO: Import required libraries\n",
        "# You will need: json, os, time, datetime, typing, dataclasses, enum, subprocess, pathlib\n",
        "# Import json for handling data\n",
        "# Import datetime for timestamps\n",
        "# Import typing for type hints (Dict, List, Any, Optional, Callable)\n",
        "# Import dataclasses for structured data (@dataclass, asdict)\n",
        "# Import enum for creating enumerations (Enum)\n",
        "# Import pathlib for file path handling (Path)\n",
        "\n",
        "# TODO: Create lab workspace directory\n",
        "# Create a Path object pointing to \"./lab_workspace\"\n",
        "# Use mkdir(exist_ok=True) to create the directory\n",
        "# Print confirmation messages\n",
        "\n",
        "# Create lab workspace\n",
        "LAB_DIR = Path(\"./lab_workspace\")\n",
        "LAB_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"🧪 Human-in-the-Loop Lab Environment Ready\")\n",
        "print(f\"📁 Lab workspace: {LAB_DIR.absolute()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Define Risky Tool Calls\n",
        "\n",
        "First, let's identify what constitutes a \"risky\" tool call that requires human approval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define a RiskLevel enum with four levels\n",
        "# Create an enum with LOW, MEDIUM, HIGH, CRITICAL values\n",
        "# Use the Enum class from the enum module\n",
        "\n",
        "# TODO: Create a ToolCall dataclass\n",
        "# Include the following fields:\n",
        "#   - name: str (name of the tool)\n",
        "#   - parameters: Dict[str, Any] (tool parameters)\n",
        "#   - risk_level: RiskLevel (assigned risk level)\n",
        "#   - confidence: float (confidence score 0.0-1.0)\n",
        "#   - description: str (human-readable description)\n",
        "\n",
        "# TODO: Create a list of example risky tool calls\n",
        "# Include examples like:\n",
        "#   - File deletion (rm, DELETE)\n",
        "#   - Database operations (DROP, ALTER)\n",
        "#   - System commands (sudo, chmod)\n",
        "#   - Network operations (wget, curl)\n",
        "# Assign different risk levels and confidence scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Implement Risk Detection\n",
        "\n",
        "Create a function that analyzes tool calls and determines their risk level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement a function to analyze tool call risk\n",
        "# Function signature: analyze_tool_call_risk(tool_call: ToolCall) -> RiskLevel\n",
        "# \n",
        "# Consider these risk factors:\n",
        "#   - Destructive commands: rm, delete, drop, truncate\n",
        "#   - System administration: sudo, chmod, chown\n",
        "#   - Network access: wget, curl, ssh\n",
        "#   - Production indicators: 'prod', 'production', 'live'\n",
        "#   - Critical data: user tables, financial data\n",
        "#   - Low confidence scores (< 0.8)\n",
        "\n",
        "# TODO: Implement a function to determine approval requirement\n",
        "# Function signature: requires_approval(tool_call: ToolCall, threshold: RiskLevel = RiskLevel.MEDIUM) -> bool\n",
        "# Return True if the tool call's risk level is >= threshold\n",
        "\n",
        "# TODO: Test your functions\n",
        "# Loop through your RISKY_TOOL_CALLS list\n",
        "# For each tool call, print: name, risk level, and approval requirement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Build Approval Interface\n",
        "\n",
        "Create an interface for human approval with proper logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create an ApprovalDecision dataclass\n",
        "# Include these fields:\n",
        "#   - tool_call_id: str (unique identifier)\n",
        "#   - approved: bool (True if approved)\n",
        "#   - reason: str (explanation for the decision)\n",
        "#   - timestamp: str (when decision was made)\n",
        "#   - approver: str (who made the decision)\n",
        "\n",
        "# TODO: Create an ApprovalLogger class\n",
        "# Initialize with:\n",
        "#   - log_file parameter (default: 'approval_log.json')\n",
        "#   - self.log_file = LAB_DIR / log_file\n",
        "#   - self.decisions = [] (empty list)\n",
        "\n",
        "# TODO: Implement log_decision method\n",
        "# Add the decision to self.decisions list\n",
        "\n",
        "# TODO: Implement save_log method\n",
        "# Convert decisions to dictionaries using asdict()\n",
        "# Save as JSON to self.log_file\n",
        "\n",
        "# TODO: Create request_human_approval function\n",
        "# Display approval request with:\n",
        "#   - Tool name and description\n",
        "#   - Risk level\n",
        "#   - Parameters (formatted as JSON)\n",
        "#   - Prompt for approval decision\n",
        "# For lab: simulate approval responses or use input()\n",
        "# Return an ApprovalDecision object\n",
        "\n",
        "# TODO: Create an ApprovalLogger instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Tool Call Executor with Blocking\n",
        "\n",
        "Create an executor that blocks risky calls and requires approval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a SafeToolExecutor class\n",
        "# Initialize with:\n",
        "#   - approval_logger: ApprovalLogger\n",
        "#   - self.logger = approval_logger\n",
        "#   - self.blocked_calls = [] (empty list)\n",
        "\n",
        "# TODO: Implement execute_tool_call method\n",
        "# Method signature: execute_tool_call(self, tool_call: ToolCall) -> Dict[str, Any]\n",
        "# Implementation flow:\n",
        "#   1. Check if approval is required using requires_approval()\n",
        "#   2. If not required, execute directly and return success\n",
        "#   3. If required, call request_human_approval()\n",
        "#   4. If approved, execute and log decision\n",
        "#   5. If denied, block and log decision\n",
        "#   6. Add blocked calls to self.blocked_calls list\n",
        "#   7. Return execution result dictionary with status and details\n",
        "\n",
        "# TODO: Implement _simulate_execution helper method\n",
        "# For lab purposes, return a simulation message instead of real execution\n",
        "# Return format: '[SIMULATED] Executed {tool_name} with {parameters}'\n",
        "\n",
        "# TODO: Implement get_blocked_calls_summary method\n",
        "# Return a dictionary with:\n",
        "#   - total_blocked: count of blocked calls\n",
        "#   - blocked_calls: list of blocked call details\n",
        "#   - summary statistics\n",
        "\n",
        "# TODO: Create a SafeToolExecutor instance using your approval logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run Lab Scenarios\n",
        "\n",
        "Test your implementation with different scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Execute all risky tool calls through your safe executor\n",
        "# Print header: 'Running Lab Scenarios...'\n",
        "\n",
        "# TODO: Loop through RISKY_TOOL_CALLS with enumerate()\n",
        "# For each tool call:\n",
        "#   1. Print scenario number and description\n",
        "#   2. Call executor.execute_tool_call(tool_call)\n",
        "#   3. Print the result status\n",
        "#   4. If there's a reason, print it\n",
        "#   5. Add a blank line for readability\n",
        "\n",
        "# TODO: Save the approval log\n",
        "# Call approval_logger.save_log()\n",
        "# Print confirmation with the log file path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Generate Deliverables\n",
        "\n",
        "Create the required deliverables: blocked-action screenshot + approval log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create generate_blocked_action_report function\n",
        "# Create a report dictionary with:\n",
        "#   - lab_session: {timestamp, student name, objective}\n",
        "#   - blocked_actions: list of blocked action details\n",
        "#   - approval_statistics: summary stats (total, approved, denied)\n",
        "#   - summary: text description of results\n",
        "\n",
        "# TODO: Populate the report with data from your executor\n",
        "# Use executor.get_blocked_calls_summary() for blocked actions\n",
        "# Use approval_logger.decisions for statistics\n",
        "\n",
        "# TODO: Save report to JSON file\n",
        "# File path: LAB_DIR / 'blocked_actions_report.json'\n",
        "# Use json.dump() with indent=2 for formatting\n",
        "\n",
        "# TODO: Create create_screenshot_simulation function\n",
        "# Create a text-based 'screenshot' showing:\n",
        "#   - ASCII art border\n",
        "#   - 'TOOL CALL BLOCKED' header\n",
        "#   - Tool details (name, risk level, reason, timestamp)\n",
        "#   - 'BLOCKED - Human approval required' status\n",
        "\n",
        "# TODO: Save screenshot to text file\n",
        "# File path: LAB_DIR / 'blocked_action_screenshot.txt'\n",
        "\n",
        "# TODO: Generate deliverables\n",
        "# Call both functions above\n",
        "# Print summary of generated files with paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Lab Validation & Reflection\n",
        "\n",
        "Validate your implementation and reflect on the experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create validate_lab_completion function\n",
        "# Return type: bool\n",
        "\n",
        "# TODO: Create validation checks list\n",
        "# Each check should be a tuple: (check_name, passed_boolean)\n",
        "# Check for:\n",
        "#   - RiskLevel enum defined\n",
        "#   - ToolCall dataclass defined\n",
        "#   - Risk analysis function implemented\n",
        "#   - Approval system created\n",
        "#   - At least one call blocked (check executor.blocked_calls)\n",
        "#   - Decisions logged (check approval_logger.decisions)\n",
        "#   - Deliverable files exist\n",
        "\n",
        "# TODO: Check if all validations passed\n",
        "# Use all() function on the boolean values\n",
        "\n",
        "# TODO: Print validation results\n",
        "# Loop through checks and print status (✅ or ❌)\n",
        "\n",
        "# TODO: Run validation and handle results\n",
        "# If passed: print congratulations and reflection questions\n",
        "# If failed: print failure message\n",
        "\n",
        "# Reflection Questions to display on success:\n",
        "# 1. What types of tool calls did you classify as high risk?\n",
        "# 2. How did you balance security with usability?\n",
        "# 3. What additional safety measures would you add in production?\n",
        "# 4. How would you handle cases where no human approver is available?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Lab Summary\n",
        "\n",
        "### What You've Built:\n",
        "- **Risk Detection System**: Automatically identifies dangerous tool calls\n",
        "- **Approval Gates**: Human-in-the-loop decision points for risky operations\n",
        "- **Comprehensive Logging**: Full audit trail of all approval decisions\n",
        "- **Safety Executor**: Tool execution engine with built-in safety checks\n",
        "\n",
        "### Key Learning Outcomes:\n",
        "1. **Risk Assessment**: How to classify and evaluate tool call danger levels\n",
        "2. **Human Oversight**: Implementing effective approval workflows\n",
        "3. **Audit Trails**: Importance of logging decisions with clear reasoning\n",
        "4. **System Safety**: Building fail-safe mechanisms into AI systems\n",
        "\n",
        "### Real-World Applications:\n",
        "- **DevOps Automation**: Preventing accidental production changes\n",
        "- **Financial Systems**: Requiring approval for large transactions\n",
        "- **Healthcare AI**: Human oversight for critical medical decisions\n",
        "- **Autonomous Vehicles**: Safety driver intervention systems\n",
        "\n",
        "---\n",
        "\n",
        "**🚀 Deliverables Checklist:**\n",
        "- [ ] Blocked-action screenshot (text simulation)\n",
        "- [ ] Approval log with decision reasons\n",
        "- [ ] Working approval gate system\n",
        "- [ ] Risk classification implementation\n",
        "\n",
        "**Great work on building a safer AI system!** 🛡️"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}