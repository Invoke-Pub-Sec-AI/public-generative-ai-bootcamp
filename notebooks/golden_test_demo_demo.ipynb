{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Golden Test Generation Demo\n",
        "\n",
        "## Learning Objectives\n",
        "- Generate comprehensive test suites using AI\n",
        "- Create \"golden\" reference tests for legacy code\n",
        "- Build automated test validation and coverage analysis\n",
        "- Design test-driven refactoring workflows\n",
        "\n",
        "## The Challenge: Testing Legacy Code\n",
        "\n",
        "Golden tests capture the current behavior of existing code as a baseline, then ensure refactored code maintains the same behavior. This demo shows how to:\n",
        "1. **Behavior Analysis** - Understanding what code actually does\n",
        "2. **Test Generation** - Creating comprehensive test coverage\n",
        "3. **Edge Case Discovery** - Finding boundary conditions and error cases\n",
        "4. **Regression Prevention** - Ensuring refactors don't break functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install asksageclient pip_system_certs pytest coverage rich tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔐 Cell 1 — Load secrets (Colab) + pricing + token utils\n",
        "# ================================\n",
        "import os, time, csv\n",
        "from typing import Optional, Dict\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "ASKSAGE_API_KEY = userdata.get(\"ASKSAGE_API_KEY\")\n",
        "ASKSAGE_BASE_URL = userdata.get(\"ASKSAGE_BASE_URL\")\n",
        "ASKSAGE_EMAIL = userdata.get(\"ASKSAGE_EMAIL\")\n",
        "\n",
        "assert ASKSAGE_API_KEY, \"ASKSAGE_API_KEY not provided.\"\n",
        "assert ASKSAGE_EMAIL, \"ASKSAGE_EMAIL not provided.\"\n",
        "\n",
        "print(\"✓ Secrets loaded\")\n",
        "print(\"  • EMAIL:\", ASKSAGE_EMAIL)\n",
        "print(\"  • BASE URL:\", ASKSAGE_BASE_URL or \"(default)\")\n",
        "\n",
        "# Pricing (USD per 1,000,000 tokens)\n",
        "PRICES_PER_M = {\n",
        "    \"gpt-5\": {\"input_per_m\": 1.25, \"output_per_m\": 10.00},\n",
        "    \"gpt-5-mini\": {\"input_per_m\": 0.25, \"output_per_m\": 2.00},\n",
        "}\n",
        "\n",
        "# Tokenizer\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text or \"\"))\n",
        "\n",
        "def cost_usd(model: str, input_tokens: int, output_tokens: int) -> float:\n",
        "    if model not in PRICES_PER_M:\n",
        "        raise ValueError(f\"Unknown model: {model}\")\n",
        "    r = PRICES_PER_M[model]\n",
        "    return (input_tokens / 1_000_000) * r[\"input_per_m\"] + (output_tokens / 1_000_000) * r[\"output_per_m\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔧 Cell 2 — Import bootcamp_common and setup AskSage client\n",
        "# ================================\n",
        "import sys\n",
        "sys.path.append('../../../')  # Adjust path to reach bootcamp_common\n",
        "\n",
        "from bootcamp_common.ask_sage import AskSageClient\n",
        "\n",
        "# Initialize AskSage client\n",
        "client = AskSageClient(\n",
        "    api_key=ASKSAGE_API_KEY,\n",
        "    base_url=ASKSAGE_BASE_URL\n",
        ")\n",
        "\n",
        "print(\"✓ AskSage client initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import inspect\n",
        "import subprocess\n",
        "from typing import Dict, List, Optional, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import openai\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.syntax import Syntax\n",
        "from rich.table import Table\n",
        "from rich.progress import Progress, SpinnerColumn, TextColumn\n",
        "\n",
        "console = Console()\n",
        "print(\"🧪 Golden test generator loading...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Target Code: Email Validator Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example function that needs comprehensive testing\n",
        "target_code = '''\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "def validate_email_list(emails: List[str], strict_mode: bool = False) -> Dict[str, any]:\n",
        "    \"\"\"Validate a list of email addresses and return detailed results.\"\"\"\n",
        "    \n",
        "    valid_emails = []\n",
        "    invalid_emails = []\n",
        "    warnings = []\n",
        "    \n",
        "    # Basic email regex (simplified)\n",
        "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "    \n",
        "    for email in emails:\n",
        "        if not email or not isinstance(email, str):\n",
        "            invalid_emails.append({\"email\": email, \"reason\": \"Invalid type or empty\"})\n",
        "            continue\n",
        "            \n",
        "        email = email.strip().lower()\n",
        "        \n",
        "        if len(email) > 254:  # RFC 5321 limit\n",
        "            invalid_emails.append({\"email\": email, \"reason\": \"Too long\"})\n",
        "            continue\n",
        "            \n",
        "        if re.match(email_pattern, email):\n",
        "            # Additional checks in strict mode\n",
        "            if strict_mode:\n",
        "                local, domain = email.split('@')\n",
        "                \n",
        "                # Check local part length\n",
        "                if len(local) > 64:\n",
        "                    invalid_emails.append({\"email\": email, \"reason\": \"Local part too long\"})\n",
        "                    continue\n",
        "                \n",
        "                # Warn about suspicious patterns\n",
        "                if '..' in email or email.startswith('.') or email.endswith('.'):\n",
        "                    warnings.append({\"email\": email, \"warning\": \"Suspicious dot pattern\"})\n",
        "                \n",
        "                # Check for disposable email domains\n",
        "                disposable_domains = ['10minutemail.com', 'tempmail.org', 'guerrillamail.com']\n",
        "                if any(domain.endswith(d) for d in disposable_domains):\n",
        "                    warnings.append({\"email\": email, \"warning\": \"Disposable email domain\"})\n",
        "            \n",
        "            valid_emails.append(email)\n",
        "        else:\n",
        "            invalid_emails.append({\"email\": email, \"reason\": \"Invalid format\"})\n",
        "    \n",
        "    return {\n",
        "        \"valid_count\": len(valid_emails),\n",
        "        \"invalid_count\": len(invalid_emails),\n",
        "        \"warning_count\": len(warnings),\n",
        "        \"valid_emails\": valid_emails,\n",
        "        \"invalid_emails\": invalid_emails,\n",
        "        \"warnings\": warnings,\n",
        "        \"success_rate\": len(valid_emails) / len(emails) if emails else 0\n",
        "    }\n",
        "'''\n",
        "\n",
        "# Execute the code to make function available\n",
        "exec(target_code)\n",
        "\n",
        "console.print(\"📧 [bold blue]Target Function Loaded[/bold blue]\")\n",
        "syntax = Syntax(target_code, \"python\", theme=\"monokai\", line_numbers=True)\n",
        "console.print(Panel(syntax, title=\"validate_email_list function\", border_style=\"blue\"))\n",
        "print(\"\\n🎯 Ready to generate comprehensive golden tests!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Golden Test Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TestCase:\n",
        "    \"\"\"Individual test case with inputs and expected outputs\"\"\"\n",
        "    name: str\n",
        "    inputs: Dict[str, Any]\n",
        "    expected_output: Any\n",
        "    test_type: str  # 'happy_path', 'edge_case', 'error_case'\n",
        "    description: str\n",
        "\n",
        "class GoldenTestGenerator:\n",
        "    \"\"\"Generate comprehensive test suites for existing functions\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.setup_client()\n",
        "        self.generated_tests = {}\n",
        "    \n",
        "    def setup_client(self):\n",
        "        \"\"\"Setup API client\"\"\"\n",
        "        if os.getenv('OPENAI_API_KEY'):\n",
        "            try:\n",
        "                self.client = openai.OpenAI()\n",
        "                self.has_api = True\n",
        "                console.print(\"✅ OpenAI client configured\")\n",
        "            except Exception as e:\n",
        "                self.has_api = False\n",
        "                console.print(f\"⚠️ Using mock responses: {e}\")\n",
        "        else:\n",
        "            self.has_api = False\n",
        "            console.print(\"💡 No API key found, using mock responses\")\n",
        "    \n",
        "    def analyze_function_behavior(self, code: str, function_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze function to understand its behavior and edge cases\"\"\"\n",
        "        \n",
        "        analysis_prompt = f\"\"\"Analyze this Python function to understand its behavior:\n",
        "\n",
        "{code}\n",
        "\n",
        "For function '{function_name}', identify:\n",
        "1. Input parameters and their types/constraints\n",
        "2. Return value structure and possible values\n",
        "3. Happy path scenarios (normal usage)\n",
        "4. Edge cases (boundary conditions, unusual inputs)\n",
        "5. Error conditions (invalid inputs, exceptions)\n",
        "6. Side effects or special behaviors\n",
        "\n",
        "Provide specific test scenarios for comprehensive coverage.\"\"\"\n",
        "        \n",
        "        if self.has_api:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "                max_tokens=800,\n",
        "                temperature=0.3\n",
        "            )\n",
        "            analysis = response.choices[0].message.content\n",
        "        else:\n",
        "            # Mock analysis\n",
        "            analysis = \"\"\"Function Analysis:\n",
        "Input: emails (List[str]), strict_mode (bool, default False)\n",
        "Output: Dict with validation results\n",
        "\n",
        "Happy Path:\n",
        "- List of valid email addresses\n",
        "- Mix of valid/invalid emails\n",
        "- Strict mode enabled/disabled\n",
        "\n",
        "Edge Cases:\n",
        "- Empty list\n",
        "- Single email\n",
        "- Very long emails (>254 chars)\n",
        "- Emails with edge case formats\n",
        "- Non-string inputs\n",
        "\n",
        "Error Conditions:\n",
        "- None/null inputs\n",
        "- Invalid data types\n",
        "- Malformed email patterns\"\"\"\n",
        "        \n",
        "        return {\"analysis\": analysis}\n",
        "    \n",
        "    def generate_test_cases(self, code: str, function_name: str) -> List[TestCase]:\n",
        "        \"\"\"Generate comprehensive test cases based on function analysis\"\"\"\n",
        "        \n",
        "        console.print(f\"[yellow]Generating test cases for {function_name}...[/yellow]\")\n",
        "        \n",
        "        # Get actual function for testing\n",
        "        func = globals().get(function_name)\n",
        "        if not func:\n",
        "            console.print(f\"[red]Function {function_name} not found![/red]\")\n",
        "            return []\n",
        "        \n",
        "        test_cases = []\n",
        "        \n",
        "        # Happy path tests\n",
        "        happy_path_inputs = [\n",
        "            {\"emails\": [\"user@example.com\", \"test@domain.org\"], \"strict_mode\": False},\n",
        "            {\"emails\": [\"valid@email.com\"], \"strict_mode\": True},\n",
        "            {\"emails\": [\"good@test.co.uk\", \"also@valid.net\", \"another@site.info\"], \"strict_mode\": False}\n",
        "        ]\n",
        "        \n",
        "        for i, inputs in enumerate(happy_path_inputs):\n",
        "            try:\n",
        "                expected = func(**inputs)\n",
        "                test_cases.append(TestCase(\n",
        "                    name=f\"test_happy_path_{i+1}\",\n",
        "                    inputs=inputs,\n",
        "                    expected_output=expected,\n",
        "                    test_type=\"happy_path\",\n",
        "                    description=f\"Normal usage with {len(inputs['emails'])} emails\"\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                console.print(f\"[red]Error in happy path test {i}: {e}[/red]\")\n",
        "        \n",
        "        # Edge case tests\n",
        "        edge_cases = [\n",
        "            {\"emails\": [], \"strict_mode\": False},  # Empty list\n",
        "            {\"emails\": [\"user@example.com\"], \"strict_mode\": False},  # Single email\n",
        "            {\"emails\": [\"a\" * 250 + \"@example.com\"], \"strict_mode\": False},  # Long email\n",
        "            {\"emails\": [\"test@10minutemail.com\"], \"strict_mode\": True},  # Disposable email\n",
        "            {\"emails\": [\".user@example.com\", \"user@example.\"], \"strict_mode\": True},  # Dot patterns\n",
        "        ]\n",
        "        \n",
        "        for i, inputs in enumerate(edge_cases):\n",
        "            try:\n",
        "                expected = func(**inputs)\n",
        "                test_cases.append(TestCase(\n",
        "                    name=f\"test_edge_case_{i+1}\",\n",
        "                    inputs=inputs,\n",
        "                    expected_output=expected,\n",
        "                    test_type=\"edge_case\",\n",
        "                    description=f\"Edge case: {list(inputs.keys())}\"\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                console.print(f\"[red]Error in edge case test {i}: {e}[/red]\")\n",
        "        \n",
        "        # Error case tests  \n",
        "        error_cases = [\n",
        "            {\"emails\": [None, \"\", 123], \"strict_mode\": False},  # Invalid types\n",
        "            {\"emails\": [\"invalid-email\", \"@domain.com\", \"user@\"], \"strict_mode\": False},  # Malformed\n",
        "        ]\n",
        "        \n",
        "        for i, inputs in enumerate(error_cases):\n",
        "            try:\n",
        "                expected = func(**inputs)\n",
        "                test_cases.append(TestCase(\n",
        "                    name=f\"test_error_case_{i+1}\",\n",
        "                    inputs=inputs,\n",
        "                    expected_output=expected,\n",
        "                    test_type=\"error_case\",\n",
        "                    description=f\"Error handling: invalid inputs\"\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                # For functions that should handle errors gracefully\n",
        "                console.print(f\"[yellow]Function raised exception for error case {i}: {e}[/yellow]\")\n",
        "        \n",
        "        return test_cases\n",
        "    \n",
        "    def generate_pytest_code(self, test_cases: List[TestCase], function_name: str) -> str:\n",
        "        \"\"\"Generate pytest code from test cases\"\"\"\n",
        "        \n",
        "        pytest_code = f'''import pytest\n",
        "from your_module import {function_name}\n",
        "\n",
        "class Test{function_name.title()}:\n",
        "    \"\"\"Comprehensive test suite for {function_name} function.\"\"\"\n",
        "    \n",
        "'''\n",
        "        \n",
        "        for test_case in test_cases:\n",
        "            pytest_code += f'''    def {test_case.name}(self):\n",
        "        \"\"\"Test: {test_case.description}\"\"\"\n",
        "        inputs = {test_case.inputs}\n",
        "        expected = {test_case.expected_output}\n",
        "        \n",
        "        result = {function_name}(**inputs)\n",
        "        assert result == expected\n",
        "        \n",
        "        # Additional assertions for key metrics\n",
        "        assert 'valid_count' in result\n",
        "        assert 'invalid_count' in result\n",
        "        assert result['valid_count'] + result['invalid_count'] == len(inputs['emails'])\n",
        "\n",
        "'''\n",
        "        \n",
        "        return pytest_code\n",
        "    \n",
        "    def create_golden_test_suite(self, code: str, function_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Create complete golden test suite\"\"\"\n",
        "        \n",
        "        with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\")) as progress:\n",
        "            task = progress.add_task(\"Creating golden test suite...\", total=None)\n",
        "            \n",
        "            # Analyze function\n",
        "            analysis = self.analyze_function_behavior(code, function_name)\n",
        "            \n",
        "            # Generate test cases\n",
        "            test_cases = self.generate_test_cases(code, function_name)\n",
        "            \n",
        "            # Generate pytest code\n",
        "            pytest_code = self.generate_pytest_code(test_cases, function_name)\n",
        "            \n",
        "            progress.update(task, completed=100)\n",
        "        \n",
        "        return {\n",
        "            \"analysis\": analysis,\n",
        "            \"test_cases\": test_cases,\n",
        "            \"pytest_code\": pytest_code,\n",
        "            \"coverage_estimate\": len(test_cases)\n",
        "        }\n",
        "\n",
        "# Initialize generator\n",
        "golden_test_gen = GoldenTestGenerator()\n",
        "print(\"🧪 Golden test generator ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo: Generate Golden Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive test suite\n",
        "console.print(\"\\n🧪 [bold blue]Generating Golden Test Suite[/bold blue]\")\n",
        "\n",
        "test_suite = golden_test_gen.create_golden_test_suite(target_code, \"validate_email_list\")\n",
        "\n",
        "# Display results\n",
        "console.print(f\"\\n[green]✅ Generated {len(test_suite['test_cases'])} test cases![/green]\")\n",
        "\n",
        "# Show test case summary\n",
        "test_summary = Table(title=\"Generated Test Cases\")\n",
        "test_summary.add_column(\"Test Name\")\n",
        "test_summary.add_column(\"Type\")\n",
        "test_summary.add_column(\"Description\")\n",
        "test_summary.add_column(\"Expected\")\n",
        "\n",
        "for test_case in test_suite['test_cases'][:8]:  # Show first 8\n",
        "    expected_summary = f\"valid: {test_case.expected_output.get('valid_count', 0)}, invalid: {test_case.expected_output.get('invalid_count', 0)}\"\n",
        "    test_summary.add_row(\n",
        "        test_case.name,\n",
        "        test_case.test_type,\n",
        "        test_case.description[:40] + \"...\" if len(test_case.description) > 40 else test_case.description,\n",
        "        expected_summary\n",
        "    )\n",
        "\n",
        "console.print(test_summary)\n",
        "\n",
        "# Show generated pytest code\n",
        "console.print(\"\\n[yellow]Generated Pytest Code:[/yellow]\")\n",
        "pytest_syntax = Syntax(test_suite['pytest_code'][:1500] + \"\\n# ... more tests ...\", \"python\", theme=\"monokai\", line_numbers=True)\n",
        "console.print(Panel(pytest_syntax, title=\"test_validate_email_list.py\", border_style=\"green\"))\n",
        "\n",
        "# Test coverage analysis\n",
        "console.print(\"\\n📊 [bold blue]Test Coverage Analysis[/bold blue]\")\n",
        "coverage_table = Table(title=\"Coverage Metrics\")\n",
        "coverage_table.add_column(\"Metric\")\n",
        "coverage_table.add_column(\"Value\")\n",
        "coverage_table.add_column(\"Status\")\n",
        "\n",
        "test_types = {}\n",
        "for test_case in test_suite['test_cases']:\n",
        "    test_types[test_case.test_type] = test_types.get(test_case.test_type, 0) + 1\n",
        "\n",
        "coverage_table.add_row(\"Total Test Cases\", str(len(test_suite['test_cases'])), \"✅ Good\")\n",
        "coverage_table.add_row(\"Happy Path Tests\", str(test_types.get('happy_path', 0)), \"✅ Covered\")\n",
        "coverage_table.add_row(\"Edge Case Tests\", str(test_types.get('edge_case', 0)), \"✅ Covered\")\n",
        "coverage_table.add_row(\"Error Case Tests\", str(test_types.get('error_case', 0)), \"✅ Covered\")\n",
        "\n",
        "console.print(coverage_table)\n",
        "\n",
        "print(\"\\n🎯 Golden test suite generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways: Golden Test Generation\n",
        "\n",
        "### 🎯 **Golden Test Strategy**\n",
        "\n",
        "1. **Capture Current Behavior**: Document what code actually does today\n",
        "2. **Comprehensive Coverage**: Test happy paths, edge cases, and error conditions\n",
        "3. **Regression Prevention**: Ensure refactors maintain existing functionality\n",
        "4. **Behavior Documentation**: Tests serve as executable specifications\n",
        "5. **Confidence Building**: Enable safe refactoring with test safety net\n",
        "\n",
        "### 🧪 **Test Generation Best Practices**\n",
        "\n",
        "- **Function Analysis**: Understand inputs, outputs, and side effects\n",
        "- **Edge Case Discovery**: Identify boundary conditions and corner cases\n",
        "- **Error Path Testing**: Verify graceful handling of invalid inputs\n",
        "- **Performance Considerations**: Include tests for performance-critical paths\n",
        "- **Maintainable Tests**: Generate readable, well-documented test code\n",
        "\n",
        "### 🔧 **Implementation Tips**\n",
        "\n",
        "- **Start Small**: Begin with critical functions before expanding\n",
        "- **Validate Tests**: Ensure generated tests actually pass with current code\n",
        "- **Review Coverage**: Use coverage tools to identify gaps\n",
        "- **Iterate**: Refine test generation based on real-world usage\n",
        "- **Team Review**: Have domain experts validate test scenarios\n",
        "\n",
        "## Next: AI UI Demo\n",
        "\n",
        "Ready to see how AI can help build user interfaces and interactive applications?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}