{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan-Do-Check Demo\n",
        "\n",
        "## Learning Objectives\n",
        "- Implement systematic planning workflows with AI\n",
        "- Build validation loops for AI-generated content\n",
        "- Create feedback mechanisms for iterative improvement\n",
        "- Design robust multi-step AI workflows\n",
        "\n",
        "## The Plan-Do-Check Pattern\n",
        "\n",
        "This demo shows how to build reliable AI workflows using the classic quality management cycle:\n",
        "1. **Plan** - Generate a structured plan for the task\n",
        "2. **Do** - Execute the plan step by step\n",
        "3. **Check** - Validate results and identify improvements\n",
        "4. **Act** - Iterate based on feedback (bonus: not in this demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install asksageclient pip_system_certs rich pydantic tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔐 Cell 1 — Load secrets (Colab) + pricing + token utils\n",
        "# ================================\n",
        "import os, time, csv\n",
        "from typing import Optional, Dict\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "ASKSAGE_API_KEY = userdata.get(\"ASKSAGE_API_KEY\")\n",
        "ASKSAGE_BASE_URL = userdata.get(\"ASKSAGE_BASE_URL\")\n",
        "ASKSAGE_EMAIL = userdata.get(\"ASKSAGE_EMAIL\")\n",
        "\n",
        "assert ASKSAGE_API_KEY, \"ASKSAGE_API_KEY not provided.\"\n",
        "assert ASKSAGE_EMAIL, \"ASKSAGE_EMAIL not provided.\"\n",
        "\n",
        "print(\"✓ Secrets loaded\")\n",
        "print(\"  • EMAIL:\", ASKSAGE_EMAIL)\n",
        "print(\"  • BASE URL:\", ASKSAGE_BASE_URL or \"(default)\")\n",
        "\n",
        "# Pricing (USD per 1,000,000 tokens)\n",
        "PRICES_PER_M = {\n",
        "    \"gpt-5\": {\"input_per_m\": 1.25, \"output_per_m\": 10.00},\n",
        "    \"gpt-5-mini\": {\"input_per_m\": 0.25, \"output_per_m\": 2.00},\n",
        "}\n",
        "\n",
        "# Tokenizer\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text or \"\"))\n",
        "\n",
        "def cost_usd(model: str, input_tokens: int, output_tokens: int) -> float:\n",
        "    if model not in PRICES_PER_M:\n",
        "        raise ValueError(f\"Unknown model: {model}\")\n",
        "    r = PRICES_PER_M[model]\n",
        "    return (input_tokens / 1_000_000) * r[\"input_per_m\"] + (output_tokens / 1_000_000) * r[\"output_per_m\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔧 Cell 2 — Import bootcamp_common and setup AskSage client\n",
        "# ================================\n",
        "import sys\n",
        "sys.path.append('../../../')  # Adjust path to reach bootcamp_common\n",
        "\n",
        "from bootcamp_common.ask_sage import AskSageClient\n",
        "\n",
        "# Initialize AskSage client\n",
        "client = AskSageClient(\n",
        "    api_key=ASKSAGE_API_KEY,\n",
        "    base_url=ASKSAGE_BASE_URL\n",
        ")\n",
        "\n",
        "print(\"✓ AskSage client initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from enum import Enum\n",
        "\n",
        "import openai\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "from rich.progress import Progress, SpinnerColumn, TextColumn\n",
        "from pydantic import BaseModel\n",
        "\n",
        "console = Console()\n",
        "print(\"✅ Libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Workflow Schemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TaskStep(BaseModel):\n",
        "    \"\"\"Individual step in a plan\"\"\"\n",
        "    step_number: int\n",
        "    description: str\n",
        "    estimated_time: int  # minutes\n",
        "    dependencies: List[int]  # step numbers this depends on\n",
        "    success_criteria: str\n",
        "\n",
        "class ExecutionPlan(BaseModel):\n",
        "    \"\"\"Complete execution plan\"\"\"\n",
        "    goal: str\n",
        "    steps: List[TaskStep]\n",
        "    total_estimated_time: int\n",
        "    risks: List[str]\n",
        "    success_metrics: List[str]\n",
        "\n",
        "class ExecutionResult(BaseModel):\n",
        "    \"\"\"Result of executing a step\"\"\"\n",
        "    step_number: int\n",
        "    output: str\n",
        "    success: bool\n",
        "    actual_time: int  # minutes\n",
        "    notes: str\n",
        "\n",
        "class QualityCheck(BaseModel):\n",
        "    \"\"\"Quality assessment of results\"\"\"\n",
        "    criteria: str\n",
        "    score: int  # 1-10\n",
        "    issues: List[str]\n",
        "    improvements: List[str]\n",
        "    overall_assessment: str\n",
        "\n",
        "class PlanDoCheckWorkflow:\n",
        "    \"\"\"Implementation of Plan-Do-Check workflow\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.setup_client()\n",
        "        self.current_plan: Optional[ExecutionPlan] = None\n",
        "        self.execution_results: List[ExecutionResult] = []\n",
        "        self.quality_checks: List[QualityCheck] = []\n",
        "    \n",
        "    def setup_client(self):\n",
        "        \"\"\"Setup API client with fallback to mock\"\"\"\n",
        "        if os.getenv('OPENAI_API_KEY'):\n",
        "            try:\n",
        "                self.client = openai.OpenAI()\n",
        "                self.has_api = True\n",
        "                console.print(\"✅ OpenAI client configured\")\n",
        "            except Exception as e:\n",
        "                self.has_api = False\n",
        "                console.print(f\"⚠️ Using mock responses: {e}\")\n",
        "        else:\n",
        "            self.has_api = False\n",
        "            console.print(\"💡 No API key found, using mock responses\")\n",
        "    \n",
        "    def generate_plan(self, goal: str) -> ExecutionPlan:\n",
        "        \"\"\"PLAN: Generate structured execution plan\"\"\"\n",
        "        \n",
        "        prompt = f\"\"\"Create a detailed execution plan for this goal: {goal}\n",
        "\n",
        "Generate a plan with:\n",
        "- 3-5 specific, actionable steps\n",
        "- Time estimates for each step\n",
        "- Dependencies between steps\n",
        "- Success criteria for each step\n",
        "- Overall risks and success metrics\n",
        "\n",
        "Respond with valid JSON matching this schema:\n",
        "{{\n",
        "  \"goal\": \"string\",\n",
        "  \"steps\": [\n",
        "    {{\n",
        "      \"step_number\": 1,\n",
        "      \"description\": \"string\",\n",
        "      \"estimated_time\": 30,\n",
        "      \"dependencies\": [],\n",
        "      \"success_criteria\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"total_estimated_time\": 120,\n",
        "  \"risks\": [\"string\"],\n",
        "  \"success_metrics\": [\"string\"]\n",
        "}}\"\"\"\n",
        "        \n",
        "        if self.has_api:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=800,\n",
        "                temperature=0.3\n",
        "            )\n",
        "            response_text = response.choices[0].message.content\n",
        "        else:\n",
        "            # Mock plan for blog post creation\n",
        "            response_text = '''{\n",
        "  \"goal\": \"Create a comprehensive blog post about sustainable technology\",\n",
        "  \"steps\": [\n",
        "    {\n",
        "      \"step_number\": 1,\n",
        "      \"description\": \"Research current sustainable technology trends and gather credible sources\",\n",
        "      \"estimated_time\": 45,\n",
        "      \"dependencies\": [],\n",
        "      \"success_criteria\": \"At least 5 credible sources from recent publications\"\n",
        "    },\n",
        "    {\n",
        "      \"step_number\": 2,\n",
        "      \"description\": \"Create detailed outline with main sections and key points\",\n",
        "      \"estimated_time\": 30,\n",
        "      \"dependencies\": [1],\n",
        "      \"success_criteria\": \"Clear outline with 4-5 main sections and subpoints\"\n",
        "    },\n",
        "    {\n",
        "      \"step_number\": 3,\n",
        "      \"description\": \"Write first draft focusing on content and flow\",\n",
        "      \"estimated_time\": 90,\n",
        "      \"dependencies\": [2],\n",
        "      \"success_criteria\": \"1500+ word draft covering all outline points\"\n",
        "    },\n",
        "    {\n",
        "      \"step_number\": 4,\n",
        "      \"description\": \"Edit for clarity, engagement, and SEO optimization\",\n",
        "      \"estimated_time\": 60,\n",
        "      \"dependencies\": [3],\n",
        "      \"success_criteria\": \"Polished post with good readability score and SEO elements\"\n",
        "    }\n",
        "  ],\n",
        "  \"total_estimated_time\": 225,\n",
        "  \"risks\": [\"Limited access to latest research\", \"Topic might be too broad\", \"SEO competition is high\"],\n",
        "  \"success_metrics\": [\"Post reaches 1500+ words\", \"Includes 5+ credible sources\", \"Readability score above 60\", \"At least 3 actionable insights for readers\"]\n",
        "}'''\n",
        "        \n",
        "        # Parse and validate\n",
        "        try:\n",
        "            plan_data = json.loads(response_text.strip().replace('```json', '').replace('```', ''))\n",
        "            self.current_plan = ExecutionPlan(**plan_data)\n",
        "            return self.current_plan\n",
        "        except Exception as e:\n",
        "            console.print(f\"[red]Plan generation failed: {e}[/red]\")\n",
        "            raise\n",
        "    \n",
        "    def execute_step(self, step_number: int) -> ExecutionResult:\n",
        "        \"\"\"DO: Execute a specific step\"\"\"\n",
        "        \n",
        "        if not self.current_plan:\n",
        "            raise ValueError(\"No plan available for execution\")\n",
        "        \n",
        "        step = next((s for s in self.current_plan.steps if s.step_number == step_number), None)\n",
        "        if not step:\n",
        "            raise ValueError(f\"Step {step_number} not found in plan\")\n",
        "        \n",
        "        prompt = f\"\"\"Execute this step from our plan:\n",
        "\n",
        "Goal: {self.current_plan.goal}\n",
        "Step {step_number}: {step.description}\n",
        "Success Criteria: {step.success_criteria}\n",
        "\n",
        "Provide a realistic simulation of executing this step. Include:\n",
        "- What specific actions you would take\n",
        "- What deliverables/outputs you would create\n",
        "- Any challenges or observations\n",
        "- How you know you've met the success criteria\n",
        "\n",
        "Keep response focused and practical (200-300 words).\"\"\"\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        if self.has_api:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=400,\n",
        "                temperature=0.5\n",
        "            )\n",
        "            output = response.choices[0].message.content\n",
        "        else:\n",
        "            # Mock execution results\n",
        "            mock_outputs = {\n",
        "                1: \"Researched sustainable tech trends. Found 6 credible sources including IEEE papers on renewable energy storage, MIT reports on carbon capture, and industry analyses from McKinsey. Key trends identified: battery recycling, green hydrogen, sustainable AI computing. All sources are from 2023-2024.\",\n",
        "                2: \"Created comprehensive outline: 1) Introduction to sustainable tech importance, 2) Current breakthrough technologies (batteries, renewables, AI efficiency), 3) Industry adoption challenges and solutions, 4) Future outlook and investment trends, 5) Actionable steps for businesses. Each section has 3-4 key subpoints.\",\n",
        "                3: \"Drafted 1,650-word blog post covering all outline sections. Strong introduction hooks readers with climate urgency. Technical sections balanced with real-world examples. Included case studies from Tesla, Google, and Nordic energy companies. Clear transitions between sections maintain flow.\",\n",
        "                4: \"Edited for clarity and SEO. Improved readability score to 65. Added target keywords naturally. Created compelling meta description. Added subheadings for better scanning. Included call-to-action encouraging readers to assess their own sustainability practices.\"\n",
        "            }\n",
        "            output = mock_outputs.get(step_number, \"Step executed successfully.\")\n",
        "        \n",
        "        actual_time = int((time.time() - start_time) * 60)  # Convert to minutes\n",
        "        if actual_time < 1:\n",
        "            actual_time = step.estimated_time  # Use estimate for demo\n",
        "        \n",
        "        result = ExecutionResult(\n",
        "            step_number=step_number,\n",
        "            output=output,\n",
        "            success=True,  # Assume success for demo\n",
        "            actual_time=actual_time,\n",
        "            notes=f\"Completed step {step_number} successfully\"\n",
        "        )\n",
        "        \n",
        "        self.execution_results.append(result)\n",
        "        return result\n",
        "    \n",
        "    def quality_check(self, criteria: str) -> QualityCheck:\n",
        "        \"\"\"CHECK: Evaluate quality of execution results\"\"\"\n",
        "        \n",
        "        if not self.execution_results:\n",
        "            raise ValueError(\"No execution results to check\")\n",
        "        \n",
        "        # Compile all outputs for assessment\n",
        "        all_outputs = \"\\n\\n\".join([f\"Step {r.step_number}: {r.output}\" for r in self.execution_results])\n",
        "        \n",
        "        prompt = f\"\"\"Evaluate the quality of these execution results against the criteria:\n",
        "\n",
        "Criteria: {criteria}\n",
        "\n",
        "Execution Results:\n",
        "{all_outputs}\n",
        "\n",
        "Provide assessment in this JSON format:\n",
        "{{\n",
        "  \"criteria\": \"{criteria}\",\n",
        "  \"score\": 8,\n",
        "  \"issues\": [\"specific issue 1\", \"specific issue 2\"],\n",
        "  \"improvements\": [\"suggestion 1\", \"suggestion 2\"],\n",
        "  \"overall_assessment\": \"Brief summary of quality and recommendations\"\n",
        "}}\n",
        "\n",
        "Score 1-10 where 10 is excellent quality.\"\"\"\n",
        "        \n",
        "        if self.has_api:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=300,\n",
        "                temperature=0.3\n",
        "            )\n",
        "            response_text = response.choices[0].message.content\n",
        "        else:\n",
        "            # Mock quality check\n",
        "            response_text = '''{\n",
        "  \"criteria\": \"Blog post completeness and quality\",\n",
        "  \"score\": 8,\n",
        "  \"issues\": [\"Could benefit from more specific data/statistics\", \"Missing direct quotes from industry experts\"],\n",
        "  \"improvements\": [\"Add 2-3 concrete statistics to strengthen arguments\", \"Include expert quotes for credibility\", \"Consider adding visual elements or diagrams\"],\n",
        "  \"overall_assessment\": \"Strong execution with comprehensive research and clear structure. Content meets word count and covers all planned sections. Minor improvements needed for expert credibility and data support.\"\n",
        "}'''\n",
        "        \n",
        "        try:\n",
        "            check_data = json.loads(response_text.strip().replace('```json', '').replace('```', ''))\n",
        "            quality_check = QualityCheck(**check_data)\n",
        "            self.quality_checks.append(quality_check)\n",
        "            return quality_check\n",
        "        except Exception as e:\n",
        "            console.print(f\"[red]Quality check failed: {e}[/red]\")\n",
        "            raise\n",
        "\n",
        "# Initialize workflow\n",
        "workflow = PlanDoCheckWorkflow()\n",
        "print(\"🔄 Plan-Do-Check workflow ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo: Complete Plan-Do-Check Cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run complete Plan-Do-Check cycle\n",
        "console.print(\"\\n🎯 [bold blue]Running Complete Plan-Do-Check Workflow[/bold blue]\\n\")\n",
        "console.print(\"─\" * 60)\n",
        "\n",
        "# PLAN Phase\n",
        "console.print(\"\\n📋 [bold yellow]PLAN PHASE[/bold yellow]\")\n",
        "goal = \"Create a comprehensive blog post about sustainable technology trends\"\n",
        "console.print(f\"[cyan]Goal:[/cyan] {goal}\")\n",
        "\n",
        "with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\")) as progress:\n",
        "    task = progress.add_task(\"Generating execution plan...\", total=None)\n",
        "    plan = workflow.generate_plan(goal)\n",
        "    progress.update(task, completed=100)\n",
        "\n",
        "console.print(\"\\n[green]✅ Plan Generated![/green]\")\n",
        "console.print(Panel(f\"**Goal:** {plan.goal}\\n\\n**Steps:** {len(plan.steps)}\\n**Estimated Time:** {plan.total_estimated_time} minutes\\n**Key Risks:** {', '.join(plan.risks[:2])}\", \n",
        "                   title=\"Execution Plan\", border_style=\"green\"))\n",
        "\n",
        "# Show detailed steps\n",
        "steps_table = Table(title=\"Detailed Step Breakdown\")\n",
        "steps_table.add_column(\"#\")\n",
        "steps_table.add_column(\"Description\")\n",
        "steps_table.add_column(\"Time\")\n",
        "steps_table.add_column(\"Success Criteria\")\n",
        "\n",
        "for step in plan.steps:\n",
        "    steps_table.add_row(\n",
        "        str(step.step_number),\n",
        "        step.description[:50] + \"...\" if len(step.description) > 50 else step.description,\n",
        "        f\"{step.estimated_time}m\",\n",
        "        step.success_criteria[:40] + \"...\" if len(step.success_criteria) > 40 else step.success_criteria\n",
        "    )\n",
        "\n",
        "console.print(steps_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DO Phase - Execute steps\n",
        "console.print(\"\\n🔨 [bold yellow]DO PHASE[/bold yellow]\")\n",
        "\n",
        "for step in plan.steps[:3]:  # Execute first 3 steps for demo\n",
        "    console.print(f\"\\n[cyan]Executing Step {step.step_number}:[/cyan] {step.description}\")\n",
        "    \n",
        "    with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\")) as progress:\n",
        "        task = progress.add_task(f\"Working on step {step.step_number}...\", total=None)\n",
        "        result = workflow.execute_step(step.step_number)\n",
        "        progress.update(task, completed=100)\n",
        "    \n",
        "    status = \"[green]✅ Success[/green]\" if result.success else \"[red]❌ Failed[/red]\"\n",
        "    console.print(f\"{status} - Completed in {result.actual_time} minutes\")\n",
        "    console.print(Panel(result.output[:200] + \"...\" if len(result.output) > 200 else result.output, \n",
        "                       title=f\"Step {step.step_number} Output\", border_style=\"blue\"))\n",
        "\n",
        "console.print(f\"\\n[green]✅ Executed {len(workflow.execution_results)} steps successfully![/green]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHECK Phase - Quality assessment\n",
        "console.print(\"\\n🔍 [bold yellow]CHECK PHASE[/bold yellow]\")\n",
        "\n",
        "criteria = \"Blog post completeness, quality of research, and adherence to success metrics\"\n",
        "console.print(f\"[cyan]Quality Criteria:[/cyan] {criteria}\")\n",
        "\n",
        "with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\")) as progress:\n",
        "    task = progress.add_task(\"Conducting quality assessment...\", total=None)\n",
        "    quality_check = workflow.quality_check(criteria)\n",
        "    progress.update(task, completed=100)\n",
        "\n",
        "# Display quality results\n",
        "score_color = \"green\" if quality_check.score >= 8 else \"yellow\" if quality_check.score >= 6 else \"red\"\n",
        "console.print(f\"\\n[{score_color}]📊 Quality Score: {quality_check.score}/10[/{score_color}]\")\n",
        "\n",
        "console.print(Panel(quality_check.overall_assessment, title=\"Quality Assessment\", border_style=score_color))\n",
        "\n",
        "if quality_check.issues:\n",
        "    console.print(\"\\n[red]🚨 Issues Identified:[/red]\")\n",
        "    for issue in quality_check.issues:\n",
        "        console.print(f\"  • {issue}\")\n",
        "\n",
        "if quality_check.improvements:\n",
        "    console.print(\"\\n[yellow]💡 Suggested Improvements:[/yellow]\")\n",
        "    for improvement in quality_check.improvements:\n",
        "        console.print(f\"  • {improvement}\")\n",
        "\n",
        "print(\"\\n🎯 Plan-Do-Check cycle complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow Summary & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate workflow summary\n",
        "console.print(\"\\n📊 [bold blue]Workflow Performance Summary[/bold blue]\")\n",
        "\n",
        "# Performance metrics table\n",
        "metrics_table = Table(title=\"Execution Metrics\")\n",
        "metrics_table.add_column(\"Metric\")\n",
        "metrics_table.add_column(\"Planned\")\n",
        "metrics_table.add_column(\"Actual\")\n",
        "metrics_table.add_column(\"Variance\")\n",
        "\n",
        "total_planned_time = sum(step.estimated_time for step in plan.steps[:3])\n",
        "total_actual_time = sum(result.actual_time for result in workflow.execution_results)\n",
        "time_variance = ((total_actual_time - total_planned_time) / total_planned_time * 100) if total_planned_time > 0 else 0\n",
        "\n",
        "metrics_table.add_row(\n",
        "    \"Total Time\",\n",
        "    f\"{total_planned_time} min\",\n",
        "    f\"{total_actual_time} min\",\n",
        "    f\"{time_variance:+.1f}%\"\n",
        ")\n",
        "\n",
        "metrics_table.add_row(\n",
        "    \"Steps Completed\",\n",
        "    f\"{len(plan.steps)} planned\",\n",
        "    f\"{len(workflow.execution_results)} done\",\n",
        "    f\"{len(workflow.execution_results)/len(plan.steps)*100:.0f}%\"\n",
        ")\n",
        "\n",
        "metrics_table.add_row(\n",
        "    \"Success Rate\",\n",
        "    \"100%\",\n",
        "    f\"{sum(1 for r in workflow.execution_results if r.success)/len(workflow.execution_results)*100:.0f}%\",\n",
        "    \"On target\"\n",
        ")\n",
        "\n",
        "console.print(metrics_table)\n",
        "\n",
        "# Final assessment\n",
        "if quality_check.score >= 8:\n",
        "    assessment = \"[green]🎉 Excellent execution! Workflow delivered high-quality results.[/green]\"\n",
        "elif quality_check.score >= 6:\n",
        "    assessment = \"[yellow]✅ Good execution with room for improvement.[/yellow]\"\n",
        "else:\n",
        "    assessment = \"[red]⚠️ Execution needs significant improvement.[/red]\"\n",
        "\n",
        "console.print(f\"\\n{assessment}\")\n",
        "console.print(f\"\\n[bold]Next Steps:[/bold] {'Implement suggested improvements and re-run check phase' if quality_check.improvements else 'Proceed to deployment/delivery'}\")\n",
        "\n",
        "print(\"\\n🔄 Complete Plan-Do-Check workflow demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways: Systematic AI Workflows\n",
        "\n",
        "### 🔄 **The Plan-Do-Check Pattern**\n",
        "\n",
        "1. **PLAN**: Generate structured, validated execution plans\n",
        "2. **DO**: Execute steps systematically with monitoring\n",
        "3. **CHECK**: Assess quality against defined criteria\n",
        "4. **ACT**: Iterate based on feedback (bonus round!)\n",
        "\n",
        "### 🎯 **Benefits of Systematic Workflows**\n",
        "\n",
        "- **Reliability**: Consistent results through structured processes\n",
        "- **Visibility**: Clear progress tracking and metrics\n",
        "- **Quality**: Built-in validation and improvement loops\n",
        "- **Scalability**: Repeatable patterns for complex tasks\n",
        "- **Learning**: Captured insights for future improvements\n",
        "\n",
        "### 📊 **Production Implementation Tips**\n",
        "\n",
        "- **Validate Plans**: Check for logical dependencies and realistic timelines\n",
        "- **Monitor Execution**: Track progress, time, and resource usage\n",
        "- **Automate Checks**: Use quantitative metrics where possible\n",
        "- **Capture Feedback**: Log lessons learned for process improvement\n",
        "- **Handle Failures**: Implement fallback strategies and error recovery\n",
        "\n",
        "### 🚀 **Advanced Extensions**\n",
        "\n",
        "- **Multi-Agent**: Different agents for planning, execution, and checking\n",
        "- **Parallel Execution**: Run independent steps concurrently\n",
        "- **Dynamic Replanning**: Adjust plans based on intermediate results\n",
        "- **Risk Assessment**: Proactive identification and mitigation\n",
        "- **Performance Learning**: Improve estimates based on historical data\n",
        "\n",
        "## Next: Day 2 Labs\n",
        "\n",
        "Ready to put these patterns into practice? The labs will have you build your own prompt library, structured output pipelines, and diagnostic dashboards!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}