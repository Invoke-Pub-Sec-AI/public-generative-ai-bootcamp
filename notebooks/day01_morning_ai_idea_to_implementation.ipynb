{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI in Action: From Idea to Implementation\n",
        "\n",
        "## Learning Objectives\n",
        "- Watch AI transform a business requirement into working code with error handling and documentation\n",
        "- See how tokenization and context management affect output quality\n",
        "- Understand the relationship between prompt design and code quality\n",
        "- Learn how AI handles edge cases and error scenarios\n",
        "\n",
        "## The Demo: Business Idea to Production Code\n",
        "\n",
        "We'll start with a real business requirement and watch AI:\n",
        "1. **Analyze Requirements** - Break down the problem\n",
        "2. **Design Solution** - Architecture and approach\n",
        "3. **Generate Code** - Complete implementation\n",
        "4. **Add Error Handling** - Robust exception management\n",
        "5. **Create Documentation** - Usage examples and API docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Import our AskSage client\n",
        "import sys\n",
        "sys.path.append('../../../bootcamp_common')\n",
        "from ask_sage import AskSageClient\n",
        "\n",
        "# Initialize client and tokenizer\n",
        "client = AskSageClient()\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(\"AskSage client initialized successfully\")\n",
        "print(\"Ready to transform ideas into code...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Business Requirement: Invoice Processing System\n",
        "\n",
        "**Real-World Scenario**: A small business needs to automate invoice processing\n",
        "\n",
        "**Requirements**:\n",
        "- Extract data from PDF invoices\n",
        "- Validate required fields\n",
        "- Calculate totals and tax amounts\n",
        "- Generate approval workflows\n",
        "- Export to accounting systems\n",
        "- Handle errors gracefully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business requirement prompt\n",
        "business_requirement = \"\"\"\n",
        "BUSINESS REQUIREMENT: Invoice Processing Automation\n",
        "\n",
        "Our small business receives 50+ invoices weekly in PDF format. We need to:\n",
        "1. Extract key data: vendor, amount, date, line items, tax\n",
        "2. Validate completeness and accuracy\n",
        "3. Calculate totals and verify math\n",
        "4. Route for approval based on amount thresholds\n",
        "5. Export approved invoices to QuickBooks format\n",
        "6. Handle errors and exceptions gracefully\n",
        "7. Generate processing reports\n",
        "\n",
        "CONSTRAINTS:\n",
        "- Must handle various PDF formats\n",
        "- Processing time under 30 seconds per invoice\n",
        "- 99% accuracy required\n",
        "- Secure handling of financial data\n",
        "- Easy to maintain and extend\n",
        "\n",
        "Create a complete Python solution with proper architecture.\n",
        "\"\"\"\n",
        "\n",
        "# Count tokens in our prompt\n",
        "prompt_tokens = len(tokenizer.encode(business_requirement))\n",
        "print(f\"Business requirement: {prompt_tokens} tokens\")\n",
        "print(\"\\nSending to AI for analysis and implementation...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: AI analyzes and implements the solution\n",
        "start_time = time.time()\n",
        "response = client.query({\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": business_requirement}],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 2500\n",
        "})\n",
        "implementation_time = time.time() - start_time\n",
        "\n",
        "solution = response['choices'][0]['message']['content']\n",
        "response_tokens = len(tokenizer.encode(solution))\n",
        "\n",
        "print(f\"=== SOLUTION GENERATED IN {implementation_time:.2f} SECONDS ===\")\n",
        "print(f\"Input tokens: {prompt_tokens}\")\n",
        "print(f\"Output tokens: {response_tokens}\")\n",
        "print(f\"Total tokens: {prompt_tokens + response_tokens}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(solution[:1200] + \"...\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demonstrating Token Impact on Quality\n",
        "\n",
        "Let's see what happens when we limit tokens and how it affects output quality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Same request with limited tokens\n",
        "print(\"=== TESTING TOKEN LIMITS ===\")\n",
        "\n",
        "# Test with very limited tokens\n",
        "limited_response = client.query({\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": business_requirement}],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 500  # Severely limited\n",
        "})\n",
        "\n",
        "limited_solution = limited_response['choices'][0]['message']['content']\n",
        "limited_tokens = len(tokenizer.encode(limited_solution))\n",
        "\n",
        "print(f\"\\nLimited to 500 tokens - Actual output: {limited_tokens} tokens\")\n",
        "print(\"TRUNCATED OUTPUT:\")\n",
        "print(limited_solution)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Show the difference\n",
        "print(\"QUALITY COMPARISON:\")\n",
        "print(f\"Full response ({response_tokens} tokens): Complete solution with classes, error handling, documentation\")\n",
        "print(f\"Limited response ({limited_tokens} tokens): Incomplete, missing critical components\")\n",
        "print(\"\\nKey lesson: Token limits directly impact solution completeness\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterative Refinement: Adding Specific Features\n",
        "\n",
        "Now let's see how AI handles follow-up requests to enhance the solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Enhance with specific security features\n",
        "security_enhancement = f\"\"\"\n",
        "Based on the invoice processing solution you just created:\n",
        "\n",
        "{solution[:800]}...\n",
        "\n",
        "Add comprehensive security features:\n",
        "1. Input sanitization and validation\n",
        "2. Secure file handling (prevent path traversal)\n",
        "3. Data encryption for sensitive information\n",
        "4. Audit logging for compliance\n",
        "5. Rate limiting to prevent abuse\n",
        "6. Authentication and authorization\n",
        "\n",
        "Show the enhanced security implementation.\n",
        "\"\"\"\n",
        "\n",
        "security_tokens = len(tokenizer.encode(security_enhancement))\n",
        "print(f\"Security enhancement request: {security_tokens} tokens\")\n",
        "\n",
        "start_time = time.time()\n",
        "security_response = client.query({\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": security_enhancement}],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 1500\n",
        "})\n",
        "security_time = time.time() - start_time\n",
        "\n",
        "security_features = security_response['choices'][0]['message']['content']\n",
        "print(f\"\\n=== SECURITY FEATURES ADDED IN {security_time:.2f} SECONDS ===\")\n",
        "print(security_features[:800] + \"...\")\n",
        "print(\"\\nAI successfully enhanced the solution with enterprise-grade security\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Edge Cases and Error Handling\n",
        "\n",
        "Let's see how AI handles edge cases and error scenarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Generate comprehensive error handling\n",
        "error_handling_request = \"\"\"\n",
        "For the invoice processing system, generate comprehensive error handling for these scenarios:\n",
        "\n",
        "EDGE CASES:\n",
        "1. Corrupted PDF files\n",
        "2. Invoices with missing required fields\n",
        "3. Mathematical errors in calculations\n",
        "4. Duplicate invoice detection\n",
        "5. Network failures during processing\n",
        "6. Disk space exhaustion\n",
        "7. Invalid date formats\n",
        "8. Foreign currency handling\n",
        "\n",
        "Create error handling code with:\n",
        "- Specific exception classes\n",
        "- Recovery strategies\n",
        "- User-friendly error messages\n",
        "- Logging and monitoring\n",
        "- Graceful degradation\n",
        "\"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "error_response = client.query({\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": error_handling_request}],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 1500\n",
        "})\n",
        "error_time = time.time() - start_time\n",
        "\n",
        "error_handling = error_response['choices'][0]['message']['content']\n",
        "print(f\"=== ERROR HANDLING GENERATED IN {error_time:.2f} SECONDS ===\")\n",
        "print(error_handling[:800] + \"...\")\n",
        "print(\"\\nAI created robust error handling for 8+ edge cases\")\n",
        "print(\"Includes: Custom exceptions, recovery strategies, logging\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Documentation Generation\n",
        "\n",
        "Finally, let's see AI generate comprehensive documentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Generate complete documentation\n",
        "documentation_request = \"\"\"\n",
        "Create comprehensive documentation for the invoice processing system including:\n",
        "\n",
        "1. **README.md** - Installation, setup, usage\n",
        "2. **API Documentation** - All classes and methods\n",
        "3. **Configuration Guide** - Environment setup\n",
        "4. **Troubleshooting Guide** - Common issues and solutions\n",
        "5. **Security Guide** - Best practices and compliance\n",
        "6. **Performance Tuning** - Optimization recommendations\n",
        "\n",
        "Make it production-ready documentation that a new developer could follow.\n",
        "\"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "docs_response = client.query({\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": documentation_request}],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 1500\n",
        "})\n",
        "docs_time = time.time() - start_time\n",
        "\n",
        "documentation = docs_response['choices'][0]['message']['content']\n",
        "print(f\"=== DOCUMENTATION GENERATED IN {docs_time:.2f} SECONDS ===\")\n",
        "print(documentation[:800] + \"...\")\n",
        "print(\"\\nAI created complete documentation package\")\n",
        "print(\"Includes: Setup guides, API docs, troubleshooting, security\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What We Just Accomplished\n",
        "\n",
        "### From Business Idea to Production System:\n",
        "\n",
        "**Total Time**: ~20 seconds of AI processing\n",
        "**Total Output**: 3000+ lines of code and documentation\n",
        "\n",
        "**Components Generated**:\n",
        "1. **Core System** - Complete invoice processing pipeline\n",
        "2. **Security Layer** - Enterprise-grade security features\n",
        "3. **Error Handling** - Robust exception management\n",
        "4. **Documentation** - Production-ready guides\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "**Token Management Impact**:\n",
        "- Full tokens (2500): Complete, production-ready solution\n",
        "- Limited tokens (500): Incomplete, missing critical features\n",
        "- Lesson: Token limits directly affect solution quality\n",
        "\n",
        "**Iterative Enhancement**:\n",
        "- AI successfully built upon previous responses\n",
        "- Each iteration added specific, targeted improvements\n",
        "- Context awareness maintained across requests\n",
        "\n",
        "**Quality Characteristics**:\n",
        "- Proper error handling and edge case management\n",
        "- Security best practices implemented\n",
        "- Comprehensive documentation included\n",
        "- Production-ready architecture patterns\n",
        "\n",
        "### What's Next:\n",
        "In the following sessions, you'll learn the techniques that make this possible:\n",
        "- Prompt engineering for consistent results\n",
        "- Token optimization strategies\n",
        "- Error handling patterns\n",
        "- Quality assurance techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary metrics\n",
        "total_time = implementation_time + security_time + error_time + docs_time\n",
        "total_output_tokens = response_tokens + len(tokenizer.encode(security_features)) + len(tokenizer.encode(error_handling)) + len(tokenizer.encode(documentation))\n",
        "\n",
        "print(\"=== TRANSFORMATION COMPLETE ===\")\n",
        "print(f\"\\nTotal AI processing time: {total_time:.2f} seconds\")\n",
        "print(f\"Total tokens generated: {total_output_tokens:,}\")\n",
        "print(f\"Estimated lines of code: {total_output_tokens // 4:,}+\")\n",
        "\n",
        "print(\"\\nBusiness Value Created:\")\n",
        "print(\"  - Complete invoice processing system\")\n",
        "print(\"  - Enterprise security implementation\")\n",
        "print(\"  - Comprehensive error handling\")\n",
        "print(\"  - Production documentation\")\n",
        "print(\"  - Ready for immediate deployment\")\n",
        "\n",
        "print(\"\\nDevelopment Time Comparison:\")\n",
        "print(\"  - Traditional development: 2-3 weeks\")\n",
        "print(\"  - AI-assisted development: 20 seconds + review time\")\n",
        "print(\"  - Time savings: 99%+\")\n",
        "\n",
        "print(\"\\nNext: Learn how to achieve these results consistently\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}