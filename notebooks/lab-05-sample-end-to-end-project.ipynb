{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "eb4376304a62475b889530a51c57a107",
      "metadata": {},
      "source": [
        "# Lab 05: Sample End-to-End Fine-Tuning Project\n",
        "\n",
        "## Learning Objectives\n",
        "- Integrate all concepts from the previous labs into a single project\n",
        "- Prepare a real-world style dataset for fine-tuning\n",
        "- Apply LoRA for parameter-efficient fine-tuning\n",
        "- Run, monitor, and evaluate the fine-tuning job from start to finish\n",
        "- Gain a holistic understanding of a complete fine-tuning workflow\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89703396983d49088408a72b7391b5aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "# We will reuse the mock classes from previous labs\n",
        "# In a real project, these would be imports from libraries like transformers, peft, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5dda304ed4e4d22846fe108f88428aa",
      "metadata": {},
      "source": [
        "### --- Mock Implementations from Previous Labs --- ###\n",
        "# (These would normally be imported from other files or libraries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b83f8527d943d3a7f9b458f6d8110a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# From Lab 01 & 02: Data Handling\n",
        "@dataclass\n",
        "class TrainingExample:\n",
        "    prompt: str\n",
        "    completion: str\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    return text.strip().lower()\n",
        "\n",
        "# From Lab 03: LoRA and PEFT\n",
        "class LoRALayer:\n",
        "    def __init__(self, original_weights: np.ndarray, rank: int):\n",
        "        self.original_weights = original_weights\n",
        "        self.A = np.random.randn(original_weights.shape[0], rank) * 0.01\n",
        "        self.B = np.zeros((rank, original_weights.shape[1]))\n",
        "        self.original_weights.setflags(write=False)\n",
        "    @property\n",
        "    def combined_weights(self): return self.original_weights + np.dot(self.A, self.B)\n",
        "\n",
        "class MockLoRAModel:\n",
        "    def __init__(self, vocab_size=256, dim=32, lora_rank=4):\n",
        "        self.lora_layer = LoRALayer(np.random.randn(dim, vocab_size) * 0.1, rank=lora_rank)\n",
        "    def generate(self, prompt: str) -> str: return 'mock completion'\n",
        "\n",
        "# From Lab 04: Evaluation\n",
        "def simple_bleu(reference: str, candidate: str) -> float:\n",
        "    ref_tokens, cand_tokens = reference.split(), candidate.split()\n",
        "    if not ref_tokens or not cand_tokens: return 0.0\n",
        "    p_numer = sum(min(cand_tokens.count(token), ref_tokens.count(token)) for token in set(cand_tokens))\n",
        "    precision = p_numer / len(cand_tokens)\n",
        "    bp = math.exp(1 - len(ref_tokens) / len(cand_tokens)) if len(cand_tokens) < len(ref_tokens) else 1.0\n",
        "    return bp * precision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e752874595aa46efade562f1097ecb52",
      "metadata": {},
      "source": [
        "## Step 1: Load and Prepare the Dataset\n",
        "\n",
        "Our task is to fine-tune a model to be a helpful assistant that answers questions about historical figures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c402c13266c47b581668a11fddc3644",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raw dataset\n",
        "raw_data = [\n",
        "    {\"question\": \"  Who was the first emperor of Rome? \", \"answer\": \"Augustus Caesar.\"},\n",
        "    {\"question\": \"Tell me about Cleopatra\", \"answer\": \"Cleopatra was the last active ruler of the Ptolemaic Kingdom of Egypt.\"},\n",
        "    {\"question\": \"When did Leonardo da Vinci live?  \", \"answer\": \"1452-1519\"},\n",
        "    {\"question\": \"what is einstein known for\", \"answer\": \"The theory of relativity.\"},\n",
        "    {\"question\": \"\", \"answer\": \"This should be removed.\"} # Bad data\n",
        "]\n",
        "\n",
        "# Data Preparation\n",
        "def prepare_dataset(raw_data: List[Dict[str, str]]) -> List[TrainingExample]:\n",
        "    prepared = []\n",
        "    for item in raw_data:\n",
        "        prompt = clean_text(item.get('question', ''))\n",
        "        completion = clean_text(item.get('answer', ''))\n",
        "        if prompt and completion:\n",
        "            # Formatting for instruction-following\n",
        "            formatted_prompt = f\"### Instruction:\\nAnswer the following question.\\\n",
        "\\\n",
        "### Question:\\n{prompt}\\\\\n",
        "### Answer:\\n\"\n",
        "            prepared.append(TrainingExample(prompt=formatted_prompt, completion=completion))\n",
        "    return prepared\n",
        "\n",
        "dataset = prepare_dataset(raw_data)\n",
        "train_dataset = dataset[:3]\n",
        "eval_dataset = dataset[3:]\n",
        "\n",
        "print(\"--- Prepared Training Example ---\")\n",
        "print(train_dataset[0].prompt + train_dataset[0].completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05b79abd47a48358b797b2d99b64b3d",
      "metadata": {},
      "source": [
        "## Step 2: Set Up the Model and Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921b7e38876b431f84265f5c2b54dfb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EndToEndPipeline:\n",
        "    def __init__(self, model, train_data, eval_data, lr=0.01):\n",
        "        self.model = model\n",
        "        self.train_data = train_data\n",
        "        self.eval_data = eval_data\n",
        "        self.lr = lr\n",
        "        self.history = {'loss': [], 'bleu_score': []}\n",
        "\n",
        "    def run(self, epochs: int):\n",
        "        print(\"--- Starting End-to-End Fine-Tuning Job ---\")\n",
        "        for epoch in range(epochs):\n",
        "            # 1. Training Step\n",
        "            total_loss = self._train_one_epoch()\n",
        "            avg_loss = total_loss / len(self.train_data)\n",
        "            \n",
        "            # 2. Evaluation Step\n",
        "            avg_bleu = self._evaluate()\n",
        "            \n",
        "            # 3. Logging\n",
        "            self.history['loss'].append(avg_loss)\n",
        "            self.history['bleu_score'].append(avg_bleu)\n",
        "            print(f'Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Avg BLEU: {avg_bleu:.4f}')\n",
        "\n",
        "    def _train_one_epoch(self) -> float:\n",
        "        # Mock training: update LoRA weights\n",
        "        grad_A = np.random.randn(*self.model.lora_layer.A.shape) * 0.01\n",
        "        grad_B = np.random.randn(*self.model.lora_layer.B.shape) * 0.01\n",
        "        self.model.lora_layer.A -= self.lr * grad_A\n",
        "        self.model.lora_layer.B -= self.lr * grad_B\n",
        "        return np.mean((grad_A**2 + grad_B**2)) * len(self.train_data) # Mock loss\n",
        "\n",
        "    def _evaluate(self) -> float:\n",
        "        # Mock generation that improves over time\n",
        "        total_bleu = 0\n",
        "        for item in self.eval_data:\n",
        "            # Simulate model getting better\n",
        "            improvement_factor = len(self.history['loss']) / 10\n",
        "            if random.random() < improvement_factor:\n",
        "                mock_completion = item.completion\n",
        "            else:\n",
        "                mock_completion = 'i do not know'\n",
        "            \n",
        "            total_bleu += simple_bleu(item.completion, mock_completion)\n",
        "        return total_bleu / len(self.eval_data)\n",
        "\n",
        "    def plot_results(self):\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        ax1.plot(self.history['loss'], label='Loss')\n",
        "        ax1.set_title('Training Loss')\n",
        "        ax2.plot(self.history['bleu_score'], label='BLEU Score', color='green')\n",
        "        ax2.set_title('Evaluation BLEU Score')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2beab4749444fef824534cdde1bc33e",
      "metadata": {},
      "source": [
        "## Step 3: Run the Fine-Tuning Job and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09c198415264d91ab4758bedc3e2115",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize everything\n",
        "lora_model = MockLoRAModel(lora_rank=8)\n",
        "pipeline = EndToEndPipeline(lora_model, train_dataset, eval_dataset, lr=0.01)\n",
        "\n",
        "# Run the job\n",
        "pipeline.run(epochs=10)\n",
        "\n",
        "# Visualize the results\n",
        "print(\"\n",
        "--- Training and Evaluation Metrics ---\")\n",
        "pipeline.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "237a1ce85f70460fb3874ef3244b8513",
      "metadata": {},
      "source": [
        "## Final Project Summary\n",
        "\n",
        "Congratulations! You have completed an end-to-end fine-tuning project. In this lab, you:\n",
        "1. **Prepared Data**: Loaded raw data, cleaned it, and formatted it for an instruction-following task.\n",
        "2. **Set Up a PEFT Model**: Configured a mock model to use LoRA, ensuring only a small fraction of parameters would be updated.\n",
        "3. **Ran a Training Pipeline**: Executed a training loop that included both training steps and evaluation on a hold-out set.\n",
        "4. **Monitored and Evaluated**: Logged key metrics (loss and BLEU score) and visualized them to assess the model's learning progress.\n",
        "\n",
        "This workflow is a blueprint for real-world fine-tuning. While the model and data were simplified, the steps—data prep, model setup, training, and evaluation—are universal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e68a0292dff4ac7b20d855826bee29a",
      "metadata": {},
      "source": [
        "## Exercises for Further Exploration\n",
        "\n",
        "1. **Integrate Real Components**: Try to replace one of the mock components with a real one. For example, use a real tokenizer from the `transformers` library.\n",
        "2. **Add More Augmentation**: In the data preparation step, add the data augmentation techniques from Lab 02 to increase the size and diversity of your training set.\n",
        "3. **Experiment with LoRA Rank**: Rerun the pipeline with different LoRA ranks (e.g., 2 vs. 32). Does a higher rank lead to a better BLEU score in this mock setup? What are the trade-offs?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}