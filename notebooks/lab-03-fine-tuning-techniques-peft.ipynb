{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "955be76a1ae640d1afc84425100d5447",
      "metadata": {},
      "source": [
        "# Lab 03: Fine-Tuning Techniques & PEFT\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the difference between full fine-tuning and parameter-efficient fine-tuning (PEFT)\n",
        "- Implement Low-Rank Adaptation (LoRA), a popular PEFT method\n",
        "- Compare the number of trainable parameters between full tuning and LoRA\n",
        "- Apply LoRA to a mock model\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1476ea18eee4ef59bc1976d493b4357",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "from dataclasses import dataclass, field"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da921825ced348af82b1329e89b50010",
      "metadata": {},
      "source": [
        "## Part 1: Full Fine-Tuning vs. PEFT\n",
        "\n",
        "**Full Fine-Tuning** involves updating all the weights of a pre-trained model. While effective, it has significant drawbacks:\n",
        "- **High Computational Cost**: Requires a lot of memory and processing power.\n",
        "- **Large Storage Needs**: A full copy of the model must be saved for each task.\n",
        "- **Catastrophic Forgetting**: The model may lose some of its general capabilities.\n",
        "\n",
        "**Parameter-Efficient Fine-Tuning (PEFT)** methods update only a small subset of the model's parameters. This offers several advantages:\n",
        "- **Efficiency**: Drastically reduces memory and computational requirements.\n",
        "- **Small Footprint**: Only the small number of updated parameters need to be saved.\n",
        "- **Reduces Forgetting**: The original pre-trained weights are frozen, preserving general knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec8757d96a745378ac7c313e8881901",
      "metadata": {},
      "source": [
        "## Part 2: Low-Rank Adaptation (LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dca010ce8684468ab4ec2fb9152f105",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoRALayer:\n",
        "    \"\"\"Implements a LoRA layer that wraps a linear (weight) layer.\"\"\"\n",
        "    def __init__(self, original_weights: np.ndarray, rank: int):\n",
        "        self.original_weights = original_weights\n",
        "        self.rank = rank\n",
        "        d, k = original_weights.shape\n",
        "        \n",
        "        # LoRA's low-rank matrices\n",
        "        self.A = np.random.randn(d, rank) * 0.01 # (d, r)\n",
        "        self.B = np.zeros((rank, k))             # (r, k)\n",
        "        \n",
        "        # The original weights are frozen\n",
        "        self.original_weights.setflags(write=False)\n",
        "        \n",
        "    @property\n",
        "    def combined_weights(self) -> np.ndarray:\n",
        "        # The core idea: W_0 + B * A\n",
        "        return self.original_weights + np.dot(self.A, self.B)\n",
        "        \n",
        "    def trainable_parameters(self) -> int:\n",
        "        return self.A.size + self.B.size\n",
        "\n",
        "# Let's create a mock weight matrix for a large linear layer\n",
        "d_model = 4096 # Dimension of the model\n",
        "vocab_size = 32000 # Size of the vocabulary\n",
        "\n",
        "original_weight_matrix = np.random.randn(d_model, vocab_size)\n",
        "\n",
        "# Now, let's apply LoRA\n",
        "lora_rank = 8 # A small rank\n",
        "lora_layer = LoRALayer(original_weight_matrix, rank=lora_rank)\n",
        "\n",
        "# Compare the number of parameters\n",
        "full_params = original_weight_matrix.size\n",
        "lora_params = lora_layer.trainable_parameters()\n",
        "\n",
        "print(f'--- Parameter Comparison ---\")\n",
        "print(f'Original Full Layer Parameters: {full_params:,}')\n",
        "print(f'LoRA (r={lora_rank}) Trainable Parameters: {lora_params:,}')\n",
        "print(f'Reduction Factor: {full_params / lora_params:.2f}x')\n",
        "print(f'LoRA uses {lora_params / full_params:.4%} of the original parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd67e3fd42b1462fb8d05cbe40be9861",
      "metadata": {},
      "source": [
        "## Part 3: Applying LoRA to a Mock Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "519ca88a33a54d2a9ad705cb9dbe38f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockLoRAModel:\n",
        "    \"\"\"A mock model with a LoRA layer.\"\"\"\n",
        "    def __init__(self, vocab_size=256, dim=32, lora_rank=4):\n",
        "        # The main weight matrix of the model\n",
        "        original_weights = np.random.randn(dim, vocab_size) * 0.1\n",
        "        self.lora_layer = LoRALayer(original_weights, rank=lora_rank)\n",
        "\n",
        "    def forward(self, input_tokens: List[int]) -> np.ndarray:\n",
        "        # The forward pass uses the combined weights\n",
        "        combined_w = self.lora_layer.combined_weights\n",
        "        input_vectors = np.array([combined_w[:, token] for token in input_tokens])\n",
        "        avg_vector = np.mean(input_vectors, axis=0)\n",
        "        logits = np.dot(avg_vector, combined_w)\n",
        "        return logits\n",
        "\n",
        "def lora_fine_tune_loop(model: MockLoRAModel, dataset, tokenizer, epochs, lr):\n",
        "    \"\"\"Simplified fine-tuning loop that only updates LoRA matrices A and B.\"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        # In a real implementation, backpropagation would compute gradients for A and B.\n",
        "        # We simulate this by applying random updates to A and B.\n",
        "        grad_A = np.random.randn(*model.lora_layer.A.shape) * 0.01\n",
        "        grad_B = np.random.randn(*model.lora_layer.B.shape) * 0.01\n",
        "        \n",
        "        model.lora_layer.A -= lr * grad_A\n",
        "        model.lora_layer.B -= lr * grad_B\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs} - LoRA weights updated.')\n",
        "\n",
        "# Initialize model\n",
        "lora_model = MockLoRAModel()\n",
        "\n",
        "# Check that original weights are not changed\n",
        "original_weights_before = lora_model.lora_layer.original_weights.copy()\n",
        "\n",
        "lora_fine_tune_loop(lora_model, [], None, epochs=50, lr=0.01)\n",
        "\n",
        "original_weights_after = lora_model.lora_layer.original_weights\n",
        "\n",
        "print(\"\n",
        "--- Weight Integrity Check ---\")\n",
        "print(f'Original weights remain unchanged: {np.allclose(original_weights_before, original_weights_after)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26292fae1e1438490635e53060eec98",
      "metadata": {},
      "source": [
        "## Part 4: Merging LoRA Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "027cb544829342519b4e22a9acf21e1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_lora_weights(lora_layer: LoRALayer) -> np.ndarray:\n",
        "    \"\"\"Merges the LoRA weights into the original weights for inference.\"\"\"\n",
        "    return lora_layer.combined_weights\n",
        "\n",
        "print(\"--- Merging for Inference ---\")\n",
        "print(f'Shape of original weights: {lora_model.lora_layer.original_weights.shape}')\n",
        "\n",
        "# After training, you can merge the weights\n",
        "merged_weights = merge_lora_weights(lora_model.lora_layer)\n",
        "print(f'Shape of merged weights: {merged_weights.shape}')\n",
        "\n",
        "# The new merged model can be used for inference without the LoRA overhead.\n",
        "# This means inference speed is identical to the original model.\n",
        "inference_model_weights = merged_weights\n",
        "print(\"LoRA weights successfully merged for efficient inference.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3ffb880c91493182ddb4dd8458d3fb",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Experiment with Rank**: Change the `lora_rank`. How does it affect the number of trainable parameters? What might be the trade-off between a very low rank (e.g., 1) and a higher rank (e.g., 64)?\n",
        "2. **Implement LoRA for Multiple Layers**: Modify the `MockLoRAModel` to have multiple linear layers (e.g., `layer1`, `layer2`) and apply LoRA to each of them. How would you manage the different LoRA matrices?\n",
        "3. **Save and Load LoRA Adapters**: Write functions to save only the LoRA matrices (`A` and `B`) to a file and then load them back into a `LoRALayer`. This demonstrates how lightweight PEFT adapters are.\n",
        "\n",
        "## Summary\n",
        "\n",
        "You learned:\n",
        "- The key differences and trade-offs between full fine-tuning and PEFT.\n",
        "- The mechanics of LoRA, a popular PEFT technique that uses low-rank matrices to adapt a model.\n",
        "- How LoRA dramatically reduces the number of trainable parameters.\n",
        "- How to merge LoRA weights back into the base model for efficient inference."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}