{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Tokenization & Costing\n",
        "\n",
        "## Objectives\n",
        "- Build a tokenization analyzer for different text types\n",
        "- Create a cost estimation calculator for various models\n",
        "- Generate a comparison table of model costs and performance\n",
        "- **Exit ticket**: Submit table as `results/costing.json`\n",
        "\n",
        "## Time Allocation: 45-60 minutes\n",
        "\n",
        "## Overview\n",
        "Understanding tokenization and costs is crucial for building production LLM applications. In this lab, you'll build tools to analyze token usage and estimate costs across different providers and models.\n",
        "\n",
        "## Prerequisites\n",
        "- Completed Lab 1 (Environment Bootstrap)\n",
        "- Basic understanding of tokenization from the demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install tiktoken pandas matplotlib seaborn tabulate asksageclient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "from tabulate import tabulate\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Build a Tokenization Analyzer (20 minutes)\n",
        "\n",
        "Create a comprehensive tokenization analyzer that works with different encodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TokenizationAnalyzer:\n",
        "    \"\"\"Analyze tokenization patterns across different encoders\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # TODO: Initialize different encoders\n",
        "        self.encoders = {\n",
        "        }\n",
        "        \n",
        "        # TODO: Initialize the encoders\n",
        "        for model, encoder in self.encoders.items():\n",
        "            try:\n",
        "                # Fill in the initialization\n",
        "                pass\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not load encoder for {model}: {e}\")\n",
        "    \n",
        "    def analyze_text(self, text: str, model: str = 'gpt-4') -> Dict:\n",
        "        \"\"\"Analyze tokenization for a given text and model\"\"\"\n",
        "        \n",
        "        if model not in self.encoders or self.encoders[model] is None:\n",
        "            return {'error': f'Model {model} not available'}\n",
        "        \n",
        "        encoder = self.encoders[model]\n",
        "        \n",
        "        # TODO: Tokenize the text\n",
        "        tokens = None  \n",
        "        token_strings = None  \n",
        "        \n",
        "        # Calculate statistics\n",
        "        char_count = len(text)\n",
        "        token_count = len(tokens)\n",
        "        \n",
        "        # TODO: Calculate tokens per character ratio\n",
        "        tokens_per_char = None  # \n",
        "        \n",
        "        # TODO: Find average token length\n",
        "        avg_token_length = None  #\n",
        "        \n",
        "        return {\n",
        "            'model': model,\n",
        "            'text_preview': text[:50] + '...' if len(text) > 50 else text,\n",
        "            'char_count': char_count,\n",
        "            'token_count': token_count,\n",
        "            'tokens_per_char': round(tokens_per_char, 3),\n",
        "            'avg_token_length': round(avg_token_length, 2),\n",
        "            'tokens': tokens[:10],  # First 10 tokens for inspection\n",
        "            'token_strings': token_strings[:10]  # First 10 token strings\n",
        "        }\n",
        "    \n",
        "    def compare_models(self, text: str) -> pd.DataFrame:\n",
        "        \"\"\"Compare tokenization across different models\"\"\"\n",
        "        results = []\n",
        "        \n",
        "        for model in self.encoders.keys():\n",
        "            if self.encoders[model] is not None:\n",
        "                analysis = self.analyze_text(text, model)\n",
        "                if 'error' not in analysis:\n",
        "                    results.append({\n",
        "                        'Model': model,\n",
        "                        'Tokens': analysis['token_count'],\n",
        "                        'Characters': analysis['char_count'],\n",
        "                        'Tokens/Char': analysis['tokens_per_char'],\n",
        "                        'Avg Token Length': analysis['avg_token_length']\n",
        "                    })\n",
        "        \n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "# TODO: Create an instance of the analyzer\n",
        "analyzer = None  #\n",
        "\n",
        "print(\"üîç Tokenization analyzer ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Test Different Text Types (15 minutes)\n",
        "\n",
        "Analyze how different types of content are tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cases for different content types\n",
        "test_texts = {\n",
        "    'simple_english': \"Hello, how are you today? I hope you're having a great day!\",\n",
        "    'technical_text': \"Machine learning algorithms utilize gradient descent optimization to minimize loss functions through backpropagation in neural networks.\",\n",
        "    'code_snippet': '''def calculate_fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)''',\n",
        "    'json_data': '{\"name\": \"John Doe\", \"age\": 30, \"city\": \"New York\", \"skills\": [\"Python\", \"JavaScript\", \"SQL\"]}',\n",
        "    'multilingual': \"Hello, Bonjour, Hola, Guten Tag, „Åì„Çì„Å´„Å°„ÅØ, ‰Ω†Â•Ω, –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ\",\n",
        "    'numbers_symbols': \"Price: $123.45, Temperature: 72¬∞F, Equation: E=mc¬≤, Date: 2024-01-15\",\n",
        "    'long_word': \"Pneumonoultramicroscopicsilicovolcanoconiosisantidisestablishmentarianism\",\n",
        "    'repeated_text': \"test \" * 20,\n",
        "}\n",
        "\n",
        "print(\"üìä Analyzing different text types...\\n\")\n",
        "\n",
        "# TODO: Analyze each text type and store results\n",
        "analysis_results = []\n",
        "\n",
        "for text_type, text in test_texts.items():\n",
        "    print(f\"Analyzing: {text_type}\")\n",
        "    \n",
        "    # TODO: Use the analyzer to analyze the text \n",
        "    result = None  #\n",
        "    \n",
        "    if result and 'error' not in result:\n",
        "        # TODO: Add text_type to the result and append to analysis_results\n",
        "        result['text_type'] = text_type\n",
        "\n",
        "        \n",
        "        print(f\"  - Characters: {result['char_count']}, Tokens: {result['token_count']}, Ratio: {result['tokens_per_char']}\")\n",
        "    else:\n",
        "        print(f\"  - Error analyzing {text_type}\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "# TODO: Create a DataFrame from results for easier analysis\n",
        "if analysis_results:\n",
        "    df_analysis = pd.DataFrame(analysis_results)\n",
        "    print(\"üìà Analysis Summary:\")\n",
        "    print(df_analysis[['text_type', 'char_count', 'token_count', 'tokens_per_char', 'avg_token_length']].to_string(index=False))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No analysis results generated. Check your implementation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Build a Cost Calculator (15 minutes)\n",
        "\n",
        "Create a comprehensive cost calculator for different models and providers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLMCostCalculator:\n",
        "    \"\"\"Calculate costs for different LLM providers and models\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.pricing = {\n",
        "            'asksage': {\n",
        "                \"gpt-5\":      {\"input_per_m\": 1.25, \"output_per_m\": 10.00},\n",
        "                \"gpt-5-mini\": {\"input_per_m\": 0.25, \"output_per_m\": 2.00},\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # Performance characteristics (tokens per second, approximate)\n",
        "        self.performance = {\n",
        "            'asksage': {\n",
        "                'gpt-5': 30,\n",
        "                'gpt-5-mini': 50,\n",
        "            },\n",
        "        }\n",
        "    \n",
        "    def calculate_cost(self, provider: str, model: str, input_tokens: int, output_tokens: int) -> Dict:\n",
        "        \"\"\"Calculate cost for a specific model and token usage\"\"\"\n",
        "        \n",
        "        if provider not in self.pricing or model not in self.pricing[provider]:\n",
        "            return {'error': f'Pricing not available for {provider}/{model}'}\n",
        "        \n",
        "        prices = self.pricing[provider][model]\n",
        "        \n",
        "        # TODO: Calculate costs (prices are per 1K tokens)\n",
        "        input_cost = None \n",
        "        output_cost = None  \n",
        "        total_cost = None \n",
        "        \n",
        "        # TODO: Estimate processing time\n",
        "        tokens_per_second = self.performance.get(provider, {}).get(model, 30)\n",
        "        estimated_time = None \n",
        "        \n",
        "        return {\n",
        "            'provider': provider,\n",
        "            'model': model,\n",
        "            'input_tokens': input_tokens,\n",
        "            'output_tokens': output_tokens,\n",
        "            'input_cost': round(input_cost, 6),\n",
        "            'output_cost': round(output_cost, 6),\n",
        "            'total_cost': round(total_cost, 6),\n",
        "            'estimated_time_seconds': round(estimated_time, 2),\n",
        "            'cost_per_token': round(total_cost / (input_tokens + output_tokens), 8) if (input_tokens + output_tokens) > 0 else 0\n",
        "        }\n",
        "    \n",
        "    def compare_models(self, input_tokens: int, output_tokens: int) -> pd.DataFrame:\n",
        "        \"\"\"Compare costs across all available models\"\"\"\n",
        "        results = []\n",
        "        \n",
        "        for provider, models in self.pricing.items():\n",
        "            for model in models.keys():\n",
        "                cost_data = self.calculate_cost(provider, model, input_tokens, output_tokens)\n",
        "                if 'error' not in cost_data:\n",
        "                    results.append({\n",
        "                        'Provider': provider.title(),\n",
        "                        'Model': model,\n",
        "                        'Total Cost ($)': cost_data['total_cost'],\n",
        "                        'Cost per 1K tokens ($)': round(cost_data['cost_per_token'] * 1000, 6),\n",
        "                        'Est. Time (s)': cost_data['estimated_time_seconds']\n",
        "                    })\n",
        "        \n",
        "        df = pd.DataFrame(results)\n",
        "        return df.sort_values('Total Cost ($)')\n",
        "\n",
        "# TODO: Create an instance of the cost calculator\n",
        "cost_calc = None  \n",
        "\n",
        "print(\"üí∞ Cost calculator ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Generate Cost Comparison Table (15 minutes)\n",
        "\n",
        "Create a comprehensive comparison table for different usage scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define different usage scenarios\n",
        "scenarios = {\n",
        "    'quick_query': {'input_tokens': 50, 'output_tokens': 100, 'description': 'Simple question with short answer'},\n",
        "    'chat_conversation': {'input_tokens': 200, 'output_tokens': 150, 'description': 'Conversational exchange'},\n",
        "    'code_generation': {'input_tokens': 300, 'output_tokens': 500, 'description': 'Generate code from description'},\n",
        "    'document_summary': {'input_tokens': 1000, 'output_tokens': 200, 'description': 'Summarize a document'},\n",
        "    'large_analysis': {'input_tokens': 2000, 'output_tokens': 1000, 'description': 'Complex analysis task'}\n",
        "}\n",
        "\n",
        "print(\"üìä Generating cost comparison tables...\\n\")\n",
        "\n",
        "# TODO: Generate comparison tables for each scenario\n",
        "comparison_results = {}\n",
        "\n",
        "for scenario_name, scenario_data in scenarios.items():\n",
        "    print(f\"Scenario: {scenario_name.replace('_', ' ').title()}\")\n",
        "    print(f\"Description: {scenario_data['description']}\")\n",
        "    print(f\"Input tokens: {scenario_data['input_tokens']}, Output tokens: {scenario_data['output_tokens']}\")\n",
        "    print()\n",
        "    \n",
        "    # TODO: Generate comparison table using cost_calc.compare_models()\n",
        "    comparison_df = None \n",
        "    \n",
        "    if comparison_df is not None and not comparison_df.empty:\n",
        "        print(comparison_df.to_string(index=False))\n",
        "        \n",
        "        # Store for final results\n",
        "        comparison_results[scenario_name] = {\n",
        "            'scenario': scenario_data,\n",
        "            'comparison': comparison_df.to_dict('records')\n",
        "        }\n",
        "    else:\n",
        "        print(\"No comparison data available\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Create Final Results and Visualizations (10 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a comprehensive analysis combining tokenization and costing data\n",
        "\n",
        "def create_final_analysis():\n",
        "    \"\"\"Create the final analysis for exit ticket\"\"\"\n",
        "    \n",
        "    final_results = {\n",
        "        'lab_info': {\n",
        "            'lab_name': 'Tokenization & Costing Analysis',\n",
        "            'completion_time': datetime.now().isoformat(),\n",
        "            'student_name': 'Your Name Here'\n",
        "        },\n",
        "        'tokenization_analysis': {},\n",
        "        'cost_comparisons': comparison_results,\n",
        "        'key_insights': [],\n",
        "        'recommendations': []\n",
        "    }\n",
        "    \n",
        "    # Add tokenization analysis if available\n",
        "    if 'analysis_results' in globals() and analysis_results:\n",
        "        # TODO: Process tokenization results for inclusion\n",
        "        tokenization_summary = []\n",
        "        for result in analysis_results:\n",
        "            tokenization_summary.append({\n",
        "                'text_type': result['text_type'],\n",
        "                'tokens_per_char': result['tokens_per_char'],\n",
        "                'efficiency': 'High' if result['tokens_per_char'] < 0.8 else 'Medium' if result['tokens_per_char'] < 1.2 else 'Low'\n",
        "            })\n",
        "        \n",
        "        final_results['tokenization_analysis'] = {\n",
        "            'text_types_analyzed': len(tokenization_summary),\n",
        "            'summary': tokenization_summary\n",
        "        }\n",
        "    \n",
        "    # TODO: Add key insights based on analysis\n",
        "    insights = [\n",
        "        # Add your insights here based on the analysis\n",
        "        \"Different text types have varying tokenization efficiency\",\n",
        "        \"Code and technical text typically require more tokens per character\",\n",
        "        \"Model costs vary significantly across providers\",\n",
        "        # TODO: Add more insights based on your findings\n",
        "    ]\n",
        "    \n",
        "    final_results['key_insights'] = insights\n",
        "    \n",
        "    # TODO: Add recommendations\n",
        "    recommendations = [\n",
        "        \"Use GPT-5-mini for cost-sensitive applications\",\n",
        "        \"Reserve GPT-5 for complex reasoning tasks\",\n",
        "        # TODO: Add more recommendations\n",
        "    ]\n",
        "    \n",
        "    final_results['recommendations'] = recommendations\n",
        "    \n",
        "    return final_results\n",
        "\n",
        "# TODO: Generate the final analysis\n",
        "final_analysis = create_final_analysis()\n",
        "\n",
        "# TODO: Save to results/costing.json for exit ticket\n",
        "with open('results/costing.json', 'w') as f:\n",
        "    json.dump(final_analysis, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Final analysis saved to results/costing.json\")\n",
        "print(\"\\nüìã Exit Ticket: Submit the file 'results/costing.json'\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nüéØ Lab Summary:\")\n",
        "print(f\"- Analyzed {len(test_texts)} different text types\")\n",
        "print(f\"- Compared costs across {len(scenarios)} usage scenarios\")\n",
        "print(f\"- Generated {len(final_analysis['key_insights'])} key insights\")\n",
        "print(f\"- Provided {len(final_analysis['recommendations'])} recommendations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Lab Completion Checklist\n",
        "\n",
        "Mark each item as complete:\n",
        "\n",
        "- [ ] **Task 1**: Built tokenization analyzer with multiple encoders\n",
        "- [ ] **Task 2**: Analyzed different text types (8 categories)\n",
        "- [ ] **Task 3**: Created comprehensive cost calculator\n",
        "- [ ] **Task 4**: Generated cost comparison tables for 5 scenarios\n",
        "- [ ] **Task 5**: Created final analysis with insights and recommendations\n",
        "- [ ] **Exit Ticket**: Saved results to `results/costing.json`\n",
        "\n",
        "## üìù Exit Ticket Submission\n",
        "\n",
        "Submit the file `results/costing.json` that contains your complete analysis.\n",
        "\n",
        "## üöÄ What You've Learned\n",
        "\n",
        "- How different content types affect tokenization efficiency\n",
        "- Cost structures across major LLM providers\n",
        "- Performance vs. cost trade-offs for different models\n",
        "- How to estimate costs for production applications\n",
        "\n",
        "## üîÑ What's Next\n",
        "\n",
        "In Lab 3, you'll build a cross-provider ping CLI that puts this cost knowledge into practice with real API calls!\n",
        "\n",
        "## üÜò Troubleshooting\n",
        "\n",
        "**Common Issues:**\n",
        "- **Tokenizer errors**: Make sure tiktoken is properly installed\n",
        "- **Missing results**: Check that all TODO sections are completed\n",
        "- **File save errors**: Ensure the results directory exists\n",
        "- **Analysis errors**: Verify your tokenization analyzer is properly initialized"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}