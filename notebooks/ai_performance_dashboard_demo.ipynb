{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Performance Dashboard Demo\n",
        "\n",
        "## Learning Objectives\n",
        "- Real-time monitoring of AI workflows in production\n",
        "- Cost optimization and quality metrics\n",
        "- Performance analytics that drive business decisions\n",
        "- Production observability patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "!pip install asksageclient pip_system_certs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import datetime\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "import random\n",
        "# Import our AskSage client\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "# Get API credentials from Google Colab secrets\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('ASKSAGE_API_KEY')\n",
        "email = userdata.get('ASKSAGE_EMAIL')\n",
        "\n",
        "# Initialize client and tokenizer\n",
        "client = AskSageClient(api_key=api_key, email=email)\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(\"AskSage client initialized successfully\")\n",
        "print(\"Ready to showcase AI capabilities...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate production metrics\n",
        "def generate_metrics():\n",
        "    base_time = datetime.now() - timedelta(hours=24)\n",
        "    metrics = []\n",
        "    \n",
        "    for i in range(24):  # 24 hours of data\n",
        "        timestamp = base_time + timedelta(hours=i)\n",
        "        \n",
        "        # Simulate realistic AI workflow metrics\n",
        "        metric = {\n",
        "            \"timestamp\": timestamp.isoformat(),\n",
        "            \"requests_per_hour\": random.randint(150, 800),\n",
        "            \"avg_response_time_ms\": random.randint(800, 3200),\n",
        "            \"success_rate\": round(random.uniform(0.92, 0.99), 3),\n",
        "            \"cost_per_hour\": round(random.uniform(12.50, 45.80), 2),\n",
        "            \"token_usage\": random.randint(50000, 200000),\n",
        "            \"error_count\": random.randint(0, 15),\n",
        "            \"quality_score\": round(random.uniform(0.85, 0.98), 3)\n",
        "        }\n",
        "        metrics.append(metric)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "production_metrics = generate_metrics()\n",
        "print(f\"Generated {len(production_metrics)} hours of production metrics\")\n",
        "print(f\"Sample metric: {production_metrics[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI analyzes performance trends\n",
        "def analyze_performance_trends(metrics_data):\n",
        "    analysis_prompt = f\"\"\"\n",
        "    Analyze these AI system performance metrics and identify trends:\n",
        "    \n",
        "    {json.dumps(metrics_data[-6:], indent=2)}  # Last 6 hours\n",
        "    \n",
        "    Provide performance analysis:\n",
        "    {{\n",
        "      \"performance_summary\": {{\n",
        "        \"avg_response_time\": \"number\",\n",
        "        \"success_rate\": \"number\",\n",
        "        \"total_cost\": \"number\",\n",
        "        \"quality_trend\": \"improving|stable|declining\"\n",
        "      }},\n",
        "      \"alerts\": [\n",
        "        {{\n",
        "          \"type\": \"performance|cost|quality|error\",\n",
        "          \"severity\": \"critical|warning|info\",\n",
        "          \"message\": \"string\",\n",
        "          \"recommendation\": \"string\"\n",
        "        }}\n",
        "      ],\n",
        "      \"optimization_opportunities\": [\n",
        "        {{\n",
        "          \"area\": \"string\",\n",
        "          \"potential_savings\": \"string\",\n",
        "          \"implementation\": \"string\"\n",
        "        }}\n",
        "      ],\n",
        "      \"business_impact\": \"string\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "    \n",
        "    # Test GPT-5-mini\n",
        "    print(\"=== TESTING GPT-5-mini ===\")\n",
        "\n",
        "    response = client.query(\n",
        "        message=analysis_prompt,\n",
        "        system_prompt=\"You are an expert in data visualization and performance monitoring. Create comprehensive dashboards with actionable insights.\",\n",
        "        temperature=0.2,\n",
        "        model=\"gpt-5-mini\",\n",
        "        live=0,\n",
        "        limit_references=0,\n",
        "    )\n",
        "\n",
        "    \n",
        "    return response.get(\"message\").strip()\n",
        "\n",
        "print(\"=== PERFORMANCE TREND ANALYSIS ===\")\n",
        "trend_analysis = analyze_performance_trends(production_metrics)\n",
        "print(trend_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cost optimization analysis\n",
        "def optimize_costs(metrics_data):\n",
        "    total_cost = sum(m['cost_per_hour'] for m in metrics_data)\n",
        "    total_tokens = sum(m['token_usage'] for m in metrics_data)\n",
        "    avg_quality = sum(m['quality_score'] for m in metrics_data) / len(metrics_data)\n",
        "    \n",
        "    optimization_prompt = f\"\"\"\n",
        "    Analyze AI system costs and suggest optimizations:\n",
        "    \n",
        "    Current metrics:\n",
        "    - Total 24h cost: ${total_cost:.2f}\n",
        "    - Total tokens: {total_tokens:,}\n",
        "    - Average quality score: {avg_quality:.3f}\n",
        "    - Cost per token: ${total_cost/total_tokens:.6f}\n",
        "    \n",
        "    Recent hourly data:\n",
        "    {json.dumps(metrics_data[-3:], indent=2)}\n",
        "    \n",
        "    Provide cost optimization strategy:\n",
        "    {{\n",
        "      \"current_efficiency\": {{\n",
        "        \"cost_per_request\": \"number\",\n",
        "        \"quality_per_dollar\": \"number\",\n",
        "        \"efficiency_rating\": \"excellent|good|fair|poor\"\n",
        "      }},\n",
        "      \"optimization_strategies\": [\n",
        "        {{\n",
        "          \"strategy\": \"string\",\n",
        "          \"estimated_savings\": \"string\",\n",
        "          \"quality_impact\": \"positive|neutral|negative\",\n",
        "          \"implementation_effort\": \"low|medium|high\"\n",
        "        }}\n",
        "      ],\n",
        "      \"recommended_actions\": [\n",
        "        {{\n",
        "          \"action\": \"string\",\n",
        "          \"priority\": \"high|medium|low\",\n",
        "          \"timeline\": \"string\"\n",
        "        }}\n",
        "      ],\n",
        "      \"projected_savings\": \"string\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "        \n",
        "    # Test GPT-5-mini\n",
        "    print(\"=== TESTING GPT-5-mini ===\")\n",
        "\n",
        "    response = client.query(\n",
        "        message=optimization_prompt,\n",
        "        system_prompt=\"You are an expert in data visualization and performance monitoring. Create comprehensive dashboards with actionable insights.\",\n",
        "        temperature=0.2,\n",
        "        model=\"gpt-5-mini\",\n",
        "        live=0,\n",
        "        limit_references=0,\n",
        "    )\n",
        "\n",
        "    \n",
        "    return response.get(\"message\").strip()\n",
        "\n",
        "print(\"\\n=== COST OPTIMIZATION ANALYSIS ===\")\n",
        "cost_analysis = optimize_costs(production_metrics)\n",
        "print(cost_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality monitoring and alerts\n",
        "def monitor_quality_metrics(metrics_data):\n",
        "    recent_metrics = metrics_data[-6:]  # Last 6 hours\n",
        "    quality_scores = [m['quality_score'] for m in recent_metrics]\n",
        "    success_rates = [m['success_rate'] for m in recent_metrics]\n",
        "    \n",
        "    quality_prompt = f\"\"\"\n",
        "    Monitor AI system quality and generate alerts:\n",
        "    \n",
        "    Recent quality scores: {quality_scores}\n",
        "    Recent success rates: {success_rates}\n",
        "    \n",
        "    Quality thresholds:\n",
        "    - Critical: Quality < 0.85 or Success < 0.90\n",
        "    - Warning: Quality < 0.90 or Success < 0.95\n",
        "    - Good: Quality >= 0.95 and Success >= 0.98\n",
        "    \n",
        "    Generate quality monitoring report:\n",
        "    {{\n",
        "      \"quality_status\": \"excellent|good|warning|critical\",\n",
        "      \"current_quality\": \"number\",\n",
        "      \"quality_trend\": \"improving|stable|declining\",\n",
        "      \"alerts\": [\n",
        "        {{\n",
        "          \"level\": \"critical|warning|info\",\n",
        "          \"metric\": \"string\",\n",
        "          \"current_value\": \"number\",\n",
        "          \"threshold\": \"number\",\n",
        "          \"action_required\": \"string\"\n",
        "        }}\n",
        "      ],\n",
        "      \"quality_insights\": [\n",
        "        \"list of insights about quality patterns\"\n",
        "      ],\n",
        "      \"recommendations\": [\n",
        "        \"list of quality improvement recommendations\"\n",
        "      ]\n",
        "    }}\n",
        "    \"\"\"\n",
        "        \n",
        "    # Test GPT-5-mini\n",
        "    print(\"=== TESTING GPT-5-mini ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = client.query(\n",
        "        message=quality_prompt,\n",
        "        system_prompt=\"You are an expert in data visualization and performance monitoring. Create comprehensive dashboards with actionable insights.\",\n",
        "        temperature=0.2,\n",
        "        model=\"gpt-5-mini\",\n",
        "        live=0,\n",
        "        limit_references=0,\n",
        "    )\n",
        "\n",
        "    \n",
        "    return response.get(\"message\").strip()\n",
        "\n",
        "print(\"\\n=== QUALITY MONITORING REPORT ===\")\n",
        "quality_report = monitor_quality_metrics(production_metrics)\n",
        "print(quality_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executive dashboard summary\n",
        "def generate_executive_summary(metrics_data):\n",
        "    total_requests = sum(m['requests_per_hour'] for m in metrics_data)\n",
        "    total_cost = sum(m['cost_per_hour'] for m in metrics_data)\n",
        "    avg_quality = sum(m['quality_score'] for m in metrics_data) / len(metrics_data)\n",
        "    avg_response_time = sum(m['avg_response_time_ms'] for m in metrics_data) / len(metrics_data)\n",
        "    \n",
        "    summary_prompt = f\"\"\"\n",
        "    Create executive dashboard summary for AI system performance:\n",
        "    \n",
        "    24-hour summary:\n",
        "    - Total requests processed: {total_requests:,}\n",
        "    - Total cost: ${total_cost:.2f}\n",
        "    - Average quality score: {avg_quality:.3f}\n",
        "    - Average response time: {avg_response_time:.0f}ms\n",
        "    \n",
        "    Generate executive summary:\n",
        "    {{\n",
        "      \"system_health\": \"excellent|good|concerning|critical\",\n",
        "      \"key_metrics\": {{\n",
        "        \"uptime\": \"string\",\n",
        "        \"throughput\": \"string\",\n",
        "        \"cost_efficiency\": \"string\",\n",
        "        \"user_satisfaction\": \"string\"\n",
        "      }},\n",
        "      \"business_impact\": {{\n",
        "        \"revenue_supported\": \"string\",\n",
        "        \"cost_savings\": \"string\",\n",
        "        \"productivity_gains\": \"string\"\n",
        "      }},\n",
        "      \"strategic_recommendations\": [\n",
        "        {{\n",
        "          \"recommendation\": \"string\",\n",
        "          \"business_value\": \"string\",\n",
        "          \"investment_required\": \"string\"\n",
        "        }}\n",
        "      ],\n",
        "      \"next_review_focus\": [\"list of areas to monitor\"]\n",
        "    }}\n",
        "    \"\"\"\n",
        "        \n",
        "    # Test GPT-5-mini\n",
        "    print(\"=== TESTING GPT-5-mini ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = client.query(\n",
        "        message=summary_prompt,\n",
        "        system_prompt=\"You are an expert in data visualization and performance monitoring. Create comprehensive dashboards with actionable insights.\",\n",
        "        temperature=0.2,\n",
        "        model=\"gpt-5-mini\",\n",
        "        live=0,\n",
        "        limit_references=0,\n",
        "    )\n",
        "\n",
        "    \n",
        "    return response.get(\"message\").strip()\n",
        "\n",
        "print(\"\\n=== EXECUTIVE DASHBOARD SUMMARY ===\")\n",
        "executive_summary = generate_executive_summary(production_metrics)\n",
        "print(executive_summary)\n",
        "\n",
        "print(\"\\nâœ… AI Performance Dashboard Demo completed!\")\n",
        "print(\"ðŸ“Š Real-time monitoring â†’ Cost optimization â†’ Business insights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Dashboard Features\n",
        "\n",
        "**Real-Time Monitoring**\n",
        "- Performance metrics tracking\n",
        "- Quality score monitoring\n",
        "- Cost analysis and optimization\n",
        "\n",
        "**Business Intelligence**\n",
        "- Executive summaries\n",
        "- Strategic recommendations\n",
        "- ROI and efficiency metrics\n",
        "\n",
        "**Proactive Alerting**\n",
        "- Quality threshold monitoring\n",
        "- Cost anomaly detection\n",
        "- Performance degradation alerts\n",
        "\n",
        "**Production Readiness**\n",
        "- Scalable monitoring architecture\n",
        "- Business-driven metrics\n",
        "- Decision-support analytics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}