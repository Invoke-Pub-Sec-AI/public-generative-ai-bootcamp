{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Provider Landscape & Limits - Lab\n",
        "\n",
        "**Hands-on**: toggle 2 providers, compare latency and tokens\n",
        "**Deliverable**: two-row comparison log\n",
        "\n",
        "## Instructions\n",
        "\n",
        "In this lab, you will compare two AI providers by measuring their latency and token usage for the same prompt. You'll create a systematic comparison that highlights the performance differences between providers.\n",
        "\n",
        "## Success Criteria\n",
        "- Successfully connect to 2 different AI providers\n",
        "- Measure response latency for identical prompts\n",
        "- Count input and output tokens for cost analysis\n",
        "- Create a structured comparison log\n",
        "- Document findings in a clear two-row format\n",
        "\n",
        "## Recommended Provider Pairs\n",
        "- OpenAI GPT-3.5-turbo vs Claude-3-haiku (speed comparison)\n",
        "- OpenAI GPT-4 vs Gemini Pro (quality vs cost)\n",
        "- Any cloud provider vs local model (latency vs privacy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Install required packages for Google Colab compatibility\n",
        "# Install: openai, anthropic, google-generativeai, langchain, langchain-openai, langchain-anthropic, langchain-google-genai\n",
        "# Also install: pandas for data handling and time measurement utilities\n",
        "\n",
        "# TODO: Import necessary modules\n",
        "# Import time, datetime for latency measurement\n",
        "# Import os for environment variables\n",
        "# Import pandas for creating comparison tables\n",
        "# Import the specific provider clients you plan to test\n",
        "# Import tiktoken or similar for token counting (OpenAI)\n",
        "\n",
        "# TODO: Print confirmation that all packages are installed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Configure Provider Credentials\n",
        "\n",
        "Set up API keys and initialize clients for your chosen providers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Set up environment variables for API keys\n",
        "# Use os.environ to set OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY as needed\n",
        "# Or prompt user to input their keys securely\n",
        "\n",
        "# TODO: Initialize clients for your two chosen providers\n",
        "# Example: ChatOpenAI, ChatAnthropic, ChatGoogleGenerativeAI\n",
        "# Set consistent parameters: temperature=0 for reproducible results\n",
        "# Store clients in variables like provider_1 and provider_2\n",
        "\n",
        "# TODO: Print confirmation of successful client initialization\n",
        "# Include which models/providers you're comparing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Test Prompts\n",
        "\n",
        "Create standardized prompts for consistent comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Define 3-5 test prompts of varying complexity\n",
        "# Include:\n",
        "# - Short prompt (10-20 words): simple question\n",
        "# - Medium prompt (50-100 words): explanation request  \n",
        "# - Long prompt (200+ words): complex analysis task\n",
        "# - Code generation prompt: ask for a specific function\n",
        "# - Creative prompt: story or creative writing task\n",
        "\n",
        "# TODO: Store prompts in a list for easy iteration\n",
        "# TODO: Print the prompts with their expected complexity levels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Token Counting Setup\n",
        "\n",
        "Implement token counting for cost analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Create a function to count tokens for different providers\n",
        "# For OpenAI: use tiktoken with appropriate encoding (cl100k_base for GPT-3.5/4)\n",
        "# For other providers: create approximate counting or use len(text.split()) * 1.3 as estimate\n",
        "# Function should take (text, provider_name) and return token count\n",
        "\n",
        "# TODO: Test your token counting function with sample text\n",
        "# Verify it works for both input and output text\n",
        "# Print test results to confirm accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Latency Measurement Function\n",
        "\n",
        "Create a function to measure response time accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Create a function to measure provider response time\n",
        "# Function should:\n",
        "# - Take (client, prompt) as parameters\n",
        "# - Record start time before API call\n",
        "# - Make the API call and get response\n",
        "# - Record end time after response received\n",
        "# - Calculate latency in seconds\n",
        "# - Return (response_text, latency_seconds)\n",
        "\n",
        "# TODO: Add error handling for API failures\n",
        "# TODO: Test the function with a simple prompt on both providers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run Comparison Tests\n",
        "\n",
        "Execute systematic tests across all prompts and both providers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Create a comprehensive comparison loop\n",
        "# For each test prompt:\n",
        "#   - Test Provider 1: measure latency, count input/output tokens\n",
        "#   - Test Provider 2: measure latency, count input/output tokens\n",
        "#   - Store results in structured format (list of dictionaries)\n",
        "# \n",
        "# Results structure should include:\n",
        "# - prompt_id/name\n",
        "# - provider_name\n",
        "# - input_tokens\n",
        "# - output_tokens\n",
        "# - latency_seconds\n",
        "# - response_preview (first 100 chars)\n",
        "\n",
        "# TODO: Add progress indicators (print statements) during testing\n",
        "# TODO: Include small delays between requests to respect rate limits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Create Comparison Log\n",
        "\n",
        "Format your results into the required two-row comparison log."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Process your test results into summary statistics\n",
        "# Calculate for each provider:\n",
        "# - Average latency across all prompts\n",
        "# - Total input tokens used\n",
        "# - Total output tokens generated\n",
        "# - Average tokens per second (throughput)\n",
        "# - Estimated cost (if you have pricing data)\n",
        "\n",
        "# TODO: Create a pandas DataFrame with exactly 2 rows\n",
        "# Columns: Provider, Avg_Latency_Sec, Total_Input_Tokens, Total_Output_Tokens, Tokens_Per_Sec, Notes\n",
        "# Row 1: Provider 1 statistics\n",
        "# Row 2: Provider 2 statistics\n",
        "\n",
        "# TODO: Display the comparison table clearly\n",
        "# TODO: Add a \"Winner\" column indicating which provider performed better in each metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Detailed Analysis\n",
        "\n",
        "Analyze the results and document your findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Create visualizations of your comparison (optional)\n",
        "# - Bar chart of average latency per provider\n",
        "# - Scatter plot of latency vs response length\n",
        "# - Token usage comparison chart\n",
        "\n",
        "# TODO: Calculate key insights:\n",
        "# - Which provider is faster?\n",
        "# - Which provider uses tokens more efficiently?\n",
        "# - Are there specific prompt types where one provider excels?\n",
        "# - What's the cost difference between providers?\n",
        "\n",
        "# TODO: Print summary insights in bullet point format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Final Deliverable\n",
        "\n",
        "Create your final two-row comparison log with conclusions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Create the final deliverable format\n",
        "# Print a clean, professional two-row comparison table\n",
        "# Include:\n",
        "# - Provider names and models tested\n",
        "# - Key performance metrics\n",
        "# - Timestamp of testing\n",
        "# - Your recommendation for different use cases\n",
        "\n",
        "# TODO: Save results to a CSV file for future reference\n",
        "# TODO: Write a brief conclusion paragraph explaining:\n",
        "# - Which provider you'd choose for speed-critical applications\n",
        "# - Which provider offers better value for money\n",
        "# - Any limitations or caveats from your testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus Challenges (If Time Permits)\n",
        "\n",
        "Extend your analysis with additional comparisons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO BONUS 1: Test streaming vs non-streaming latency\n",
        "# Compare time-to-first-token vs time-to-completion\n",
        "# Measure user-perceived responsiveness\n",
        "\n",
        "# TODO BONUS 2: Test with different model parameters\n",
        "# Try temperature=0 vs temperature=0.7\n",
        "# Test max_tokens limits and their impact\n",
        "\n",
        "# TODO BONUS 3: Add a third provider for comparison\n",
        "# Create a three-row comparison log\n",
        "# Test local model vs cloud providers if possible"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deliverable Checklist\n",
        "\n",
        "Before submitting, ensure you have:\n",
        "\n",
        "- [ ] Successfully connected to 2 different AI providers\n",
        "- [ ] Tested multiple prompts of varying complexity\n",
        "- [ ] Measured latency accurately for all tests\n",
        "- [ ] Counted input and output tokens for cost analysis\n",
        "- [ ] Created a clean two-row comparison table\n",
        "- [ ] Included provider names, models, and key metrics\n",
        "- [ ] Documented which provider performs better for different use cases\n",
        "- [ ] Added timestamp and testing conditions\n",
        "- [ ] Written clear conclusions and recommendations\n",
        "\n",
        "**Final Deliverable**: A two-row comparison log showing latency and token usage differences between your chosen providers, with analysis and recommendations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}