{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "efc6007b1a70417aa2b57fd60dbf19f5",
      "metadata": {},
      "source": [
        "# Lab 01: LLM Deployment Fundamentals\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand LLM deployment architectures\n",
        "- Implement basic model serving endpoints\n",
        "- Handle deployment configurations\n",
        "- Monitor deployment health"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926d54ce07da4947bf66c2fa9206415e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "\n",
        "# Web framework\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4746112b1614fd79d1cf00102749003",
      "metadata": {},
      "source": [
        "## Part 1: Deployment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c32630cc02648ddac1dbf0b11c0ec28",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Model deployment configuration\"\"\"\n",
        "    model_name: str\n",
        "    model_version: str\n",
        "    max_batch_size: int = 32\n",
        "    timeout_seconds: int = 30\n",
        "\n",
        "class PredictionRequest(BaseModel):\n",
        "    text: str\n",
        "    max_length: int = 100\n",
        "    temperature: float = 0.7\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    text: str\n",
        "    model_version: str\n",
        "    processing_time: float"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "209935c84fd34544acf51f483742e76c",
      "metadata": {},
      "source": [
        "## Part 2: Model Server Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ad11272737456fbeb7855968d446bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelServer:\n",
        "    \"\"\"LLM Model Server\"\"\"\n",
        "    \n",
        "    def __init__(self, model_config: ModelConfig):\n",
        "        self.config = model_config\n",
        "        self.app = FastAPI(title=\"LLM Server\")\n",
        "        self.setup_routes()\n",
        "        \n",
        "    def setup_routes(self):\n",
        "        @self.app.get(\"/health\")\n",
        "        async def health():\n",
        "            return {\"status\": \"healthy\", \"model\": self.config.model_name}\n",
        "        \n",
        "        @self.app.post(\"/predict\")\n",
        "        async def predict(request: PredictionRequest):\n",
        "            start = time.time()\n",
        "            # Mock prediction\n",
        "            result = f\"Response for: {request.text[:30]}...\"\n",
        "            return PredictionResponse(\n",
        "                text=result,\n",
        "                model_version=self.config.model_version,\n",
        "                processing_time=time.time() - start\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93e34807074498996bbf92fd0f201cf",
      "metadata": {},
      "source": [
        "## Part 3: Load Balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8aa8f496f9545259c553cb08ee3b3fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoadBalancer:\n",
        "    \"\"\"Simple round-robin load balancer\"\"\"\n",
        "    \n",
        "    def __init__(self, backends: List[str]):\n",
        "        self.backends = backends\n",
        "        self.current = 0\n",
        "        \n",
        "    def get_next_backend(self) -> str:\n",
        "        backend = self.backends[self.current % len(self.backends)]\n",
        "        self.current += 1\n",
        "        return backend"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1590e989308d4d6b908654b49939825b",
      "metadata": {},
      "source": [
        "## Part 4: Deployment Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4995abf858d648c39376e5132ba3b592",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeploymentMonitor:\n",
        "    \"\"\"Monitor deployment metrics\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            \"requests\": 0,\n",
        "            \"errors\": 0,\n",
        "            \"total_latency\": 0.0\n",
        "        }\n",
        "    \n",
        "    def record_request(self, latency: float, error: bool = False):\n",
        "        self.metrics[\"requests\"] += 1\n",
        "        self.metrics[\"total_latency\"] += latency\n",
        "        if error:\n",
        "            self.metrics[\"errors\"] += 1\n",
        "    \n",
        "    def get_stats(self) -> Dict:\n",
        "        total = self.metrics[\"requests\"]\n",
        "        if total == 0:\n",
        "            return {\"requests\": 0, \"error_rate\": 0, \"avg_latency\": 0}\n",
        "        \n",
        "        return {\n",
        "            \"requests\": total,\n",
        "            \"error_rate\": self.metrics[\"errors\"] / total,\n",
        "            \"avg_latency\": self.metrics[\"total_latency\"] / total\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "779797e095c8434ea49863ddde94b387",
      "metadata": {},
      "source": [
        "## Part 5: Example Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f398647270ae4b8fb143c969e8e505a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure and test deployment\n",
        "config = ModelConfig(\n",
        "    model_name=\"llm-v1\",\n",
        "    model_version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Initialize components\n",
        "server = ModelServer(config)\n",
        "balancer = LoadBalancer([\"http://localhost:8001\", \"http://localhost:8002\"])\n",
        "monitor = DeploymentMonitor()\n",
        "\n",
        "# Simulate requests\n",
        "print(\"Simulating deployment...\")\n",
        "for i in range(10):\n",
        "    backend = balancer.get_next_backend()\n",
        "    latency = 0.1 + (i % 3) * 0.05\n",
        "    monitor.record_request(latency)\n",
        "    \n",
        "print(\"\\nDeployment Stats:\")\n",
        "print(json.dumps(monitor.get_stats(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89efabf4bd04c6a8ef8168f05795204",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. Implement request batching for efficiency\n",
        "2. Add circuit breaker pattern for fault tolerance\n",
        "3. Create a canary deployment strategy\n",
        "\n",
        "## Summary\n",
        "\n",
        "You learned:\n",
        "- Building model serving APIs\n",
        "- Load balancing strategies\n",
        "- Monitoring production deployments\n",
        "- Key deployment patterns"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}