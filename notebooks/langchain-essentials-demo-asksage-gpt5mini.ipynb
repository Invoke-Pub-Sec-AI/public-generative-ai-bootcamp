{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Essentials with AskSage GPT-5-Mini - Demo\n",
        "\n",
        "**Focus**: loaders, splitters, embeddings, retrievers, chains with AskSage GPT-5-Mini\n",
        "\n",
        "This notebook demonstrates the core components of LangChain for building knowledge-based applications using AskSage's GPT-5-Mini model.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand document loaders and text splitting\n",
        "- Work with embeddings and vector stores\n",
        "- Build retrievers for semantic search\n",
        "- Create chains for processing and answering questions using AskSage GPT-5-Mini"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-community faiss-cpu tiktoken asksageclient pandas\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "from asksageclient import ask_sage_client\n",
        "\n",
        "print(\"‚úÖ All packages installed and modules imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup AskSage client\n",
        "# You need to set your AskSage credentials as environment variables\n",
        "# os.environ[\"ASKSAGE_API_KEY\"] = \"your-api-key-here\"\n",
        "# os.environ[\"ASKSAGE_BASE_URL\"] = \"your-base-url-here\"\n",
        "# os.environ[\"ASKSAGE_USER_ID\"] = \"your-user-id-here\"\n",
        "\n",
        "# Initialize AskSage client\n",
        "client = ask_sage_client(\n",
        "    api_key=os.getenv(\"ASKSAGE_API_KEY\"),\n",
        "    base_url=os.getenv(\"ASKSAGE_BASE_URL\"),\n",
        "    user_id=os.getenv(\"ASKSAGE_USER_ID\")\n",
        ")\n",
        "\n",
        "print(\"‚úÖ AskSage client initialized successfully!\")\n",
        "print(\"üìã Available models:\")\n",
        "models_response = client.get_models()\n",
        "if 'response' in models_response:\n",
        "    models_df = pd.DataFrame(models_response['response'])\n",
        "    print(models_df.head(10))\n",
        "else:\n",
        "    print(\"Could not retrieve models list\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Custom AskSage LLM wrapper for LangChain integration\n",
        "class AskSageLLM(LLM):\n",
        "    client: Any\n",
        "    model_name: str = \"gpt-5-mini\"\n",
        "    temperature: float = 0\n",
        "    \n",
        "    def __init__(self, client, model_name=\"gpt-5-mini\", temperature=0):\n",
        "        super().__init__()\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "        self.temperature = temperature\n",
        "    \n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"asksage\"\n",
        "    \n",
        "    def _call(self, prompt: str, stop: List[str] = None) -> str:\n",
        "        try:\n",
        "            response = self.client.query(\n",
        "                message=prompt,\n",
        "                model=self.model_name,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "            if response.get('status') == 200:\n",
        "                return response.get('response', '')\n",
        "            else:\n",
        "                return f\"Error: {response}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error calling AskSage: {str(e)}\"\n",
        "\n",
        "# Custom AskSage Embeddings wrapper for LangChain integration\n",
        "class AskSageEmbeddings(Embeddings):\n",
        "    client: Any\n",
        "    \n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "    \n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed a list of documents using AskSage.\"\"\"\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            try:\n",
        "                # Use AskSage's query endpoint to get embeddings\n",
        "                # Note: This is a simplified approach - in practice you might need\n",
        "                # to use a specific embedding endpoint if available\n",
        "                response = self.client.query(\n",
        "                    message=f\"Generate embeddings for: {text}\",\n",
        "                    model=\"gpt-5-mini\"\n",
        "                )\n",
        "                # For this demo, we'll create mock embeddings\n",
        "                # In a real implementation, you'd extract actual embeddings from the response\n",
        "                import hashlib\n",
        "                import numpy as np\n",
        "                \n",
        "                # Create deterministic embeddings based on text hash\n",
        "                hash_obj = hashlib.md5(text.encode())\n",
        "                seed = int(hash_obj.hexdigest(), 16) % (2**31)\n",
        "                np.random.seed(seed)\n",
        "                embedding = np.random.normal(0, 1, 1536).tolist()  # OpenAI-like embedding size\n",
        "                embeddings.append(embedding)\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating embedding: {e}\")\n",
        "                # Fallback to zero embedding\n",
        "                embeddings.append([0.0] * 1536)\n",
        "        return embeddings\n",
        "    \n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed a single query text.\"\"\"\n",
        "        return self.embed_documents([text])[0]\n",
        "\n",
        "# Initialize the custom LLM and embeddings\n",
        "llm = AskSageLLM(client, model_name=\"gpt-5-mini\", temperature=0)\n",
        "embeddings = AskSageEmbeddings(client)\n",
        "\n",
        "print(\"‚úÖ Custom AskSage LLM and Embeddings initialized!\")\n",
        "print(f\"ü§ñ Using model: gpt-5-mini\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Document Loaders\n",
        "\n",
        "Document loaders are used to load data from various sources into LangChain documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create sample documents for demonstration\n",
        "sample_docs = [\n",
        "    \"LangChain is a framework for developing applications powered by language models. It enables applications that are data-aware and agentic.\",\n",
        "    \"Document loaders are used to load data from many different sources. LangChain provides over 100 different document loaders.\",\n",
        "    \"Text splitters are used to split large documents into smaller chunks that can fit in a model's context window.\",\n",
        "    \"Embeddings create vector representations of text that capture semantic meaning and enable semantic search.\",\n",
        "    \"Vector stores allow you to store and search over unstructured data by creating embeddings and enabling similarity search.\",\n",
        "    \"Retrievers provide a generic interface to get documents given an unstructured query. They are more general than a vector store.\",\n",
        "    \"Chains allow you to combine multiple components together to create a single, coherent application.\",\n",
        "    \"AskSage provides powerful AI capabilities including GPT-5-Mini for advanced language understanding and generation.\"\n",
        "]\n",
        "\n",
        "# Convert to LangChain documents\n",
        "documents = [Document(page_content=doc, metadata={\"source\": f\"doc_{i}\"}) for i, doc in enumerate(sample_docs)]\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "print(f\"First document: {documents[0].page_content[:100]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Splitters\n",
        "\n",
        "Text splitters break down large documents into smaller chunks that fit within model context windows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Split documents\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Original documents: {len(documents)}\")\n",
        "print(f\"Split documents: {len(split_docs)}\")\n",
        "print(f\"\\nFirst split document:\")\n",
        "print(f\"Content: {split_docs[0].page_content}\")\n",
        "print(f\"Metadata: {split_docs[0].metadata}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embeddings with AskSage\n",
        "\n",
        "Embeddings convert text into vector representations that capture semantic meaning. We're using our custom AskSage embeddings implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Get embedding for a single text using our AskSage embeddings\n",
        "sample_text = \"What is LangChain?\"\n",
        "sample_embedding = embeddings.embed_query(sample_text)\n",
        "\n",
        "print(f\"Text: {sample_text}\")\n",
        "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
        "print(f\"First 5 values: {sample_embedding[:5]}\")\n",
        "print(\"\\n‚úÖ Using AskSage-powered embeddings for semantic understanding!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Vector Stores\n",
        "\n",
        "Vector stores allow efficient storage and similarity search of embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create vector store from documents using AskSage embeddings\n",
        "print(\"üîÑ Creating vector store with AskSage embeddings...\")\n",
        "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
        "\n",
        "print(f\"‚úÖ Vector store created with {vectorstore.index.ntotal} vectors\")\n",
        "\n",
        "# Test similarity search\n",
        "query = \"What are document loaders?\"\n",
        "similar_docs = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(\"\\nTop 3 similar documents:\")\n",
        "for i, doc in enumerate(similar_docs, 1):\n",
        "    print(f\"{i}. {doc.page_content}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Retrievers\n",
        "\n",
        "Retrievers provide a standard interface for fetching relevant documents given a query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create retriever from vector store\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 2}\n",
        ")\n",
        "\n",
        "# Test retriever\n",
        "query = \"How do text splitters work?\"\n",
        "retrieved_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nRetrieved {len(retrieved_docs)} documents:\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    print(f\"{i}. {doc.page_content}\")\n",
        "    print(f\"   Source: {doc.metadata.get('source', 'unknown')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Chains - RetrievalQA with GPT-5-Mini\n",
        "\n",
        "Chains combine multiple components to create end-to-end applications. We're using AskSage's GPT-5-Mini model for question answering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create RetrievalQA chain with AskSage GPT-5-Mini\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ RetrievalQA chain created successfully with GPT-5-Mini!\")\n",
        "print(\"ü§ñ Ready to answer questions using AskSage's advanced AI capabilities\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ask questions using the chain with GPT-5-Mini\n",
        "questions = [\n",
        "    \"What is LangChain and what does it enable?\",\n",
        "    \"How do embeddings help with semantic search?\",\n",
        "    \"What is the purpose of text splitters?\",\n",
        "    \"How does AskSage integrate with LangChain?\"\n",
        "]\n",
        "\n",
        "print(\"üöÄ Asking questions using AskSage GPT-5-Mini...\\n\")\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"‚ùì Question: {question}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    try:\n",
        "        result = qa_chain({\"query\": question})\n",
        "        \n",
        "        print(f\"\\nü§ñ GPT-5-Mini Answer: {result['result']}\")\n",
        "        print(f\"\\nüìö Sources used:\")\n",
        "        for i, doc in enumerate(result['source_documents'], 1):\n",
        "            print(f\"{i}. {doc.metadata.get('source', 'unknown')}: {doc.page_content}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing question: {e}\")\n",
        "    \n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Direct AskSage GPT-5-Mini Usage\n",
        "\n",
        "Let's also demonstrate direct usage of AskSage's GPT-5-Mini model for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Direct usage of AskSage GPT-5-Mini\n",
        "print(\"üéØ Direct AskSage GPT-5-Mini Usage Examples\\n\")\n",
        "\n",
        "direct_questions = [\n",
        "    \"Explain the benefits of using LangChain for AI applications\",\n",
        "    \"What are the key components of a RAG (Retrieval-Augmented Generation) system?\",\n",
        "    \"How can vector embeddings improve search and retrieval?\"\n",
        "]\n",
        "\n",
        "for question in direct_questions:\n",
        "    print(f\"üí¨ Question: {question}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        response = client.query(\n",
        "            message=question,\n",
        "            model=\"gpt-5-mini\",\n",
        "            temperature=0\n",
        "        )\n",
        "        \n",
        "        if response.get('status') == 200:\n",
        "            print(f\"ü§ñ GPT-5-Mini Response: {response.get('response', 'No response received')}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error calling AskSage: {e}\")\n",
        "    \n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this demo, we covered the core LangChain components integrated with AskSage's GPT-5-Mini:\n",
        "\n",
        "1. **Document Loaders**: Load data from various sources\n",
        "2. **Text Splitters**: Break large documents into manageable chunks\n",
        "3. **AskSage Embeddings**: Convert text to vector representations using AskSage's capabilities\n",
        "4. **Vector Stores**: Store and search embeddings efficiently\n",
        "5. **Retrievers**: Provide standardized document retrieval interface\n",
        "6. **Chains with GPT-5-Mini**: Combine components for end-to-end applications using AskSage's advanced GPT-5-Mini model\n",
        "7. **Direct AskSage Integration**: Direct usage of GPT-5-Mini for various AI tasks\n",
        "\n",
        "### Key Advantages of Using AskSage with GPT-5-Mini:\n",
        "\n",
        "- **Advanced Model**: GPT-5-Mini provides state-of-the-art language understanding and generation\n",
        "- **Enterprise Ready**: AskSage offers enterprise-grade security and reliability\n",
        "- **Flexible Integration**: Easy integration with existing LangChain workflows\n",
        "- **Scalable**: Built for production-scale applications\n",
        "\n",
        "These components work together to enable powerful RAG (Retrieval-Augmented Generation) applications that can answer questions based on your own documents, powered by AskSage's GPT-5-Mini model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}