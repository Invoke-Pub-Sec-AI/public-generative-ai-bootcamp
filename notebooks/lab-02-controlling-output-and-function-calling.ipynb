{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "0dbda2428a62488cbcfd974b88a1b342",
      "metadata": {},
      "source": [
        "# Lab 02: Controlling Output & Function Calling\n",
        "\n",
        "## Learning Objectives\n",
        "- Prompt an LLM to generate structured output (e.g., JSON)\n",
        "- Understand the concept of function calling and how to prompt for it\n",
        "- Implement a system to parse and execute function calls from an LLM\n",
        "- Handle errors and validate the model's output\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86bbb2b46dcb41eba175408f5a3808cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Dict, Any, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e97a058b45724d6fb2e997141c45d302",
      "metadata": {},
      "source": [
        "## Part 1: Prompting for Structured Output (JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc62a0e645444df9c4c5a8487ecb0a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockLLM:\n",
        "    \"\"\"A mock LLM that can generate JSON based on the prompt.\"\"\"\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if 'json' in prompt.lower() and 'user' in prompt.lower():\n",
        "            return '''\n",
        "            ```json\n",
        "            {\n",
        "                \"name\": \"John Doe\",\n",
        "                \"age\": 30,\n",
        "                \"email\": \"john.doe@example.com\"\n",
        "            }\n",
        "            ```\n",
        "            '''\n",
        "        return 'I cannot provide that information as a JSON object.'\n",
        "\n",
        "def extract_json_from_response(response: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Extracts a JSON object from a markdown code block.\"\"\"\n",
        "    match = re.search(r'```json\\n(.*)\\n```', response, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "llm = MockLLM()\n",
        "\n",
        "# A prompt designed to elicit a JSON response\n",
        "json_prompt = (\n",
        "    'Extract the user information from the following text and provide it as a JSON object. \n",
        "'\n",
        "    'The user is John Doe, he is 30 years old, and his email is john.doe@example.com.\n",
        "'\n",
        "    'Your response must contain only the JSON object inside a ```json code block.'\n",
        ")\n",
        "\n",
        "response = llm.generate(json_prompt)\n",
        "parsed_json = extract_json_from_response(response)\n",
        "\n",
        "print(\"--- Prompting for JSON ---\")\n",
        "print(f'LLM raw response:\n",
        "{response}')\n",
        "print(f'Parsed JSON object: {parsed_json}')\n",
        "if parsed_json:\n",
        "    print(f'Successfully extracted user name: {parsed_json.get(\"name\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc90e9cc4c5470c9262b119d5d83319",
      "metadata": {},
      "source": [
        "## Part 2: Function Calling\n",
        "\n",
        "Function calling allows an LLM to request that a specific function be executed in the client's code. The LLM doesn't actually run the function; it generates a structured JSON object specifying the function name and arguments. The application then parses this, runs the function, and can even return the result to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75ce2138e29c4a2d91159e7516f3fa93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the tools (functions) available to the LLM\n",
        "tools_schema = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the current weather for a specific location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "class FunctionCallingLLM:\n",
        "    def generate(self, prompt: str, tools: List[Dict]) -> str:\n",
        "        prompt_lower = prompt.lower()\n",
        "        if 'weather' in prompt_lower and 'boston' in prompt_lower:\n",
        "            # The LLM generates a JSON object representing the function call\n",
        "            return json.dumps({\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"type\": \"function\",\n",
        "                        \"function\": {\n",
        "                            \"name\": \"get_weather\",\n",
        "                            \"arguments\": '{\"location\": \"Boston, MA\"}'\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            })\n",
        "        return 'I can only provide weather information.'\n",
        "\n",
        "# The actual function that exists in our application code\n",
        "def get_weather(location: str) -> str:\n",
        "    if 'boston' in location.lower():\n",
        "        return f'The weather in {location} is 70°F and sunny.'\n",
        "    return f'Weather for {location} is not available.'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed983aa2d6634f1abf4c76c7b03c3590",
      "metadata": {},
      "source": [
        "## Part 3: Parsing and Executing Function Calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c557d36325445ba32d8eb02b8481e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_tool_calls(response_json: Dict, available_tools: Dict) -> Any:\n",
        "    tool_calls = response_json.get('tool_calls')\n",
        "    if not tool_calls:\n",
        "        return \"No function call requested.\"\n",
        "    \n",
        "    call = tool_calls[0]['function']\n",
        "    func_name = call['name']\n",
        "    func_to_call = available_tools.get(func_name)\n",
        "    \n",
        "    if not func_to_call:\n",
        "        return f\"Error: Function '{func_name}' not found.\"\n",
        "        \n",
        "    try:\n",
        "        func_args = json.loads(call['arguments'])\n",
        "        return func_to_call(**func_args)\n",
        "    except (json.JSONDecodeError, TypeError) as e:\n",
        "        return f\"Error executing function: {e}\"\n",
        "\n",
        "# Create a mapping of available tool names to actual functions\n",
        "available_tools = {\n",
        "    \"get_weather\": get_weather\n",
        "}\n",
        "\n",
        "# Simulate the end-to-end process\n",
        "fc_llm = FunctionCallingLLM()\n",
        "user_prompt = \"What's the weather like in Boston today?\"\n",
        "\n",
        "# 1. LLM generates the function call\n",
        "llm_response = fc_llm.generate(user_prompt, tools=tools_schema)\n",
        "response_data = json.loads(llm_response)\n",
        "\n",
        "print(\"--- Function Calling Workflow ---\")\n",
        "print(f'LLM generated tool call: {response_data}')\n",
        "\n",
        "# 2. Application executes the function\n",
        "execution_result = execute_tool_calls(response_data, available_tools)\n",
        "print(f'Result of function execution: {execution_result}')\n",
        "\n",
        "# 3. (Optional) Return the result to the LLM for a final, natural language response\n",
        "final_prompt = f'The user asked: {user_prompt}. The tool returned: {execution_result}. Formulate a natural language response.'\n",
        "# final_response = llm.generate(final_prompt) -> 'The weather in Boston, MA is 70°F and sunny.'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e2b0602140d460395e3b346bc5ca7a7",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Add a New Tool**: Define a new tool in the `tools_schema` for a function called `send_email(to: str, subject: str, body: str)`. Implement the mock `send_email` function and update the `available_tools` mapping. Then, write a prompt that would cause the LLM to call it.\n",
        "2. **Handle Multiple Function Calls**: Modify `execute_tool_calls` to handle a response from the LLM that requests multiple function calls in a single turn. The function should loop through `tool_calls` and execute each one.\n",
        "3. **Implement Response Validation**: In `extract_json_from_response`, add a validation step. For example, after parsing the JSON, check if it contains the required keys (`name`, `age`, `email`). If not, return an error or `None`. You could use a library like `jsonschema` for more robust validation.\n",
        "\n",
        "## Summary\n",
        "\n",
        "You learned:\n",
        "- How to instruct an LLM to return structured data like JSON by providing clear formatting requirements in the prompt.\n",
        "- The concept of **function calling**, where an LLM generates a request to execute a function defined in your application.\n",
        "- How to build a system that can parse the LLM's request, map it to an actual function, execute it, and use the result."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}