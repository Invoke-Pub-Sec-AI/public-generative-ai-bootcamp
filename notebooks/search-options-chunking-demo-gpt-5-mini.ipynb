{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Search Options & Chunking - Demo with gpt-5-mini\n",
        "\n",
        "**Focus**: dense vs sparse vs hybrid retrieval; metadata filters; chunk A/B testing with gpt-5-mini integration.\n",
        "\n",
        "This notebook demonstrates different retrieval strategies and their trade-offs in RAG systems using AskSageClient with gpt-5-mini and nvidia/NV-Embed-v2.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand dense retrieval (embedding-based similarity)\n",
        "- Learn sparse retrieval (keyword-based like BM25)\n",
        "- Explore hybrid retrieval combining both approaches\n",
        "- Apply metadata filtering for refined search\n",
        "- Compare different chunking strategies\n",
        "- **NEW**: Use gpt-5-mini for enhanced RAG responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-community faiss-cpu tiktoken rank_bm25 pandas numpy matplotlib sentence-transformers transformers torch asksageclient\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from typing import List, Dict, Any\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "print(\"✅ All packages installed and modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load credentials from a JSON file\n",
        "def load_credentials(filename):\n",
        "    try:\n",
        "        with open(filename) as file:\n",
        "            return json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The credentials file was not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Failed to decode JSON from the credentials file.\")\n",
        "\n",
        "# Load the credentials\n",
        "credentials = load_credentials('../../credentials.json')\n",
        "\n",
        "# Extract the API key, and email from the credentials to be used in the API request\n",
        "api_key = credentials['credentials']['api_key']\n",
        "email = credentials['credentials']['Ask_sage_user_info']['username']\n",
        "\n",
        "# Create an instance of the AskSageClient class with the email and api_key\n",
        "ask_sage_client = AskSageClient(email, api_key)\n",
        "\n",
        "print(\"✅ AskSageClient initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Embedding class for nvidia/NV-Embed-v2\n",
        "class NVidiaEmbeddings(Embeddings):\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
        "        \n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed a list of documents.\"\"\"\n",
        "        embeddings = self.model.encode(texts, normalize_embeddings=True)\n",
        "        return embeddings.tolist()\n",
        "    \n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed a single query text.\"\"\"\n",
        "        embedding = self.model.encode([text], normalize_embeddings=True)\n",
        "        return embedding[0].tolist()\n",
        "\n",
        "print(\"✅ Custom NVidiaEmbeddings class defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gpt-5-mini RAG Helper Function\n",
        "def ask_gpt_5_mini_with_context(query: str, context_docs: List[Document], client: AskSageClient) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Use gpt-5-mini to answer a query using retrieved context documents.\n",
        "    \"\"\"\n",
        "    # Prepare context from retrieved documents\n",
        "    context_text = \"\\n\\n\".join([\n",
        "        f\"Document {i+1} (Topic: {doc.metadata.get('topic', 'N/A')}, \"\n",
        "        f\"Difficulty: {doc.metadata.get('difficulty', 'N/A')}):\\n{doc.page_content}\"\n",
        "        for i, doc in enumerate(context_docs)\n",
        "    ])\n",
        "    \n",
        "    # Create the prompt with context\n",
        "    prompt = f\"\"\"Based on the following context documents, please answer the question comprehensively.\n",
        "\n",
        "Context Documents:\n",
        "{context_text}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a detailed answer based on the context provided above. If the context doesn't contain enough information to fully answer the question, please indicate what additional information would be helpful.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Make the API call using gpt-5-mini\n",
        "        response = client.query(\n",
        "            message=prompt,\n",
        "            system_prompt=\"You are a helpful assistant.\",\n",
        "            model=\"gpt-5-mini\",\n",
        "\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"answer\": response.get(\"response\", \"No response received\"),\n",
        "            \"context_used\": len(context_docs),\n",
        "            \"model\": \"gpt-5-mini\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e),\n",
        "            \"context_used\": len(context_docs),\n",
        "            \"model\": \"gpt-5-mini\"\n",
        "        }\n",
        "\n",
        "print(\"✅ gpt-5-mini RAG helper function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Sample Dataset with Metadata\n",
        "\n",
        "Let's create a diverse dataset with rich metadata for our retrieval experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample documents with metadata\n",
        "sample_docs = [\n",
        "    {\n",
        "        \"content\": \"Machine learning algorithms like neural networks require large datasets for training. Deep learning models with millions of parameters need extensive computational resources and GPU acceleration. Popular frameworks include TensorFlow, PyTorch, and scikit-learn for different ML tasks.\",\n",
        "        \"metadata\": {\"topic\": \"machine_learning\", \"difficulty\": \"advanced\", \"word_count\": 32, \"author\": \"Dr. Smith\", \"year\": 2023}\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Python is a versatile programming language used in data science, web development, and artificial intelligence. Its simple syntax makes it beginner-friendly. Key libraries include NumPy, Pandas, and Matplotlib for data analysis and visualization.\",\n",
        "        \"metadata\": {\"topic\": \"programming\", \"difficulty\": \"beginner\", \"word_count\": 28, \"author\": \"Prof. Johnson\", \"year\": 2022}\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Natural language processing involves text analysis, sentiment analysis, and language understanding. Modern NLP uses transformer architectures like BERT and GPT. Applications include chatbots, translation services, and document analysis systems.\",\n",
        "        \"metadata\": {\"topic\": \"nlp\", \"difficulty\": \"intermediate\", \"word_count\": 28, \"author\": \"Dr. Smith\", \"year\": 2023}\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Data visualization helps communicate insights effectively. Popular libraries include matplotlib, seaborn, and plotly for creating charts and interactive dashboards. Good visualizations tell a story and make complex data accessible to stakeholders.\",\n",
        "        \"metadata\": {\"topic\": \"data_science\", \"difficulty\": \"beginner\", \"word_count\": 26, \"author\": \"Prof. Davis\", \"year\": 2022}\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Reinforcement learning agents learn through interaction with environments. Q-learning and policy gradient methods are fundamental approaches in RL. Applications include game playing, robotics, and autonomous systems optimization.\",\n",
        "        \"metadata\": {\"topic\": \"machine_learning\", \"difficulty\": \"advanced\", \"word_count\": 25, \"author\": \"Dr. Wilson\", \"year\": 2023}\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Web scraping extracts data from websites using libraries like BeautifulSoup and Scrapy. Always respect robots.txt and rate limits when scraping. Common applications include price monitoring, content aggregation, and market research.\",\n",
        "        \"metadata\": {\"topic\": \"programming\", \"difficulty\": \"intermediate\", \"word_count\": 26, \"author\": \"Prof. Johnson\", \"year\": 2022}\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Statistical analysis involves hypothesis testing, correlation analysis, and regression modeling. Understanding p-values and confidence intervals is crucial. Modern tools include R, Python's scipy.stats, and specialized software like SPSS.\",\n",
        "        \"metadata\": {\"topic\": \"statistics\", \"difficulty\": \"intermediate\", \"word_count\": 25, \"author\": \"Dr. Brown\", \"year\": 2023}\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Cloud computing platforms like AWS, Azure, and GCP provide scalable infrastructure. Serverless computing reduces operational overhead significantly. Key services include compute instances, storage solutions, and managed databases.\",\n",
        "        \"metadata\": {\"topic\": \"cloud\", \"difficulty\": \"intermediate\", \"word_count\": 24, \"author\": \"Prof. Davis\", \"year\": 2023}\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to LangChain documents\n",
        "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in sample_docs]\n",
        "\n",
        "print(f\"Created {len(documents)} documents with metadata\")\n",
        "print(f\"\\nExample document:\")\n",
        "print(f\"Content: {documents[0].page_content}\")\n",
        "print(f\"Metadata: {documents[0].metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chunking Strategy Comparison\n",
        "\n",
        "Let's test different chunking strategies and see their impact on retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy A: Smaller chunks (150 chars)\n",
        "splitter_a = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "# Strategy B: Larger chunks (300 chars)\n",
        "splitter_b = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=30,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "# Apply both strategies\n",
        "chunks_a = splitter_a.split_documents(documents)\n",
        "chunks_b = splitter_b.split_documents(documents)\n",
        "\n",
        "print(\"Chunking Strategy Comparison:\")\n",
        "print(f\"Strategy A (150 chars): {len(chunks_a)} chunks\")\n",
        "print(f\"Strategy B (300 chars): {len(chunks_b)} chunks\")\n",
        "\n",
        "print(f\"\\nExample chunk A: '{chunks_a[0].page_content}'\")\n",
        "print(f\"Length: {len(chunks_a[0].page_content)} chars\")\n",
        "\n",
        "print(f\"\\nExample chunk B: '{chunks_b[0].page_content}'\")\n",
        "print(f\"Length: {len(chunks_b[0].page_content)} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dense Retrieval (Embedding-Based)\n",
        "\n",
        "Dense retrieval uses embeddings to capture semantic similarity. Now using nvidia/NV-Embed-v2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize nvidia embeddings\n",
        "embeddings = NVidiaEmbeddings()\n",
        "print(\"✅ NVidia NV-Embed-v2 model loaded successfully!\")\n",
        "\n",
        "# Create dense retriever using Strategy A chunks\n",
        "dense_vectorstore_a = FAISS.from_documents(chunks_a, embeddings)\n",
        "dense_retriever_a = dense_vectorstore_a.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}\n",
        ")\n",
        "\n",
        "# Create dense retriever using Strategy B chunks\n",
        "dense_vectorstore_b = FAISS.from_documents(chunks_b, embeddings)\n",
        "dense_retriever_b = dense_vectorstore_b.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}\n",
        ")\n",
        "\n",
        "print(\"✅ Dense retrievers created for both chunking strategies\")\n",
        "\n",
        "# Test dense retrieval\n",
        "test_query = \"machine learning algorithms and neural networks\"\n",
        "print(f\"\\nTest Query: '{test_query}'\")\n",
        "\n",
        "start_time = time.time()\n",
        "dense_results_a = dense_retriever_a.get_relevant_documents(test_query)\n",
        "dense_latency_a = time.time() - start_time\n",
        "\n",
        "print(f\"\\nDense Retrieval (Strategy A) - Latency: {dense_latency_a:.4f}s\")\n",
        "for i, doc in enumerate(dense_results_a, 1):\n",
        "    print(f\"{i}. {doc.page_content[:100]}...\")\n",
        "    print(f\"   Topic: {doc.metadata.get('topic', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sparse Retrieval (BM25 Keyword-Based)\n",
        "\n",
        "Sparse retrieval uses exact keyword matching and term frequency statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create BM25 retriever for Strategy A chunks\n",
        "sparse_retriever_a = BM25Retriever.from_documents(chunks_a)\n",
        "sparse_retriever_a.k = 3\n",
        "\n",
        "# Create BM25 retriever for Strategy B chunks\n",
        "sparse_retriever_b = BM25Retriever.from_documents(chunks_b)\n",
        "sparse_retriever_b.k = 3\n",
        "\n",
        "print(\"✅ Sparse (BM25) retrievers created for both chunking strategies\")\n",
        "\n",
        "# Test sparse retrieval\n",
        "start_time = time.time()\n",
        "sparse_results_a = sparse_retriever_a.get_relevant_documents(test_query)\n",
        "sparse_latency_a = time.time() - start_time\n",
        "\n",
        "print(f\"\\nSparse Retrieval (Strategy A) - Latency: {sparse_latency_a:.4f}s\")\n",
        "for i, doc in enumerate(sparse_results_a, 1):\n",
        "    print(f\"{i}. {doc.page_content[:100]}...\")\n",
        "    print(f\"   Topic: {doc.metadata.get('topic', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Hybrid Retrieval (Dense + Sparse)\n",
        "\n",
        "Hybrid retrieval combines the strengths of both approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create hybrid retriever for Strategy A\n",
        "hybrid_retriever_a = EnsembleRetriever(\n",
        "    retrievers=[dense_retriever_a, sparse_retriever_a],\n",
        "    weights=[0.5, 0.5]  # Equal weighting\n",
        ")\n",
        "\n",
        "# Create hybrid retriever for Strategy B\n",
        "hybrid_retriever_b = EnsembleRetriever(\n",
        "    retrievers=[dense_retriever_b, sparse_retriever_b],\n",
        "    weights=[0.5, 0.5]\n",
        ")\n",
        "\n",
        "print(\"✅ Hybrid retrievers created for both chunking strategies\")\n",
        "\n",
        "# Test hybrid retrieval\n",
        "start_time = time.time()\n",
        "hybrid_results_a = hybrid_retriever_a.get_relevant_documents(test_query)\n",
        "hybrid_latency_a = time.time() - start_time\n",
        "\n",
        "print(f\"\\nHybrid Retrieval (Strategy A) - Latency: {hybrid_latency_a:.4f}s\")\n",
        "for i, doc in enumerate(hybrid_results_a, 1):\n",
        "    print(f\"{i}. {doc.page_content[:100]}...\")\n",
        "    print(f\"   Topic: {doc.metadata.get('topic', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. gpt-5-mini RAG Integration\n",
        "\n",
        "Now let's use gpt-5-mini to generate comprehensive answers using our retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test gpt-5-mini with different retrieval strategies\n",
        "print(\"=\" * 60)\n",
        "print(\"gpt-5-mini RAG Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test with dense retrieval\n",
        "print(\"\\n🔍 DENSE RETRIEVAL + gpt-5-mini\")\n",
        "print(\"-\" * 40)\n",
        "dense_rag_result = ask_gpt_o3_mini_with_context(test_query, dense_results_a, ask_sage_client)\n",
        "if dense_rag_result[\"success\"]:\n",
        "    print(f\"Answer: {dense_rag_result['answer']}\")\n",
        "    print(f\"Context docs used: {dense_rag_result['context_used']}\")\n",
        "else:\n",
        "    print(f\"Error: {dense_rag_result['error']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Test with sparse retrieval\n",
        "print(\"\\n🔍 SPARSE RETRIEVAL + gpt-5-mini\")\n",
        "print(\"-\" * 40)\n",
        "sparse_rag_result = ask_gpt_o3_mini_with_context(test_query, sparse_results_a, ask_sage_client)\n",
        "if sparse_rag_result[\"success\"]:\n",
        "    print(f\"Answer: {sparse_rag_result['answer']}\")\n",
        "    print(f\"Context docs used: {sparse_rag_result['context_used']}\")\n",
        "else:\n",
        "    print(f\"Error: {sparse_rag_result['error']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Test with hybrid retrieval\n",
        "print(\"\\n🔍 HYBRID RETRIEVAL + gpt-5-mini\")\n",
        "print(\"-\" * 40)\n",
        "hybrid_rag_result = ask_gpt_o3_mini_with_context(test_query, hybrid_results_a, ask_sage_client)\n",
        "if hybrid_rag_result[\"success\"]:\n",
        "    print(f\"Answer: {hybrid_rag_result['answer']}\")\n",
        "    print(f\"Context docs used: {hybrid_rag_result['context_used']}\")\n",
        "else:\n",
        "    print(f\"Error: {hybrid_rag_result['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Metadata Filtering with gpt-5-mini\n",
        "\n",
        "Apply filters to restrict search to specific document characteristics and use gpt-5-mini for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for advanced difficulty documents only\n",
        "filter_advanced = {\"difficulty\": \"advanced\"}\n",
        "\n",
        "# Test filtered dense retrieval\n",
        "filtered_results = dense_vectorstore_a.similarity_search(\n",
        "    test_query,\n",
        "    k=3,\n",
        "    filter=filter_advanced\n",
        ")\n",
        "\n",
        "print(f\"Filtered Results (Advanced difficulty only):\")\n",
        "for i, doc in enumerate(filtered_results, 1):\n",
        "    print(f\"{i}. {doc.page_content[:100]}...\")\n",
        "    print(f\"   Difficulty: {doc.metadata.get('difficulty')}\")\n",
        "    print(f\"   Topic: {doc.metadata.get('topic')}\")\n",
        "\n",
        "# Use gpt-5-mini with filtered results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🔍 FILTERED (ADVANCED) + gpt-5-mini\")\n",
        "print(\"-\" * 40)\n",
        "filtered_rag_result = ask_gpt_o3_mini_with_context(test_query, filtered_results, ask_sage_client)\n",
        "if filtered_rag_result[\"success\"]:\n",
        "    print(f\"Answer: {filtered_rag_result['answer']}\")\n",
        "    print(f\"Context docs used: {filtered_rag_result['context_used']}\")\n",
        "else:\n",
        "    print(f\"Error: {filtered_rag_result['error']}\")\n",
        "\n",
        "# Filter by author and year\n",
        "filter_author_year = {\"author\": \"Dr. Smith\", \"year\": 2023}\n",
        "\n",
        "author_results = dense_vectorstore_a.similarity_search(\n",
        "    \"natural language processing\",\n",
        "    k=2,\n",
        "    filter=filter_author_year\n",
        ")\n",
        "\n",
        "print(f\"\\nFiltered Results (Dr. Smith, 2023):\")\n",
        "for i, doc in enumerate(author_results, 1):\n",
        "    print(f\"{i}. {doc.page_content[:100]}...\")\n",
        "    print(f\"   Author: {doc.metadata.get('author')}\")\n",
        "    print(f\"   Year: {doc.metadata.get('year')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Chunking Strategy Comparison with gpt-5-mini\n",
        "\n",
        "Let's compare how different chunking strategies affect gpt-5-mini's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare chunking strategies with gpt-5-mini\n",
        "comparison_query = \"What are the key aspects of data science and machine learning?\"\n",
        "\n",
        "# Get results from both chunking strategies\n",
        "dense_results_strategy_a = dense_retriever_a.get_relevant_documents(comparison_query)\n",
        "dense_results_strategy_b = dense_retriever_b.get_relevant_documents(comparison_query)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CHUNKING STRATEGY COMPARISON WITH gpt-5-mini\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Strategy A (smaller chunks)\n",
        "print(\"\\n📊 STRATEGY A (150 chars) + gpt-5-mini\")\n",
        "print(\"-\" * 50)\n",
        "strategy_a_result = ask_gpt_o3_mini_with_context(comparison_query, dense_results_strategy_a, ask_sage_client)\n",
        "if strategy_a_result[\"success\"]:\n",
        "    print(f\"Answer: {strategy_a_result['answer']}\")\n",
        "    print(f\"Context docs used: {strategy_a_result['context_used']}\")\n",
        "else:\n",
        "    print(f\"Error: {strategy_a_result['error']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Strategy B (larger chunks)\n",
        "print(\"\\n📊 STRATEGY B (300 chars) + gpt-5-mini\")\n",
        "print(\"-\" * 50)\n",
        "strategy_b_result = ask_gpt_o3_mini_with_context(comparison_query, dense_results_strategy_b, ask_sage_client)\n",
        "if strategy_b_result[\"success\"]:\n",
        "    print(f\"Answer: {strategy_b_result['answer']}\")\n",
        "    print(f\"Context docs used: {strategy_b_result['context_used']}\")\n",
        "else:\n",
        "    print(f\"Error: {strategy_b_result['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Performance Analysis and Visualization\n",
        "\n",
        "Let's create visualizations to compare the performance of different retrieval strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance comparison data\n",
        "retrieval_methods = ['Dense', 'Sparse', 'Hybrid']\n",
        "latencies = [dense_latency_a, sparse_latency_a, hybrid_latency_a]\n",
        "chunk_counts = [len(chunks_a), len(chunks_b)]\n",
        "chunk_strategies = ['Strategy A (150 chars)', 'Strategy B (300 chars)']\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Retrieval latencies\n",
        "bars1 = ax1.bar(retrieval_methods, latencies, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "ax1.set_title('Retrieval Method Latency Comparison', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Latency (seconds)')\n",
        "ax1.set_xlabel('Retrieval Method')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, latency in zip(bars1, latencies):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "             f'{latency:.4f}s', ha='center', va='bottom')\n",
        "\n",
        "# Plot 2: Chunk count comparison\n",
        "bars2 = ax2.bar(chunk_strategies, chunk_counts, color=['#d62728', '#9467bd'])\n",
        "ax2.set_title('Chunking Strategy Comparison', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('Number of Chunks')\n",
        "ax2.set_xlabel('Chunking Strategy')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, count in zip(bars2, chunk_counts):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "             f'{count}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n📈 PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Fastest retrieval method: {retrieval_methods[latencies.index(min(latencies))]} ({min(latencies):.4f}s)\")\n",
        "print(f\"Most chunks generated: {chunk_strategies[chunk_counts.index(max(chunk_counts))]} ({max(chunk_counts)} chunks)\")\n",
        "print(f\"Fewest chunks generated: {chunk_strategies[chunk_counts.index(min(chunk_counts))]} ({min(chunk_counts)} chunks)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Key Findings\n",
        "\n",
        "This demo explored the key aspects of search options and chunking in RAG systems using AskSageClient with gpt-5-mini and nvidia/NV-Embed-v2:\n",
        "\n",
        "### Key Updates Made:\n",
        "1. **LLM Model**: Integrated gpt-5-mini for enhanced RAG responses\n",
        "2. **Embedding Model**: Used nvidia/NV-Embed-v2 from HuggingFace\n",
        "3. **Custom Integration**: Created NVidiaEmbeddings class for LangChain compatibility\n",
        "4. **RAG Pipeline**: Added comprehensive RAG helper function with gpt-5-mini\n",
        "\n",
        "### Retrieval Methods Compared:\n",
        "1. **Dense Retrieval**: Embedding-based semantic similarity using nvidia/NV-Embed-v2\n",
        "2. **Sparse Retrieval**: Keyword-based BM25 matching\n",
        "3. **Hybrid Retrieval**: Combines both approaches for optimal performance\n",
        "\n",
        "### Chunking Strategies:\n",
        "- **Strategy A**: Smaller chunks (150 chars) for precision and granular retrieval\n",
        "- **Strategy B**: Larger chunks (300 chars) for better context preservation\n",
        "\n",
        "### Key Features Demonstrated:\n",
        "- **gpt-5-mini Integration**: Enhanced answer generation using retrieved context\n",
        "- **Metadata filtering**: Refined search results based on document attributes\n",
        "- **Performance comparison**: Comprehensive analysis across different approaches\n",
        "- **Visualization**: Clear charts showing performance metrics and trade-offs\n",
        "\n",
        "### Best Practices Identified:\n",
        "1. **Hybrid retrieval** typically provides the best balance of performance and quality\n",
        "2. **Metadata filtering** offers powerful ways to constrain and improve search results\n",
        "3. **gpt-5-mini** excels at synthesizing information from multiple retrieved documents\n",
        "4. **Chunking strategy** should be chosen based on your specific use case:\n",
        "   - Smaller chunks for precise information retrieval\n",
        "   - Larger chunks for maintaining context and coherence\n",
        "\n",
        "The combination of sophisticated retrieval strategies with gpt-5-mini's reasoning capabilities creates a powerful RAG system that can provide comprehensive, contextually-aware responses to user queries."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}