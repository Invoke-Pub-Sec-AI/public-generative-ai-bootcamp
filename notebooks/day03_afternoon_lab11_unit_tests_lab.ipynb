{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 11: AI-Generated Unit Tests\n",
        "\n",
        "## Learning Objectives\n",
        "- Generate comprehensive unit tests using AI assistance\n",
        "- Implement test-driven development patterns\n",
        "- Create test suites with high code coverage\n",
        "- Design edge case and error condition tests\n",
        "\n",
        "## Lab Overview\n",
        "Build an AI-powered testing system that can:\n",
        "1. **Analyze Functions** - Understand code behavior and requirements\n",
        "2. **Generate Test Cases** - Create comprehensive test scenarios\n",
        "3. **Write Test Code** - Produce pytest-compatible test functions\n",
        "4. **Coverage Analysis** - Measure and improve test coverage\n",
        "\n",
        "## Exit Ticket\n",
        "- [ ] Function analysis and test case generation\n",
        "- [ ] Automated pytest code generation\n",
        "- [ ] Edge case and error testing\n",
        "- [ ] Coverage report and improvement suggestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install asksageclient pip_system_certs pytest pytest-cov coverage rich ast-tools tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔐 Cell 1 — Load secrets (Colab) + pricing + token utils\n",
        "# ================================\n",
        "import os, time, csv\n",
        "from typing import Optional, Dict\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "ASKSAGE_API_KEY = userdata.get(\"ASKSAGE_API_KEY\")\n",
        "ASKSAGE_BASE_URL = userdata.get(\"ASKSAGE_BASE_URL\")\n",
        "ASKSAGE_EMAIL = userdata.get(\"ASKSAGE_EMAIL\")\n",
        "\n",
        "assert ASKSAGE_API_KEY, \"ASKSAGE_API_KEY not provided.\"\n",
        "assert ASKSAGE_EMAIL, \"ASKSAGE_EMAIL not provided.\"\n",
        "\n",
        "print(\"✓ Secrets loaded\")\n",
        "print(\"  • EMAIL:\", ASKSAGE_EMAIL)\n",
        "print(\"  • BASE URL:\", ASKSAGE_BASE_URL or \"(default)\")\n",
        "\n",
        "# Pricing (USD per 1,000,000 tokens)\n",
        "PRICES_PER_M = {\n",
        "    \"gpt-5\": {\"input_per_m\": 1.25, \"output_per_m\": 10.00},\n",
        "    \"gpt-5-mini\": {\"input_per_m\": 0.25, \"output_per_m\": 2.00},\n",
        "}\n",
        "\n",
        "# Tokenizer\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text or \"\"))\n",
        "\n",
        "def cost_usd(model: str, input_tokens: int, output_tokens: int) -> float:\n",
        "    if model not in PRICES_PER_M:\n",
        "        raise ValueError(f\"Unknown model: {model}\")\n",
        "    r = PRICES_PER_M[model]\n",
        "    return (input_tokens / 1_000_000) * r[\"input_per_m\"] + (output_tokens / 1_000_000) * r[\"output_per_m\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔧 Cell 2 — Import bootcamp_common and setup AskSage client\n",
        "# ================================\n",
        "import sys\n",
        "sys.path.append('../../../')  # Adjust path to reach bootcamp_common\n",
        "\n",
        "from bootcamp_common.ask_sage import AskSageClient\n",
        "\n",
        "# Initialize AskSage client\n",
        "client = AskSageClient(\n",
        "    api_key=ASKSAGE_API_KEY,\n",
        "    base_url=ASKSAGE_BASE_URL\n",
        ")\n",
        "\n",
        "print(\"✓ AskSage client initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import inspect\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Callable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import openai\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.syntax import Syntax\n",
        "from rich.table import Table\n",
        "\n",
        "console = Console()\n",
        "print(\"🧪 AI Unit Test Generator loading...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Target Functions for Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample functions to generate tests for\n",
        "# TODO: Implement Target Functions for Testing\n",
        "#\n",
        "# Your task:\n",
        "# 1. Create calculate_tax() function with proper validation\n",
        "# 2. Build validate_email() with comprehensive format checking\n",
        "# 3. Implement fibonacci() with edge case handling\n",
        "# 4. Design process_user_data() for complex data processing\n",
        "#\n",
        "# Key requirements:\n",
        "# - Include proper docstrings with Args, Returns, Raises\n",
        "# - Add input validation and error handling\n",
        "# - Consider edge cases and boundary conditions\n",
        "# - Use type hints for better code clarity\n",
        "#\n",
        "# Functions to implement:\n",
        "# - calculate_tax(income: float, tax_rate: float) -> float\n",
        "# - validate_email(email: str) -> bool\n",
        "# - fibonacci(n: int) -> int\n",
        "# - process_user_data(users: List[Dict]) -> Dict[str, Any]\n",
        "\n",
        "print(\"⚠️ TODO: Implement the target functions listed above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AI Test Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AITestGenerator:\n",
        "    \"\"\"Generate comprehensive unit tests using AI\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.setup_client()\n",
        "        self.generated_tests = {}\n",
        "    \n",
        "    def setup_client(self):\n",
        "        if os.getenv('OPENAI_API_KEY'):\n",
        "            try:\n",
        "                self.client = openai.OpenAI()\n",
        "                self.has_api = True\n",
        "                console.print(\"✅ OpenAI client configured\")\n",
        "            except Exception as e:\n",
        "                self.has_api = False\n",
        "                console.print(f\"⚠️ Using mock test generation: {e}\")\n",
        "        else:\n",
        "            self.has_api = False\n",
        "            console.print(\"💡 No API key, using template tests\")\n",
        "    \n",
        "    def analyze_function(self, func: Callable) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze function to understand testing requirements\"\"\"\n",
        "        \n",
        "        # Get function metadata\n",
        "        signature = inspect.signature(func)\n",
        "        docstring = func.__doc__ or \"\"\n",
        "        source = inspect.getsource(func)\n",
        "        \n",
        "        analysis = {\n",
        "            'name': func.__name__,\n",
        "            'signature': str(signature),\n",
        "            'docstring': docstring,\n",
        "            'source': source,\n",
        "            'parameters': [],\n",
        "            'return_type': None,\n",
        "            'raises': []\n",
        "        }\n",
        "        \n",
        "        # Extract parameter information\n",
        "        for param_name, param in signature.parameters.items():\n",
        "            param_info = {\n",
        "                'name': param_name,\n",
        "                'type': str(param.annotation) if param.annotation != param.empty else 'Any',\n",
        "                'default': str(param.default) if param.default != param.empty else None\n",
        "            }\n",
        "            analysis['parameters'].append(param_info)\n",
        "        \n",
        "        # Extract return type\n",
        "        if signature.return_annotation != signature.empty:\n",
        "            analysis['return_type'] = str(signature.return_annotation)\n",
        "        \n",
        "        # Extract exceptions from docstring\n",
        "        if 'Raises:' in docstring:\n",
        "            raises_section = docstring.split('Raises:')[1].split('\\n')\n",
        "            for line in raises_section:\n",
        "                if ':' in line:\n",
        "                    exception = line.split(':')[0].strip()\n",
        "                    if exception:\n",
        "                        analysis['raises'].append(exception)\n",
        "        \n",
        "        return analysis\n",
        "\n",
        "# TODO: Implement generate_test_cases method\n",
        "    def generate_test_cases(self, func_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Generate test case scenarios for the function\"\"\"\n",
        "        \n",
        "        # TODO: Use AI to generate comprehensive test cases including:\n",
        "        # - Normal/happy path cases\n",
        "        # - Edge cases (empty inputs, boundary values)\n",
        "        # - Error cases that should raise exceptions\n",
        "        # - Invalid input types and formats\n",
        "        \n",
        "        # HINT: Create a structured prompt with function analysis\n",
        "        # HINT: Ask for JSON output with test case structure\n",
        "        \n",
        "        if self.has_api:\n",
        "            # TODO: Implement API call here\n",
        "            pass\n",
        "        \n",
        "        # Mock test cases for demonstration\n",
        "        if func_analysis['name'] == 'calculate_tax':\n",
        "            return [\n",
        "                {'type': 'normal', 'inputs': {'income': 50000, 'tax_rate': 0.25}, 'expected': 12500.0},\n",
        "                {'type': 'edge', 'inputs': {'income': 0, 'tax_rate': 0.25}, 'expected': 0.0},\n",
        "                {'type': 'error', 'inputs': {'income': -1000, 'tax_rate': 0.25}, 'expected_exception': 'ValueError'},\n",
        "                {'type': 'error', 'inputs': {'income': 50000, 'tax_rate': 1.5}, 'expected_exception': 'ValueError'}\n",
        "            ]\n",
        "        \n",
        "        return [{'type': 'normal', 'inputs': {}, 'expected': None}]\n",
        "\n",
        "# Initialize generator\n",
        "test_generator = AITestGenerator()\n",
        "print(\"🧪 Test generator ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Analyze Target Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze all target functions\n",
        "target_functions = [calculate_tax, validate_email, fibonacci, process_user_data]\n",
        "function_analyses = {}\n",
        "\n",
        "console.print(\"\\n🔍 [bold blue]Function Analysis Results[/bold blue]\")\n",
        "\n",
        "for func in target_functions:\n",
        "    analysis = test_generator.analyze_function(func)\n",
        "    function_analyses[func.__name__] = analysis\n",
        "    \n",
        "    # Display analysis\n",
        "    table = Table(title=f\"Analysis: {func.__name__}()\")\n",
        "    table.add_column(\"Aspect\", style=\"cyan\")\n",
        "    table.add_column(\"Details\", style=\"white\")\n",
        "    \n",
        "    table.add_row(\"Parameters\", str(len(analysis['parameters'])))\n",
        "    table.add_row(\"Return Type\", analysis['return_type'] or 'Unknown')\n",
        "    table.add_row(\"Exceptions\", ', '.join(analysis['raises']) or 'None')\n",
        "    table.add_row(\"Has Docstring\", \"Yes\" if analysis['docstring'] else \"No\")\n",
        "    \n",
        "    console.print(table)\n",
        "\n",
        "print(\"\\n✅ Function analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Generate Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Complete the generate_test_cases method above, then test it here\n",
        "# \n",
        "# console.print(\"\\n🧪 [bold blue]Generated Test Cases[/bold blue]\")\n",
        "# \n",
        "# for func_name, analysis in function_analyses.items():\n",
        "#     test_cases = test_generator.generate_test_cases(analysis)\n",
        "#     \n",
        "#     console.print(f\"\\n[green]{func_name}()[/green] - {len(test_cases)} test cases:\")\n",
        "#     for i, case in enumerate(test_cases, 1):\n",
        "#         console.print(f\"  {i}. [{case['type'].upper()}] {case}\")\n",
        "\n",
        "pass  # Replace with your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pytest Code Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PytestCodeGenerator:\n",
        "    \"\"\"Generate pytest-compatible test code\"\"\"\n",
        "    \n",
        "    def __init__(self, test_generator: AITestGenerator):\n",
        "        self.test_generator = test_generator\n",
        "    \n",
        "    def generate_test_file(self, func_name: str, test_cases: List[Dict]) -> str:\n",
        "        \"\"\"Generate complete pytest file for a function\"\"\"\n",
        "        \n",
        "        imports = '''import pytest\n",
        "from typing import Dict, List, Any\n",
        "from your_module import {func_name}  # Update import path\n",
        "\n",
        "'''.format(func_name=func_name)\n",
        "        \n",
        "        # Generate test functions\n",
        "        test_functions = []\n",
        "        \n",
        "        # Normal test cases\n",
        "        normal_cases = [case for case in test_cases if case['type'] == 'normal']\n",
        "        if normal_cases:\n",
        "            test_functions.append(self._generate_normal_tests(func_name, normal_cases))\n",
        "        \n",
        "        # Edge case tests\n",
        "        edge_cases = [case for case in test_cases if case['type'] == 'edge']\n",
        "        if edge_cases:\n",
        "            test_functions.append(self._generate_edge_tests(func_name, edge_cases))\n",
        "        \n",
        "        # Error tests\n",
        "        error_cases = [case for case in test_cases if case['type'] == 'error']\n",
        "        if error_cases:\n",
        "            test_functions.append(self._generate_error_tests(func_name, error_cases))\n",
        "        \n",
        "        return imports + '\\n\\n'.join(test_functions)\n",
        "    \n",
        "    def _generate_normal_tests(self, func_name: str, cases: List[Dict]) -> str:\n",
        "        \"\"\"Generate normal/happy path test cases\"\"\"\n",
        "        test_code = f'''def test_{func_name}_normal_cases():\n",
        "    \"\"\"Test normal/happy path scenarios for {func_name}\"\"\"\n",
        "'''\n",
        "        \n",
        "        for i, case in enumerate(cases):\n",
        "            inputs = case['inputs']\n",
        "            expected = case['expected']\n",
        "            \n",
        "            # Generate function call\n",
        "            if inputs:\n",
        "                args = ', '.join(f\"{k}={repr(v)}\" for k, v in inputs.items())\n",
        "                call = f\"{func_name}({args})\"\n",
        "            else:\n",
        "                call = f\"{func_name}()\"\n",
        "            \n",
        "            test_code += f'''    \n",
        "    # Test case {i+1}\n",
        "    result = {call}\n",
        "    assert result == {repr(expected)}\n",
        "'''\n",
        "        \n",
        "        return test_code\n",
        "    \n",
        "    def _generate_edge_tests(self, func_name: str, cases: List[Dict]) -> str:\n",
        "        \"\"\"Generate edge case tests\"\"\"\n",
        "        test_code = f'''def test_{func_name}_edge_cases():\n",
        "    \"\"\"Test edge cases for {func_name}\"\"\"\n",
        "'''\n",
        "        \n",
        "        for i, case in enumerate(cases):\n",
        "            inputs = case['inputs']\n",
        "            expected = case['expected']\n",
        "            \n",
        "            if inputs:\n",
        "                args = ', '.join(f\"{k}={repr(v)}\" for k, v in inputs.items())\n",
        "                call = f\"{func_name}({args})\"\n",
        "            else:\n",
        "                call = f\"{func_name}()\"\n",
        "            \n",
        "            test_code += f'''    \n",
        "    # Edge case {i+1}\n",
        "    result = {call}\n",
        "    assert result == {repr(expected)}\n",
        "'''\n",
        "        \n",
        "        return test_code\n",
        "    \n",
        "    def _generate_error_tests(self, func_name: str, cases: List[Dict]) -> str:\n",
        "        \"\"\"Generate error/exception tests\"\"\"\n",
        "        test_code = f'''def test_{func_name}_error_cases():\n",
        "    \"\"\"Test error conditions for {func_name}\"\"\"\n",
        "'''\n",
        "        \n",
        "        for i, case in enumerate(cases):\n",
        "            inputs = case['inputs']\n",
        "            exception = case.get('expected_exception', 'Exception')\n",
        "            \n",
        "            if inputs:\n",
        "                args = ', '.join(f\"{k}={repr(v)}\" for k, v in inputs.items())\n",
        "                call = f\"{func_name}({args})\"\n",
        "            else:\n",
        "                call = f\"{func_name}()\"\n",
        "            \n",
        "            test_code += f'''    \n",
        "    # Error case {i+1}\n",
        "    with pytest.raises({exception}):\n",
        "        {call}\n",
        "'''\n",
        "        \n",
        "        return test_code\n",
        "\n",
        "# Initialize pytest generator\n",
        "pytest_generator = PytestCodeGenerator(test_generator)\n",
        "print(\"🧪 Pytest code generator ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Generate Complete Test Suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate test files for all functions\n",
        "test_dir = Path(\"generated_tests\")\n",
        "test_dir.mkdir(exist_ok=True)\n",
        "\n",
        "console.print(\"\\n🧪 [bold blue]Generating Test Files[/bold blue]\")\n",
        "\n",
        "for func_name, analysis in function_analyses.items():\n",
        "    # Generate test cases (using mock data for now)\n",
        "    test_cases = test_generator.generate_test_cases(analysis)\n",
        "    \n",
        "    # Generate pytest code\n",
        "    test_code = pytest_generator.generate_test_file(func_name, test_cases)\n",
        "    \n",
        "    # Save test file\n",
        "    test_file = test_dir / f\"test_{func_name}.py\"\n",
        "    test_file.write_text(test_code)\n",
        "    \n",
        "    console.print(f\"✅ Generated {test_file}\")\n",
        "    \n",
        "    # Show preview of generated code\n",
        "    preview = test_code[:400] + \"\\n# ... rest of test file ...\"\n",
        "    syntax = Syntax(preview, \"python\", theme=\"monokai\", line_numbers=True)\n",
        "    console.print(Panel(syntax, title=f\"Preview: {test_file.name}\", border_style=\"green\"))\n",
        "\n",
        "print(f\"\\n🎉 Generated {len(function_analyses)} test files!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Coverage Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement coverage analysis and improvement suggestions\n",
        "# \n",
        "# def analyze_test_coverage(source_file: str, test_files: List[str]) -> Dict:\n",
        "#     \"\"\"Analyze test coverage and suggest improvements\"\"\"\n",
        "#     \n",
        "#     # TODO: Use coverage.py to analyze test coverage\n",
        "#     # TODO: Identify uncovered lines and branches\n",
        "#     # TODO: Generate suggestions for additional tests\n",
        "#     \n",
        "#     pass\n",
        "# \n",
        "# def suggest_missing_tests(func_analysis: Dict, current_tests: List[Dict]) -> List[str]:\n",
        "#     \"\"\"Suggest additional test cases that might be missing\"\"\"\n",
        "#     \n",
        "#     # TODO: Analyze function complexity and current test coverage\n",
        "#     # TODO: Identify potential gaps (performance, integration, etc.)\n",
        "#     # TODO: Return suggestions for additional test scenarios\n",
        "#     \n",
        "#     pass\n",
        "\n",
        "print(\"📊 Coverage analysis framework ready (implement above)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension Ideas\n",
        "\n",
        "🚀 **Advanced Testing Features:**\n",
        "\n",
        "1. **Property-Based Testing**: Generate hypothesis-based tests\n",
        "2. **Performance Testing**: Create benchmarks and performance tests\n",
        "3. **Integration Testing**: Generate tests for function interactions\n",
        "4. **Mutation Testing**: Test the quality of generated tests\n",
        "5. **Mock Generation**: Auto-create mocks for dependencies\n",
        "6. **Test Data Generation**: Create realistic test datasets\n",
        "7. **Regression Testing**: Generate tests from bug reports\n",
        "\n",
        "## Deliverable Checklist\n",
        "\n",
        "- [ ] Function analysis extracting signatures, types, and behavior\n",
        "- [ ] AI-generated comprehensive test case scenarios\n",
        "- [ ] Pytest-compatible test code generation\n",
        "- [ ] Edge case and error condition testing\n",
        "- [ ] Test file organization and structure\n",
        "- [ ] Coverage analysis and improvement suggestions\n",
        "\n",
        "**Bonus Points:**\n",
        "- [ ] Property-based testing with Hypothesis\n",
        "- [ ] Performance benchmarks\n",
        "- [ ] Integration with CI/CD pipelines\n",
        "- [ ] Test quality metrics and reporting"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}