{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Observability & Caching - Demo (AskSage Edition)\n",
        "\n",
        "**Focus**: prompt hashing, read-through cache, TTLs, cache busting with AskSage's gpt-5-mini\n",
        "\n",
        "This notebook demonstrates advanced caching strategies for LLM applications to improve performance and reduce costs using AskSage's Python client.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand prompt hashing for cache keys\n",
        "- Implement read-through caching patterns\n",
        "- Configure TTL (Time-To-Live) for cache entries\n",
        "- Implement cache busting strategies\n",
        "- Use AskSage Python client with gpt-5-mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install asksageclient diskcache pandas\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Optional, Dict, Any\n",
        "import diskcache as dc\n",
        "import pandas as pd\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "# Initialize AskSage client\n",
        "# Set your AskSage credentials as environment variables:\n",
        "# export ASKSAGE_API_KEY=\"your-api-key\"\n",
        "# export ASKSAGE_TENANT=\"your-tenant\"\n",
        "# export ASKSAGE_USERNAME=\"your-username\"\n",
        "\n",
        "ask_sage_client = AskSageClient(\n",
        "    api_key=os.getenv(\"ASKSAGE_API_KEY\"),\n",
        "    tenant=os.getenv(\"ASKSAGE_TENANT\"),\n",
        "    username=os.getenv(\"ASKSAGE_USERNAME\")\n",
        ")\n",
        "\n",
        "print(\"✅ All packages installed and AskSage client initialized!\")\n",
        "print(f\"Available models: {ask_sage_client.get_models()['response'][:5]}...\")  # Show first 5 models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prompt Hashing\n",
        "\n",
        "First, we'll implement a system to generate consistent hash keys from prompts for caching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptHasher:\n",
        "    \"\"\"Generate consistent hash keys from prompts for caching.\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def hash_prompt(prompt: str, model: str = \"gpt-5-mini\", **kwargs) -> str:\n",
        "        \"\"\"Generate a hash from prompt components.\"\"\"\n",
        "        # Create a string representation of all prompt components\n",
        "        prompt_data = {\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"params\": {k: v for k, v in kwargs.items() if k in [\"temperature\", \"max_tokens\", \"top_p\"]}\n",
        "        }\n",
        "        \n",
        "        # Convert to JSON string for consistent ordering\n",
        "        prompt_str = json.dumps(prompt_data, sort_keys=True)\n",
        "        \n",
        "        # Generate SHA-256 hash\n",
        "        return hashlib.sha256(prompt_str.encode('utf-8')).hexdigest()[:16]\n",
        "\n",
        "# Test prompt hashing\n",
        "hasher = PromptHasher()\n",
        "\n",
        "# Example prompts\n",
        "prompt1 = \"What is the capital of France?\"\n",
        "prompt2 = \"What is the capital of Germany?\"\n",
        "prompt1_duplicate = \"What is the capital of France?\"\n",
        "\n",
        "hash1 = hasher.hash_prompt(prompt1)\n",
        "hash2 = hasher.hash_prompt(prompt2)\n",
        "hash1_dup = hasher.hash_prompt(prompt1_duplicate)\n",
        "\n",
        "print(f\"Prompt 1: '{prompt1}'\")\n",
        "print(f\"Hash 1:   {hash1}\")\n",
        "print(f\"\\nPrompt 2: '{prompt2}'\")\n",
        "print(f\"Hash 2:   {hash2}\")\n",
        "print(f\"\\nPrompt 1 (duplicate): '{prompt1_duplicate}'\")\n",
        "print(f\"Hash 1 (duplicate):   {hash1_dup}\")\n",
        "print(f\"\\n✅ Same prompts produce identical hashes: {hash1 == hash1_dup}\")\n",
        "print(f\"✅ Different prompts produce different hashes: {hash1 != hash2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Read-Through Cache Implementation\n",
        "\n",
        "Implement a read-through cache that automatically fetches and caches LLM responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLMCache:\n",
        "    \"\"\"Read-through cache for LLM responses with TTL support.\"\"\"\n",
        "    \n",
        "    def __init__(self, cache_dir: str = \"/tmp/llm_cache_asksage\", default_ttl: int = 3600):\n",
        "        \"\"\"Initialize cache with disk storage and default TTL.\"\"\"\n",
        "        self.cache = dc.Cache(cache_dir)\n",
        "        self.default_ttl = default_ttl\n",
        "        self.hasher = PromptHasher()\n",
        "        \n",
        "        # Statistics\n",
        "        self.hits = 0\n",
        "        self.misses = 0\n",
        "    \n",
        "    def get_cache_key(self, prompt: str, model: str, **kwargs) -> str:\n",
        "        \"\"\"Generate cache key for the request.\"\"\"\n",
        "        return self.hasher.hash_prompt(prompt, model, **kwargs)\n",
        "    \n",
        "    def get(self, cache_key: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get item from cache if not expired.\"\"\"\n",
        "        try:\n",
        "            cached_item = self.cache.get(cache_key)\n",
        "            if cached_item is None:\n",
        "                self.misses += 1\n",
        "                return None\n",
        "            \n",
        "            # Check TTL\n",
        "            if 'expires_at' in cached_item:\n",
        "                if datetime.now() > datetime.fromisoformat(cached_item['expires_at']):\n",
        "                    self.cache.delete(cache_key)\n",
        "                    self.misses += 1\n",
        "                    return None\n",
        "            \n",
        "            self.hits += 1\n",
        "            return cached_item\n",
        "        except Exception as e:\n",
        "            print(f\"Cache get error: {e}\")\n",
        "            self.misses += 1\n",
        "            return None\n",
        "    \n",
        "    def set(self, cache_key: str, response: str, ttl: Optional[int] = None) -> None:\n",
        "        \"\"\"Store item in cache with TTL.\"\"\"\n",
        "        ttl = ttl or self.default_ttl\n",
        "        expires_at = datetime.now() + timedelta(seconds=ttl)\n",
        "        \n",
        "        cache_item = {\n",
        "            'response': response,\n",
        "            'cached_at': datetime.now().isoformat(),\n",
        "            'expires_at': expires_at.isoformat(),\n",
        "            'ttl': ttl\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            self.cache.set(cache_key, cache_item, expire=ttl)\n",
        "        except Exception as e:\n",
        "            print(f\"Cache set error: {e}\")\n",
        "    \n",
        "    def bust_cache(self, pattern: Optional[str] = None) -> int:\n",
        "        \"\"\"Clear cache entries. If pattern provided, clear matching keys only.\"\"\"\n",
        "        if pattern:\n",
        "            # For simplicity, clear all - in production you'd implement pattern matching\n",
        "            cleared = len(self.cache)\n",
        "            self.cache.clear()\n",
        "            return cleared\n",
        "        else:\n",
        "            cleared = len(self.cache)\n",
        "            self.cache.clear()\n",
        "            return cleared\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get cache statistics.\"\"\"\n",
        "        total_requests = self.hits + self.misses\n",
        "        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            'hits': self.hits,\n",
        "            'misses': self.misses,\n",
        "            'total_requests': total_requests,\n",
        "            'hit_rate': f\"{hit_rate:.1f}%\",\n",
        "            'cache_size': len(self.cache)\n",
        "        }\n",
        "\n",
        "# Initialize cache\n",
        "cache = LLMCache(default_ttl=300)  # 5 minute TTL\n",
        "print(\"✅ LLM Cache initialized with 5-minute TTL\")\n",
        "print(f\"Cache stats: {cache.get_stats()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cached AskSage LLM Wrapper\n",
        "\n",
        "Create a wrapper that integrates caching with AskSage LLM calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CachedAskSageLLM:\n",
        "    \"\"\"AskSage LLM wrapper with integrated caching.\"\"\"\n",
        "    \n",
        "    def __init__(self, client: AskSageClient, cache: LLMCache, model: str = \"gpt-5-mini\"):\n",
        "        self.client = client\n",
        "        self.cache = cache\n",
        "        self.model = model\n",
        "    \n",
        "    def query(self, prompt: str, **kwargs) -> str:\n",
        "        \"\"\"Query AskSage with caching (read-through pattern).\"\"\"\n",
        "        # Generate cache key\n",
        "        cache_key = self.cache.get_cache_key(prompt, self.model, **kwargs)\n",
        "        \n",
        "        print(f\"🔍 Cache key: {cache_key}\")\n",
        "        \n",
        "        # Try to get from cache first (read-through)\n",
        "        cached_result = self.cache.get(cache_key)\n",
        "        if cached_result:\n",
        "            print(f\"💾 Cache HIT! Retrieved from cache (TTL: {cached_result['ttl']}s)\")\n",
        "            return cached_result['response']\n",
        "        \n",
        "        print(\"🌐 Cache MISS! Calling AskSage...\")\n",
        "        \n",
        "        # Cache miss - call the actual LLM\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            # Use AskSage client to query with gpt-5-mini\n",
        "            response = self.client.query(\n",
        "                query=prompt,\n",
        "                model=self.model,\n",
        "                **kwargs\n",
        "            )\n",
        "            \n",
        "            # Extract response text from AskSage response\n",
        "            if 'ret' in response:\n",
        "                response_text = response['ret']\n",
        "            elif 'response' in response:\n",
        "                response_text = response['response']\n",
        "            else:\n",
        "                response_text = str(response)\n",
        "            \n",
        "            call_duration = time.time() - start_time\n",
        "            \n",
        "            print(f\"⚡ AskSage call completed in {call_duration:.2f}s\")\n",
        "            \n",
        "            # Store in cache\n",
        "            self.cache.set(cache_key, response_text)\n",
        "            print(f\"💾 Response cached with key: {cache_key}\")\n",
        "            \n",
        "            return response_text\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ AskSage call failed: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def get_cache_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current cache statistics.\"\"\"\n",
        "        return self.cache.get_stats()\n",
        "    \n",
        "    def bust_cache(self) -> int:\n",
        "        \"\"\"Clear the cache.\"\"\"\n",
        "        return self.cache.bust_cache()\n",
        "\n",
        "# Initialize cached AskSage LLM\n",
        "cached_llm = CachedAskSageLLM(ask_sage_client, cache, model=\"gpt-5-mini\")\n",
        "\n",
        "print(\"✅ Cached AskSage LLM wrapper initialized with gpt-5-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Testing Read-Through Cache\n",
        "\n",
        "Demonstrate the cache in action with repeated queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test questions\n",
        "questions = [\n",
        "    \"What is the capital of Japan?\",\n",
        "    \"Explain quantum computing in one sentence.\",\n",
        "    \"What is the capital of Japan?\",  # Duplicate to test cache\n",
        "    \"Name three benefits of caching in software systems.\"\n",
        "]\n",
        "\n",
        "print(\"🚀 Testing read-through cache behavior with AskSage gpt-5-mini\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, question in enumerate(questions, 1):\n",
        "    print(f\"\\n📝 Question {i}: {question}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    response = cached_llm.query(question)\n",
        "    \n",
        "    print(f\"🤖 Response: {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
        "    print(f\"📊 Cache Stats: {cached_llm.get_cache_stats()}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"📈 Final Cache Statistics: {cached_llm.get_cache_stats()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. TTL (Time-To-Live) Demonstration\n",
        "\n",
        "Show how cache entries expire based on TTL settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new cache with very short TTL for demonstration\n",
        "short_ttl_cache = LLMCache(cache_dir=\"/tmp/llm_cache_short_asksage\", default_ttl=5)  # 5 seconds\n",
        "cached_llm_short = CachedAskSageLLM(ask_sage_client, short_ttl_cache, model=\"gpt-5-mini\")\n",
        "\n",
        "print(\"⏰ Testing TTL behavior with AskSage (5-second expiration)\\n\")\n",
        "\n",
        "test_question = \"What is machine learning?\"\n",
        "\n",
        "print(f\"📝 Question: {test_question}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# First call - should be a cache miss\n",
        "print(\"\\n🔥 First call:\")\n",
        "response1 = cached_llm_short.query(test_question)\n",
        "print(f\"Stats: {cached_llm_short.get_cache_stats()}\")\n",
        "\n",
        "# Immediate second call - should be a cache hit\n",
        "print(\"\\n🔄 Immediate second call:\")\n",
        "response2 = cached_llm_short.query(test_question)\n",
        "print(f\"Stats: {cached_llm_short.get_cache_stats()}\")\n",
        "\n",
        "print(\"\\n⏳ Waiting 6 seconds for cache to expire...\")\n",
        "time.sleep(6)\n",
        "\n",
        "# Third call after TTL expiration - should be a cache miss\n",
        "print(\"\\n🔥 Third call (after TTL expiration):\")\n",
        "response3 = cached_llm_short.query(test_question)\n",
        "print(f\"Stats: {cached_llm_short.get_cache_stats()}\")\n",
        "\n",
        "print(\"\\n✅ TTL demonstration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Cache Busting Strategies\n",
        "\n",
        "Demonstrate different approaches to invalidating cache entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"💥 Cache Busting Demonstration\\n\")\n",
        "\n",
        "# Fill cache with several entries\n",
        "test_questions = [\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"How does blockchain work?\",\n",
        "    \"Explain cloud computing.\"\n",
        "]\n",
        "\n",
        "print(\"📚 Filling cache with multiple entries...\")\n",
        "for question in test_questions:\n",
        "    cached_llm.query(question)\n",
        "\n",
        "print(f\"\\n📊 Cache after filling: {cached_llm.get_cache_stats()}\")\n",
        "\n",
        "# Strategy 1: Manual cache busting\n",
        "print(\"\\n🗑️  Strategy 1: Manual cache clear\")\n",
        "cleared_count = cached_llm.bust_cache()\n",
        "print(f\"Cleared {cleared_count} cache entries\")\n",
        "print(f\"📊 Cache after busting: {cached_llm.get_cache_stats()}\")\n",
        "\n",
        "# Strategy 2: Version-based cache busting\n",
        "print(\"\\n🔄 Strategy 2: Version-based cache keys\")\n",
        "\n",
        "class VersionedCache(LLMCache):\n",
        "    \"\"\"Cache with version-based keys for easy invalidation.\"\"\"\n",
        "    \n",
        "    def __init__(self, *args, version=\"v1\", **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.version = version\n",
        "    \n",
        "    def get_cache_key(self, prompt: str, model: str, **kwargs) -> str:\n",
        "        base_key = super().get_cache_key(prompt, model, **kwargs)\n",
        "        return f\"{self.version}:{base_key}\"\n",
        "    \n",
        "    def increment_version(self):\n",
        "        \"\"\"Increment version to invalidate all existing cache entries.\"\"\"\n",
        "        current_num = int(self.version[1:]) if self.version.startswith('v') else 1\n",
        "        self.version = f\"v{current_num + 1}\"\n",
        "        print(f\"🆕 Cache version updated to: {self.version}\")\n",
        "\n",
        "# Test versioned cache\n",
        "versioned_cache = VersionedCache(cache_dir=\"/tmp/llm_cache_versioned_asksage\", version=\"v1\")\n",
        "versioned_llm = CachedAskSageLLM(ask_sage_client, versioned_cache, model=\"gpt-5-mini\")\n",
        "\n",
        "# Add entry with v1\n",
        "test_prompt = \"What is Python?\"\n",
        "versioned_llm.query(test_prompt)\n",
        "print(f\"📊 V1 cache: {versioned_llm.get_cache_stats()}\")\n",
        "\n",
        "# Increment version (effectively busts cache)\n",
        "versioned_cache.increment_version()\n",
        "\n",
        "# Same query with v2 - should be cache miss\n",
        "versioned_llm.query(test_prompt)\n",
        "print(f\"📊 V2 cache: {versioned_llm.get_cache_stats()}\")\n",
        "\n",
        "print(\"\\n✅ Cache busting strategies demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Comparison\n",
        "\n",
        "Compare performance with and without caching using AskSage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "print(\"🏁 Performance Comparison: Cached vs Uncached AskSage calls\\n\")\n",
        "\n",
        "# Test questions (some duplicates to show cache benefits)\n",
        "performance_questions = [\n",
        "    \"What is the meaning of life?\",\n",
        "    \"How does photosynthesis work?\",\n",
        "    \"What is the meaning of life?\",  # Duplicate\n",
        "    \"Explain the theory of relativity.\",\n",
        "    \"How does photosynthesis work?\",  # Duplicate\n",
        "    \"What is quantum mechanics?\",\n",
        "    \"What is the meaning of life?\",  # Another duplicate\n",
        "]\n",
        "\n",
        "# Test without cache (regular AskSage client)\n",
        "print(\"⚡ Testing WITHOUT cache...\")\n",
        "uncached_times = []\n",
        "for i, question in enumerate(performance_questions, 1):\n",
        "    start_time = time.time()\n",
        "    ask_sage_client.query(query=question, model=\"gpt-5-mini\")\n",
        "    duration = time.time() - start_time\n",
        "    uncached_times.append(duration)\n",
        "    print(f\"  Query {i}: {duration:.2f}s\")\n",
        "\n",
        "print(\"\\n💾 Testing WITH cache...\")\n",
        "# Reset cache for fair comparison\n",
        "cache.bust_cache()\n",
        "cached_times = []\n",
        "for i, question in enumerate(performance_questions, 1):\n",
        "    start_time = time.time()\n",
        "    cached_llm.query(question)\n",
        "    duration = time.time() - start_time\n",
        "    cached_times.append(duration)\n",
        "    print(f\"  Query {i}: {duration:.2f}s\")\n",
        "\n",
        "# Calculate statistics\n",
        "uncached_total = sum(uncached_times)\n",
        "cached_total = sum(cached_times)\n",
        "uncached_avg = statistics.mean(uncached_times)\n",
        "cached_avg = statistics.mean(cached_times)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📊 PERFORMANCE RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Without Cache:\")\n",
        "print(f\"  Total Time: {uncached_total:.2f}s\")\n",
        "print(f\"  Average per query: {uncached_avg:.2f}s\")\n",
        "print(f\"\\nWith Cache:\")\n",
        "print(f\"  Total Time: {cached_total:.2f}s\")\n",
        "print(f\"  Average per query: {cached_avg:.2f}s\")\n",
        "print(f\"\\n🚀 Performance Improvement:\")\n",
        "print(f\"  Time saved: {uncached_total - cached_total:.2f}s\")\n",
        "print(f\"  Speed improvement: {((uncached_total / cached_total - 1) * 100):.1f}%\")\n",
        "print(f\"\\n📈 Final cache stats: {cached_llm.get_cache_stats()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this demo, we covered key caching concepts for LLM applications using AskSage's gpt-5-mini:\n",
        "\n",
        "### 1. **Prompt Hashing**\n",
        "- Created consistent hash keys from prompt components\n",
        "- Included model parameters in hash calculation\n",
        "- Ensured identical prompts produce identical cache keys\n",
        "\n",
        "### 2. **Read-Through Cache**\n",
        "- Implemented automatic cache population on cache misses\n",
        "- Used disk-based storage for persistence\n",
        "- Provided transparent caching interface\n",
        "\n",
        "### 3. **TTL (Time-To-Live)**\n",
        "- Configured automatic cache expiration\n",
        "- Prevented stale data from being served\n",
        "- Balanced performance with freshness\n",
        "\n",
        "### 4. **Cache Busting**\n",
        "- Manual cache clearing for immediate invalidation\n",
        "- Version-based keys for gradual cache invalidation\n",
        "- Strategic cache management for different scenarios\n",
        "\n",
        "### 5. **AskSage Integration**\n",
        "- Used AskSage Python client with gpt-5-mini model\n",
        "- Maintained API compatibility while adding caching\n",
        "- Demonstrated enterprise-grade LLM usage patterns\n",
        "\n",
        "### Key Benefits Demonstrated:\n",
        "- **Performance**: Significant reduction in response times for repeated queries\n",
        "- **Cost Savings**: Reduced API calls to expensive LLM services\n",
        "- **Scalability**: Better handling of high-frequency, repetitive requests\n",
        "- **Reliability**: Consistent responses for identical inputs\n",
        "- **Enterprise Ready**: Integration with AskSage's enterprise platform"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}