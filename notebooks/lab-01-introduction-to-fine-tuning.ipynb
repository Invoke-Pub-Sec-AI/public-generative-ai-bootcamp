{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "e3fba08897a74393a286426ca7c905ed",
      "metadata": {},
      "source": [
        "# Lab 01: Introduction to Fine-Tuning\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand what fine-tuning is and why it's important\n",
        "- Prepare a dataset for a fine-tuning task\n",
        "- Implement a basic fine-tuning loop from scratch\n",
        "- Evaluate a fine-tuned model's performance\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3fe57a7a2d43e09f07d9d83d09c824",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "342874eab50a4ce599020a86aa6dcdf4",
      "metadata": {},
      "source": [
        "## Part 1: What is Fine-Tuning?\n",
        "\n",
        "Fine-tuning is the process of taking a pre-trained Large Language Model (LLM) and further training it on a smaller, task-specific dataset. This adapts the model's general knowledge to excel at a particular task, such as classification, summarization, or, in our case, question answering.\n",
        "\n",
        "**Why Fine-Tune?**\n",
        "- **Improved Performance**: Achieves state-of-the-art results on specific tasks.\n",
        "- **Domain Adaptation**: Tailors the model to specific jargon, styles, or knowledge domains.\n",
        "- **Efficiency**: Much cheaper and faster than training a model from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19411a4c0f53427bb81fd566d595b335",
      "metadata": {},
      "source": [
        "## Part 2: Preparing a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eead6fec608469f88ba06f057db7f89",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrainingExample:\n",
        "    prompt: str\n",
        "    completion: str\n",
        "\n",
        "# A simple dataset for a chatbot that knows capital cities\n",
        "dataset = [\n",
        "    TrainingExample(prompt='What is the capital of France?', completion='Paris'),\n",
        "    TrainingExample(prompt='What is the capital of Japan?', completion='Tokyo'),\n",
        "    TrainingExample(prompt='What is the capital of Canada?', completion='Ottawa'),\n",
        "    TrainingExample(prompt='What is the capital of Australia?', completion='Canberra'),\n",
        "    TrainingExample(prompt='What is the capital of Brazil?', completion='BrasÃ­lia'),\n",
        "]\n",
        "\n",
        "# In a real scenario, we would tokenize the text into integer IDs.\n",
        "# Here, we'll use a mock tokenizer.\n",
        "class MockTokenizer:\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # Simple character-to-integer mapping\n",
        "        return [ord(c) for c in text]\n",
        "    \n",
        "    def decode(self, tokens: List[int]) -> str:\n",
        "        return \"\".join([chr(t) for t in tokens])\n",
        "\n",
        "tokenizer = MockTokenizer()\n",
        "\n",
        "# Example of tokenization\n",
        "encoded_prompt = tokenizer.encode(dataset[0].prompt)\n",
        "print(f'Original: {dataset[0].prompt}')\n",
        "print(f'Encoded: {encoded_prompt}')\n",
        "print(f'Decoded: {tokenizer.decode(encoded_prompt)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f6efe09b994738aa30728f5381fbdc",
      "metadata": {},
      "source": [
        "## Part 3: The Fine-Tuning Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557109094dc54d9c909d3b451726e6e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockLLM:\n",
        "    \"\"\"A simplified mock LLM with a single weight matrix.\"\"\"\n",
        "    def __init__(self, vocab_size=256, dim=32):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        # A single linear layer represents the model's knowledge\n",
        "        self.weights = np.random.randn(dim, vocab_size) * 0.1\n",
        "\n",
        "    def forward(self, input_tokens: List[int]) -> np.ndarray:\n",
        "        # Super simple forward pass: average input embeddings\n",
        "        # In reality, this is a massive transformer architecture\n",
        "        input_vectors = np.array([self.weights[:, token] for token in input_tokens])\n",
        "        avg_vector = np.mean(input_vectors, axis=0)\n",
        "        # Output logits for the next token\n",
        "        logits = np.dot(avg_vector, self.weights)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, prompt: str, tokenizer: MockTokenizer) -> str:\n",
        "        tokens = tokenizer.encode(prompt)\n",
        "        logits = self.forward(tokens)\n",
        "        next_token = np.argmax(logits)\n",
        "        # In a real model, we'd generate token by token. Here, we simplify.\n",
        "        # Let's pretend the model's top prediction is a single character.\n",
        "        return tokenizer.decode([next_token])\n",
        "\n",
        "def loss_fn(logits: np.ndarray, target_tokens: List[int]) -> Tuple[float, np.ndarray]:\n",
        "    \"\"\"Cross-entropy loss (simplified).\"\"\"\n",
        "    # We'll use a simple MSE loss for simplicity, as implementing softmax + cross-entropy is verbose\n",
        "    target_one_hot = np.zeros_like(logits)\n",
        "    target_one_hot[target_tokens[0]] = 1 # Assume we only predict the first token of completion\n",
        "    \n",
        "    loss = np.mean((logits - target_one_hot)**2)\n",
        "    grad = 2 * (logits - target_one_hot) / logits.size\n",
        "    return loss, grad\n",
        "\n",
        "def fine_tune(model: MockLLM, dataset: List[TrainingExample], tokenizer: MockTokenizer, epochs: int, lr: float):\n",
        "    \"\"\"A basic fine-tuning loop.\"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for example in dataset:\n",
        "            # Forward pass\n",
        "            input_tokens = tokenizer.encode(example.prompt)\n",
        "            logits = model.forward(input_tokens)\n",
        "            \n",
        "            # Calculate loss\n",
        "            target_tokens = tokenizer.encode(example.completion)\n",
        "            loss, grad = loss_fn(logits, target_tokens)\n",
        "            total_loss += loss\n",
        "            \n",
        "            # Backward pass (simplified gradient update)\n",
        "            # This is a huge simplification of backpropagation\n",
        "            input_vectors = np.array([model.weights[:, token] for token in input_tokens])\n",
        "            avg_vector = np.mean(input_vectors, axis=0).reshape(-1, 1)\n",
        "            grad_reshaped = grad.reshape(1, -1)\n",
        "            weight_grad = np.dot(avg_vector, grad_reshaped)\n",
        "            model.weights -= lr * weight_grad\n",
        "            \n",
        "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {total_loss / len(dataset):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef11635175264b619d7ebe8eebcb9bb9",
      "metadata": {},
      "source": [
        "## Part 4: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10dc0f46d744dc4931051bd4e17e8df",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model: MockLLM, test_dataset: List[TrainingExample]):\n",
        "    correct = 0\n",
        "    for example in test_dataset:\n",
        "        # Our mock generation is very basic, so we'll check if the first letter matches\n",
        "        predicted_char = model.generate(example.prompt, tokenizer)\n",
        "        if example.completion and predicted_char == example.completion[0]:\n",
        "            correct += 1\n",
        "    return correct / len(test_dataset)\n",
        "\n",
        "# Create a test set\n",
        "test_dataset = [\n",
        "    ,TrainingExample(prompt=\"What is the capital of Germany?completion=\"Berlin\"\n",
        "    ),\n",
        "    ,TrainingExample(prompt=\"What is the capital of Italy?completion=\"Rome\"\n",
        "    ),\n",
        "    ]\n",
        "\n",
        "# Initialize model\n",
        "model = MockLLM()\n",
        "\n",
        "# Evaluate before fine-tuning\n",
        "accuracy_before = evaluate_model(model, dataset + test_dataset)\n",
        "print(f'Accuracy before fine-tuning: {accuracy_before:.2%}')\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tune(model, dataset, tokenizer, epochs=100, lr=0.01)\n",
        "\n",
        "# Evaluate after fine-tuning\n",
        "accuracy_after = evaluate_model(model, dataset + test_dataset)\n",
        "print(f'Accuracy after fine-tuning: {accuracy_after:.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1526b44cffd41c987325e1ff2f61003",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Experiment with Hyperparameters**: Change the learning rate (`lr`) and number of `epochs`. How does it affect the final accuracy? What happens if the learning rate is too high or too low?\n",
        "2. **Expand the Dataset**: Add more country-capital pairs to the `dataset`. Does the model's accuracy on the test set improve?\n",
        "3. **Implement a Better Evaluation Metric**: The current evaluation only checks the first character. Modify `evaluate_model` to check for the full word completion (you may need to adjust the `generate` function to produce more tokens).\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this lab, you learned:\n",
        "- The core concept of fine-tuning a pre-trained model.\n",
        "- How to structure a dataset for a question-answering task.\n",
        "- The components of a fine-tuning loop: forward pass, loss calculation, and backward pass (gradient update).\n",
        "- The importance of evaluating a model before and after fine-tuning to measure improvement."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}