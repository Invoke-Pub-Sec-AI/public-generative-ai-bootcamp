{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "2dd96ce8e06942249bbd4c64aa6e1523",
      "metadata": {},
      "source": [
        "# Lab 01: Introduction to LLM Evaluation\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand evaluation metrics for LLM applications\n",
        "- Implement basic evaluation frameworks\n",
        "- Create evaluation datasets\n",
        "- Measure model performance systematically\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c657a39c96842c992f1a9d6fe236c15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c188c9f305a44d498611cbfe91999134",
      "metadata": {},
      "source": [
        "## Part 1: Evaluation Framework Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86978b0cbbb34236b27f0f8eeced1913",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class EvaluationCase:\n",
        "    \"\"\"Single evaluation test case\"\"\"\n",
        "    case_id: str\n",
        "    input_text: str\n",
        "    expected_output: str\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "    \n",
        "@dataclass\n",
        "class EvaluationResult:\n",
        "    \"\"\"Result of a single evaluation\"\"\"\n",
        "    case_id: str\n",
        "    actual_output: str\n",
        "    expected_output: str\n",
        "    score: float\n",
        "    metrics: Dict[str, float]\n",
        "    execution_time: float\n",
        "    error: Optional[str] = None\n",
        "\n",
        "class EvaluationMetric(Enum):\n",
        "    \"\"\"Available evaluation metrics\"\"\"\n",
        "    EXACT_MATCH = \"exact_match\"\n",
        "    CONTAINS = \"contains\"\n",
        "    SEMANTIC_SIMILARITY = \"semantic_similarity\"\n",
        "    REGEX_MATCH = \"regex_match\"\n",
        "    CUSTOM = \"custom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987d864f9af34f80afb632d0a1cccc08",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLMEvaluator:\n",
        "    \"\"\"Base evaluation framework for LLM applications\"\"\"\n",
        "    \n",
        "    def __init__(self, name: str = \"default_evaluator\"):\n",
        "        self.name = name\n",
        "        self.results: List[EvaluationResult] = []\n",
        "        \n",
        "    def evaluate_single(\n",
        "        self,\n",
        "        case: EvaluationCase,\n",
        "        llm_function,\n",
        "        metric: EvaluationMetric = EvaluationMetric.EXACT_MATCH\n",
        "    ) -> EvaluationResult:\n",
        "        \"\"\"Evaluate a single test case\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Get LLM output\n",
        "            actual_output = llm_function(case.input_text)\n",
        "            \n",
        "            # Calculate score based on metric\n",
        "            score = self._calculate_score(\n",
        "                actual_output,\n",
        "                case.expected_output,\n",
        "                metric\n",
        "            )\n",
        "            \n",
        "            # Additional metrics\n",
        "            metrics = {\n",
        "                \"length_ratio\": len(actual_output) / max(len(case.expected_output), 1),\n",
        "                \"metric_type\": metric.value\n",
        "            }\n",
        "            \n",
        "            result = EvaluationResult(\n",
        "                case_id=case.case_id,\n",
        "                actual_output=actual_output,\n",
        "                expected_output=case.expected_output,\n",
        "                score=score,\n",
        "                metrics=metrics,\n",
        "                execution_time=time.time() - start_time\n",
        "            )\n",
        "            \n",
        "        except Exception as e:\n",
        "            result = EvaluationResult(\n",
        "                case_id=case.case_id,\n",
        "                actual_output=\"\",\n",
        "                expected_output=case.expected_output,\n",
        "                score=0.0,\n",
        "                metrics={},\n",
        "                execution_time=time.time() - start_time,\n",
        "                error=str(e)\n",
        "            )\n",
        "        \n",
        "        self.results.append(result)\n",
        "        return result\n",
        "    \n",
        "    def _calculate_score(\n",
        "        self,\n",
        "        actual: str,\n",
        "        expected: str,\n",
        "        metric: EvaluationMetric\n",
        "    ) -> float:\n",
        "        \"\"\"Calculate score based on metric type\"\"\"\n",
        "        if metric == EvaluationMetric.EXACT_MATCH:\n",
        "            return 1.0 if actual.strip() == expected.strip() else 0.0\n",
        "        \n",
        "        elif metric == EvaluationMetric.CONTAINS:\n",
        "            return 1.0 if expected.lower() in actual.lower() else 0.0\n",
        "        \n",
        "        elif metric == EvaluationMetric.SEMANTIC_SIMILARITY:\n",
        "            # Simplified - in production use embeddings\n",
        "            common_words = set(actual.lower().split()) & set(expected.lower().split())\n",
        "            total_words = set(actual.lower().split()) | set(expected.lower().split())\n",
        "            return len(common_words) / max(len(total_words), 1)\n",
        "        \n",
        "        else:\n",
        "            return 0.0\n",
        "    \n",
        "    def evaluate_dataset(\n",
        "        self,\n",
        "        cases: List[EvaluationCase],\n",
        "        llm_function,\n",
        "        metric: EvaluationMetric = EvaluationMetric.EXACT_MATCH\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate entire dataset\"\"\"\n",
        "        for case in cases:\n",
        "            self.evaluate_single(case, llm_function, metric)\n",
        "        \n",
        "        return self.get_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11cb4c6d8b448088fe77ab99297153d",
      "metadata": {},
      "source": [
        "## Part 2: Creating Evaluation Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4668ddf4edb843b28dd7445e2a50761c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EvaluationDatasetBuilder:\n",
        "    \"\"\"Build and manage evaluation datasets\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_classification_dataset() -> List[EvaluationCase]:\n",
        "        \"\"\"Create dataset for classification tasks\"\"\"\n",
        "        return [\n",
        "            EvaluationCase(\n",
        "                case_id=\"cls_001\",\n",
        "                input_text=\"The movie was fantastic! Best I've seen all year.\",\n",
        "                expected_output=\"positive\",\n",
        "                metadata={\"category\": \"sentiment\", \"difficulty\": \"easy\"}\n",
        "            ),\n",
        "            EvaluationCase(\n",
        "                case_id=\"cls_002\",\n",
        "                input_text=\"The service was terrible and the food was cold.\",\n",
        "                expected_output=\"negative\",\n",
        "                metadata={\"category\": \"sentiment\", \"difficulty\": \"easy\"}\n",
        "            ),\n",
        "            EvaluationCase(\n",
        "                case_id=\"cls_003\",\n",
        "                input_text=\"It was okay, nothing special but not bad either.\",\n",
        "                expected_output=\"neutral\",\n",
        "                metadata={\"category\": \"sentiment\", \"difficulty\": \"medium\"}\n",
        "            )\n",
        "        ]\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_extraction_dataset() -> List[EvaluationCase]:\n",
        "        \"\"\"Create dataset for information extraction\"\"\"\n",
        "        return [\n",
        "            EvaluationCase(\n",
        "                case_id=\"ext_001\",\n",
        "                input_text=\"John Smith lives at 123 Main St, Boston, MA 02101\",\n",
        "                expected_output=json.dumps({\n",
        "                    \"name\": \"John Smith\",\n",
        "                    \"address\": \"123 Main St\",\n",
        "                    \"city\": \"Boston\",\n",
        "                    \"state\": \"MA\",\n",
        "                    \"zip\": \"02101\"\n",
        "                }),\n",
        "                metadata={\"category\": \"address_extraction\"}\n",
        "            ),\n",
        "            EvaluationCase(\n",
        "                case_id=\"ext_002\",\n",
        "                input_text=\"Contact me at john@example.com or call 555-1234\",\n",
        "                expected_output=json.dumps({\n",
        "                    \"email\": \"john@example.com\",\n",
        "                    \"phone\": \"555-1234\"\n",
        "                }),\n",
        "                metadata={\"category\": \"contact_extraction\"}\n",
        "            )\n",
        "        ]\n",
        "    \n",
        "    @staticmethod\n",
        "    def save_dataset(cases: List[EvaluationCase], filename: str):\n",
        "        \"\"\"Save dataset to JSON file\"\"\"\n",
        "        data = [\n",
        "            {\n",
        "                \"case_id\": case.case_id,\n",
        "                \"input_text\": case.input_text,\n",
        "                \"expected_output\": case.expected_output,\n",
        "                \"metadata\": case.metadata\n",
        "            }\n",
        "            for case in cases\n",
        "        ]\n",
        "        \n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "    \n",
        "    @staticmethod\n",
        "    def load_dataset(filename: str) -> List[EvaluationCase]:\n",
        "        \"\"\"Load dataset from JSON file\"\"\"\n",
        "        with open(filename, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        return [\n",
        "            EvaluationCase(\n",
        "                case_id=item[\"case_id\"],\n",
        "                input_text=item[\"input_text\"],\n",
        "                expected_output=item[\"expected_output\"],\n",
        "                metadata=item.get(\"metadata\", {})\n",
        "            )\n",
        "            for item in data\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c423daf4cc42049a27fa8913e1a964",
      "metadata": {},
      "source": [
        "## Part 3: Evaluation Metrics & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d12b64189c43a4ad54056943ca8871",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EvaluationAnalyzer:\n",
        "    \"\"\"Analyze evaluation results\"\"\"\n",
        "    \n",
        "    def __init__(self, results: List[EvaluationResult]):\n",
        "        self.results = results\n",
        "        self.df = self._results_to_dataframe()\n",
        "    \n",
        "    def _results_to_dataframe(self) -> pd.DataFrame:\n",
        "        \"\"\"Convert results to DataFrame for analysis\"\"\"\n",
        "        data = []\n",
        "        for result in self.results:\n",
        "            data.append({\n",
        "                'case_id': result.case_id,\n",
        "                'score': result.score,\n",
        "                'execution_time': result.execution_time,\n",
        "                'has_error': result.error is not None,\n",
        "                **result.metrics\n",
        "            })\n",
        "        return pd.DataFrame(data)\n",
        "    \n",
        "    def get_summary_statistics(self) -> Dict[str, float]:\n",
        "        \"\"\"Get summary statistics\"\"\"\n",
        "        return {\n",
        "            'mean_score': self.df['score'].mean(),\n",
        "            'std_score': self.df['score'].std(),\n",
        "            'min_score': self.df['score'].min(),\n",
        "            'max_score': self.df['score'].max(),\n",
        "            'success_rate': (self.df['score'] > 0.5).mean(),\n",
        "            'mean_execution_time': self.df['execution_time'].mean(),\n",
        "            'error_rate': self.df['has_error'].mean()\n",
        "        }\n",
        "    \n",
        "    def plot_results(self):\n",
        "        \"\"\"Visualize evaluation results\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "        \n",
        "        # Score distribution\n",
        "        axes[0, 0].hist(self.df['score'], bins=20, edgecolor='black')\n",
        "        axes[0, 0].set_title('Score Distribution')\n",
        "        axes[0, 0].set_xlabel('Score')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        \n",
        "        # Execution time\n",
        "        axes[0, 1].scatter(range(len(self.df)), self.df['execution_time'])\n",
        "        axes[0, 1].set_title('Execution Time by Case')\n",
        "        axes[0, 1].set_xlabel('Case Index')\n",
        "        axes[0, 1].set_ylabel('Time (seconds)')\n",
        "        \n",
        "        # Score vs execution time\n",
        "        axes[1, 0].scatter(self.df['execution_time'], self.df['score'])\n",
        "        axes[1, 0].set_title('Score vs Execution Time')\n",
        "        axes[1, 0].set_xlabel('Execution Time (s)')\n",
        "        axes[1, 0].set_ylabel('Score')\n",
        "        \n",
        "        # Success rate by metric type (if available)\n",
        "        if 'metric_type' in self.df.columns:\n",
        "            metric_scores = self.df.groupby('metric_type')['score'].mean()\n",
        "            axes[1, 1].bar(metric_scores.index, metric_scores.values)\n",
        "            axes[1, 1].set_title('Average Score by Metric Type')\n",
        "            axes[1, 1].set_xlabel('Metric Type')\n",
        "            axes[1, 1].set_ylabel('Average Score')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def get_failure_analysis(self, threshold: float = 0.5) -> pd.DataFrame:\n",
        "        \"\"\"Analyze failed cases\"\"\"\n",
        "        failures = self.df[self.df['score'] < threshold]\n",
        "        return failures[['case_id', 'score', 'execution_time', 'has_error']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ecbfb8d6b9e4c088bc42d2b150bc639",
      "metadata": {},
      "source": [
        "## Part 4: Practical Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49de6aa88d04be9b34ff49742d45e22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mock LLM function for testing\n",
        "def mock_sentiment_classifier(text: str) -> str:\n",
        "    \"\"\"Mock sentiment classifier for demonstration\"\"\"\n",
        "    positive_words = ['fantastic', 'great', 'excellent', 'amazing', 'best']\n",
        "    negative_words = ['terrible', 'bad', 'awful', 'worst', 'horrible']\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    if any(word in text_lower for word in positive_words):\n",
        "        return \"positive\"\n",
        "    elif any(word in text_lower for word in negative_words):\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Create evaluation dataset\n",
        "dataset = EvaluationDatasetBuilder.create_classification_dataset()\n",
        "\n",
        "# Run evaluation\n",
        "evaluator = LLMEvaluator(\"sentiment_evaluator\")\n",
        "results = evaluator.evaluate_dataset(\n",
        "    cases=dataset,\n",
        "    llm_function=mock_sentiment_classifier,\n",
        "    metric=EvaluationMetric.EXACT_MATCH\n",
        ")\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff6f0de48dd4c228abcde503ed5c333",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze results\n",
        "analyzer = EvaluationAnalyzer(evaluator.results)\n",
        "\n",
        "# Get summary statistics\n",
        "stats = analyzer.get_summary_statistics()\n",
        "print(\"Summary Statistics:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"{key}: {value:.3f}\")\n",
        "\n",
        "# Plot results\n",
        "analyzer.plot_results()\n",
        "\n",
        "# Analyze failures\n",
        "failures = analyzer.get_failure_analysis()\n",
        "if not failures.empty:\n",
        "    print(\"\\nFailed Cases:\")\n",
        "    print(failures)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99892d7d0bb34eb3a8a2b872f9615acb",
      "metadata": {},
      "source": [
        "## Part 5: Exercises\n",
        "\n",
        "Complete these exercises to practice evaluation techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cfa92e664af43c2bde7893c36710115",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Create a custom evaluation metric\n",
        "# TODO: Implement a metric that checks if the output contains all required keywords\n",
        "# Example: For input \"list fruits\", output should contain \"apple\", \"banana\", \"orange\"\n",
        "\n",
        "def keyword_coverage_metric(actual: str, expected_keywords: List[str]) -> float:\n",
        "    \"\"\"Calculate what percentage of expected keywords are in the output\"\"\"\n",
        "    # Your code here:\n",
        "    pass\n",
        "\n",
        "\n",
        "# Exercise 2: Implement A/B testing for prompts\n",
        "# TODO: Create a function that compares two different prompts on the same dataset\n",
        "\n",
        "def compare_prompts(prompt_a: str, prompt_b: str, test_cases: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"Compare performance of two prompts\"\"\"\n",
        "    # Your code here:\n",
        "    pass\n",
        "\n",
        "\n",
        "# Exercise 3: Build an evaluation pipeline\n",
        "# TODO: Create a complete pipeline that:\n",
        "# 1. Loads a dataset\n",
        "# 2. Runs multiple evaluation metrics\n",
        "# 3. Generates a report\n",
        "# 4. Saves results to file\n",
        "\n",
        "class EvaluationPipeline:\n",
        "    \"\"\"Complete evaluation pipeline\"\"\"\n",
        "    # Your code here:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f49e0d097c41c8bd4d988e483f6628",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you learned:\n",
        "- How to build evaluation frameworks for LLM applications\n",
        "- Different types of evaluation metrics\n",
        "- How to create and manage evaluation datasets\n",
        "- Techniques for analyzing evaluation results\n",
        "\n",
        "Next steps:\n",
        "- Implement more sophisticated metrics (BLEU, ROUGE, etc.)\n",
        "- Build automated evaluation pipelines\n",
        "- Integrate with CI/CD systems"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}