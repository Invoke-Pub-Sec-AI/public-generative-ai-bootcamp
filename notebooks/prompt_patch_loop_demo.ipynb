{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Diagnosis and Patch Loop Demo\n",
        "\n",
        "## Learning Objectives\n",
        "- Learn systematic approaches to diagnosing prompt failures\n",
        "- Practice iterative prompt improvement techniques\n",
        "- Understand common failure patterns and their solutions\n",
        "- Build a feedback loop for prompt optimization\n",
        "\n",
        "## The Problem: When Good Prompts Go Bad\n",
        "\n",
        "Even well-crafted prompts can fail in unexpected ways. This demo shows how to:\n",
        "1. **Identify** what went wrong\n",
        "2. **Diagnose** the root cause  \n",
        "3. **Patch** the prompt systematically\n",
        "4. **Validate** the improvement\n",
        "5. **Iterate** until satisfactory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install asksageclient pip_system_certs rich pandas difflib tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# üîê Cell 1 ‚Äî Load secrets (Colab) + pricing + token utils\n",
        "# ================================\n",
        "import os, time, csv\n",
        "from typing import Optional, Dict\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "ASKSAGE_API_KEY = userdata.get(\"ASKSAGE_API_KEY\")\n",
        "ASKSAGE_BASE_URL = userdata.get(\"ASKSAGE_BASE_URL\")\n",
        "ASKSAGE_EMAIL = userdata.get(\"ASKSAGE_EMAIL\")\n",
        "\n",
        "assert ASKSAGE_API_KEY, \"ASKSAGE_API_KEY not provided.\"\n",
        "assert ASKSAGE_EMAIL, \"ASKSAGE_EMAIL not provided.\"\n",
        "\n",
        "print(\"‚úì Secrets loaded\")\n",
        "print(\"  ‚Ä¢ EMAIL:\", ASKSAGE_EMAIL)\n",
        "print(\"  ‚Ä¢ BASE URL:\", ASKSAGE_BASE_URL or \"(default)\")\n",
        "\n",
        "# Pricing (USD per 1,000,000 tokens)\n",
        "PRICES_PER_M = {\n",
        "    \"gpt-5\": {\"input_per_m\": 1.25, \"output_per_m\": 10.00},\n",
        "    \"gpt-5-mini\": {\"input_per_m\": 0.25, \"output_per_m\": 2.00},\n",
        "}\n",
        "\n",
        "# Tokenizer\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text or \"\"))\n",
        "\n",
        "def cost_usd(model: str, input_tokens: int, output_tokens: int) -> float:\n",
        "    if model not in PRICES_PER_M:\n",
        "        raise ValueError(f\"Unknown model: {model}\")\n",
        "    r = PRICES_PER_M[model]\n",
        "    return (input_tokens / 1_000_000) * r[\"input_per_m\"] + (output_tokens / 1_000_000) * r[\"output_per_m\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# üîß Cell 2 ‚Äî Import bootcamp_common and setup AskSage client\n",
        "# ================================\n",
        "import sys\n",
        "sys.path.append('../../../')  # Adjust path to reach bootcamp_common\n",
        "\n",
        "from bootcamp_common.ask_sage import AskSageClient\n",
        "import json, difflib\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import pandas as pd\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.panel import Panel\n",
        "from rich.syntax import Syntax\n",
        "\n",
        "# Initialize AskSage client\n",
        "client = AskSageClient(\n",
        "    api_key=ASKSAGE_API_KEY,\n",
        "    base_url=ASKSAGE_BASE_URL\n",
        ")\n",
        "\n",
        "console = Console()\n",
        "print(\"‚úì AskSage client initialized\")\n",
        "print(\"‚úÖ Libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scenario: Code Documentation Generator\n",
        "\n",
        "We'll build a prompt to generate documentation for Python functions, then systematically improve it when we encounter failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PromptIteration:\n",
        "    version: int\n",
        "    prompt: str\n",
        "    test_input: str\n",
        "    expected_output: str\n",
        "    actual_output: str\n",
        "    success: bool\n",
        "    issues_found: List[str]\n",
        "    fixes_applied: List[str]\n",
        "    timestamp: str\n",
        "\n",
        "class PromptDiagnosticTool:\n",
        "    \"\"\"Tool for diagnosing and fixing prompt issues\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.setup_client()\n",
        "        self.iterations = []\n",
        "        \n",
        "        # Test cases for our documentation generator\n",
        "        self.test_cases = [\n",
        "            {\n",
        "                'input': '''def calculate_fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)''',\n",
        "                'expected_elements': ['parameters', 'returns', 'description', 'example']\n",
        "            },\n",
        "            {\n",
        "                'input': '''def process_data(data, filter_func=None, transform=True):\n",
        "    if filter_func:\n",
        "        data = [x for x in data if filter_func(x)]\n",
        "    if transform:\n",
        "        data = [str(x).upper() for x in data]\n",
        "    return data''',\n",
        "                'expected_elements': ['parameters', 'returns', 'description', 'example']\n",
        "            }\n",
        "        ]\n",
        "    \n",
        "    def setup_client(self):\n",
        "        \"\"\"Setup API client with fallback to mock\"\"\"\n",
        "        if os.getenv('OPENAI_API_KEY'):\n",
        "            try:\n",
        "                self.client = openai.OpenAI()\n",
        "                self.has_api = True\n",
        "                console.print(\"‚úÖ OpenAI client configured\")\n",
        "            except Exception as e:\n",
        "                self.has_api = False\n",
        "                console.print(f\"‚ö†Ô∏è Using mock responses: {e}\")\n",
        "        else:\n",
        "            self.has_api = False\n",
        "            console.print(\"üí° No API key found, using mock responses\")\n",
        "    \n",
        "    def test_prompt(self, prompt: str, test_input: str, version: int) -> Dict:\n",
        "        \"\"\"Test a prompt with given input\"\"\"\n",
        "        \n",
        "        full_prompt = prompt.format(code=test_input)\n",
        "        \n",
        "        if self.has_api:\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "                    max_tokens=300,\n",
        "                    temperature=0.3\n",
        "                )\n",
        "                return {\n",
        "                    'output': response.choices[0].message.content,\n",
        "                    'success': True\n",
        "                }\n",
        "            except Exception as e:\n",
        "                return {\n",
        "                    'output': f\"Error: {str(e)}\",\n",
        "                    'success': False\n",
        "                }\n",
        "        else:\n",
        "            # Mock responses that demonstrate different failure patterns\n",
        "            mock_responses = {\n",
        "                1: \"This function calculates Fibonacci numbers.\",  # Too brief\n",
        "                2: \"\"\"This function calculates Fibonacci numbers recursively.\n",
        "                \n",
        "Args:\n",
        "    n: A number\n",
        "    \n",
        "Returns:\n",
        "    The nth Fibonacci number\"\"\",  # Better but missing example\n",
        "                3: \"\"\"Calculate the nth Fibonacci number using recursion.\n",
        "                \n",
        "Args:\n",
        "    n (int): The position in the Fibonacci sequence (non-negative integer)\n",
        "    \n",
        "Returns:\n",
        "    int: The nth Fibonacci number\n",
        "    \n",
        "Example:\n",
        "    >>> calculate_fibonacci(5)\n",
        "    5\n",
        "    >>> calculate_fibonacci(10)\n",
        "    55\n",
        "    \n",
        "Note:\n",
        "    This implementation has exponential time complexity. Consider using\n",
        "    dynamic programming for large values of n.\"\"\"\n",
        "            }\n",
        "            \n",
        "            return {\n",
        "                'output': mock_responses.get(version, mock_responses[3]),\n",
        "                'success': True\n",
        "            }\n",
        "    \n",
        "    def diagnose_issues(self, output: str, expected_elements: List[str]) -> List[str]:\n",
        "        \"\"\"Diagnose issues with the generated output\"\"\"\n",
        "        issues = []\n",
        "        output_lower = output.lower()\n",
        "        \n",
        "        # Check for missing elements\n",
        "        if 'parameters' in expected_elements or 'args' in expected_elements:\n",
        "            if not any(word in output_lower for word in ['args:', 'parameters:', 'param']):\n",
        "                issues.append(\"Missing parameter documentation\")\n",
        "        \n",
        "        if 'returns' in expected_elements:\n",
        "            if not any(word in output_lower for word in ['returns:', 'return']):\n",
        "                issues.append(\"Missing return value documentation\")\n",
        "        \n",
        "        if 'example' in expected_elements:\n",
        "            if not any(word in output_lower for word in ['example', '>>>', 'usage']):\n",
        "                issues.append(\"Missing usage example\")\n",
        "        \n",
        "        # Check for quality issues\n",
        "        if len(output.split()) < 10:\n",
        "            issues.append(\"Documentation too brief\")\n",
        "        \n",
        "        if not any(char in output for char in ['.', '!', '?']):\n",
        "            issues.append(\"Missing proper sentence structure\")\n",
        "        \n",
        "        # Check for type hints\n",
        "        if ':' in output and '(' in output:\n",
        "            if not any(word in output for word in ['int', 'str', 'list', 'dict', 'bool']):\n",
        "                issues.append(\"Missing or vague type information\")\n",
        "        \n",
        "        return issues\n",
        "    \n",
        "    def generate_fixes(self, issues: List[str], current_prompt: str) -> Tuple[str, List[str]]:\n",
        "        \"\"\"Generate fixes for identified issues\"\"\"\n",
        "        fixes_applied = []\n",
        "        new_prompt = current_prompt\n",
        "        \n",
        "        if \"Missing parameter documentation\" in issues:\n",
        "            if \"Args:\" not in new_prompt:\n",
        "                new_prompt += \"\\n- Include detailed parameter descriptions with types\"\n",
        "                fixes_applied.append(\"Added parameter documentation requirement\")\n",
        "        \n",
        "        if \"Missing return value documentation\" in issues:\n",
        "            if \"Returns:\" not in new_prompt:\n",
        "                new_prompt += \"\\n- Document the return value with its type\"\n",
        "                fixes_applied.append(\"Added return value documentation requirement\")\n",
        "        \n",
        "        if \"Missing usage example\" in issues:\n",
        "            if \"example\" not in new_prompt.lower():\n",
        "                new_prompt += \"\\n- Provide a practical usage example with expected output\"\n",
        "                fixes_applied.append(\"Added usage example requirement\")\n",
        "        \n",
        "        if \"Documentation too brief\" in issues:\n",
        "            if \"comprehensive\" not in new_prompt.lower():\n",
        "                new_prompt = new_prompt.replace(\"Generate\", \"Generate comprehensive\")\n",
        "                fixes_applied.append(\"Added comprehensiveness requirement\")\n",
        "        \n",
        "        if \"Missing or vague type information\" in issues:\n",
        "            if \"type\" not in new_prompt.lower():\n",
        "                new_prompt += \"\\n- Include specific Python types for all parameters and return values\"\n",
        "                fixes_applied.append(\"Added specific type requirement\")\n",
        "        \n",
        "        return new_prompt, fixes_applied\n",
        "    \n",
        "    def run_diagnostic_loop(self, max_iterations: int = 3):\n",
        "        \"\"\"Run the full diagnostic and patch loop\"\"\"\n",
        "        \n",
        "        # Initial prompt (deliberately flawed)\n",
        "        current_prompt = \"Generate documentation for this Python function:\\n\\n{code}\\n\\nDocumentation:\"\n",
        "        \n",
        "        console.print(\"üî¨ Starting Prompt Diagnostic Loop\\n\")\n",
        "        \n",
        "        for iteration in range(1, max_iterations + 1):\n",
        "            console.print(f\"üîÑ [bold blue]Iteration {iteration}[/bold blue]\")\n",
        "            console.print(\"‚îÄ\" * 50)\n",
        "            \n",
        "            # Show current prompt\n",
        "            console.print(f\"[yellow]Current Prompt:[/yellow]\")\n",
        "            console.print(Panel(current_prompt, border_style=\"yellow\"))\n",
        "            \n",
        "            # Test with first test case\n",
        "            test_case = self.test_cases[0]\n",
        "            result = self.test_prompt(current_prompt, test_case['input'], iteration)\n",
        "            \n",
        "            console.print(f\"[green]Generated Output:[/green]\")\n",
        "            console.print(Panel(result['output'], border_style=\"green\"))\n",
        "            \n",
        "            # Diagnose issues\n",
        "            issues = self.diagnose_issues(result['output'], test_case['expected_elements'])\n",
        "            \n",
        "            if not issues:\n",
        "                console.print(\"‚úÖ [bold green]No issues found! Prompt is working well.[/bold green]\")\n",
        "                break\n",
        "            \n",
        "            # Show issues\n",
        "            console.print(f\"[red]Issues Identified:[/red]\")\n",
        "            for issue in issues:\n",
        "                console.print(f\"  ‚Ä¢ {issue}\")\n",
        "            \n",
        "            # Record this iteration\n",
        "            self.iterations.append(PromptIteration(\n",
        "                version=iteration,\n",
        "                prompt=current_prompt,\n",
        "                test_input=test_case['input'],\n",
        "                expected_output=\"Complete documentation with all elements\",\n",
        "                actual_output=result['output'],\n",
        "                success=len(issues) == 0,\n",
        "                issues_found=issues,\n",
        "                fixes_applied=[],\n",
        "                timestamp=datetime.now().isoformat()\n",
        "            ))\n",
        "            \n",
        "            if iteration < max_iterations:\n",
        "                # Generate fixes\n",
        "                new_prompt, fixes_applied = self.generate_fixes(issues, current_prompt)\n",
        "                \n",
        "                console.print(f\"[cyan]Fixes Applied:[/cyan]\")\n",
        "                for fix in fixes_applied:\n",
        "                    console.print(f\"  ‚Ä¢ {fix}\")\n",
        "                \n",
        "                # Update iteration record with fixes\n",
        "                self.iterations[-1].fixes_applied = fixes_applied\n",
        "                current_prompt = new_prompt\n",
        "                \n",
        "                console.print(\"\\n\")\n",
        "                time.sleep(1)  # Brief pause for readability\n",
        "        \n",
        "        return current_prompt\n",
        "    \n",
        "    def show_improvement_summary(self):\n",
        "        \"\"\"Show summary of improvements across iterations\"\"\"\n",
        "        \n",
        "        if not self.iterations:\n",
        "            console.print(\"No iterations recorded\")\n",
        "            return\n",
        "        \n",
        "        # Create summary table\n",
        "        table = Table(title=\"Prompt Evolution Summary\")\n",
        "        table.add_column(\"Version\")\n",
        "        table.add_column(\"Issues Found\")\n",
        "        table.add_column(\"Fixes Applied\")\n",
        "        table.add_column(\"Success\")\n",
        "        \n",
        "        for iteration in self.iterations:\n",
        "            issues_str = \", \".join(iteration.issues_found[:2])  # First 2 issues\n",
        "            if len(iteration.issues_found) > 2:\n",
        "                issues_str += f\" (+{len(iteration.issues_found)-2} more)\"\n",
        "            \n",
        "            fixes_str = \", \".join(iteration.fixes_applied[:2])  # First 2 fixes\n",
        "            if len(iteration.fixes_applied) > 2:\n",
        "                fixes_str += f\" (+{len(iteration.fixes_applied)-2} more)\"\n",
        "            \n",
        "            success_icon = \"‚úÖ\" if iteration.success else \"‚ùå\"\n",
        "            \n",
        "            table.add_row(\n",
        "                str(iteration.version),\n",
        "                issues_str,\n",
        "                fixes_str,\n",
        "                success_icon\n",
        "            )\n",
        "        \n",
        "        console.print(table)\n",
        "\n",
        "# Initialize the diagnostic tool\n",
        "diagnostic_tool = PromptDiagnosticTool()\n",
        "print(\"üîß Prompt diagnostic tool ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the Diagnostic Loop\n",
        "\n",
        "Let's run the full diagnosis and patch cycle to see how we can systematically improve a flawed prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the diagnostic loop\n",
        "final_prompt = diagnostic_tool.run_diagnostic_loop(max_iterations=3)\n",
        "\n",
        "# Show the evolution summary\n",
        "console.print(\"\\n\" + \"=\"*60)\n",
        "diagnostic_tool.show_improvement_summary()\n",
        "\n",
        "# Show final optimized prompt\n",
        "console.print(\"\\n\" + \"=\"*60)\n",
        "console.print(\"[bold green]üéØ Final Optimized Prompt:[/bold green]\")\n",
        "console.print(Panel(final_prompt, border_style=\"green\", title=\"Optimized Prompt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Diagnostic Techniques\n",
        "\n",
        "Let's explore additional diagnostic methods for different types of prompt failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedPromptDiagnostics:\n",
        "    \"\"\"Advanced techniques for prompt diagnosis\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.failure_patterns = {\n",
        "            'inconsistent_format': {\n",
        "                'indicators': ['varying structure', 'different layouts', 'inconsistent sections'],\n",
        "                'solutions': ['Add format template', 'Use structured output', 'Provide clear format example']\n",
        "            },\n",
        "            'hallucination': {\n",
        "                'indicators': ['factual errors', 'made up information', 'impossible details'],\n",
        "                'solutions': ['Add \"only use provided info\"', 'Request citations', 'Use retrieval augmentation']\n",
        "            },\n",
        "            'instruction_following': {\n",
        "                'indicators': ['ignores constraints', 'wrong task performed', 'missing requirements'],\n",
        "                'solutions': ['Emphasize requirements', 'Use step-by-step format', 'Add negative examples']\n",
        "            },\n",
        "            'context_length': {\n",
        "                'indicators': ['truncated responses', 'incomplete information', 'abrupt endings'],\n",
        "                'solutions': ['Reduce prompt length', 'Split into subtasks', 'Increase max tokens']\n",
        "            },\n",
        "            'ambiguity': {\n",
        "                'indicators': ['multiple interpretations', 'unclear responses', 'asks for clarification'],\n",
        "                'solutions': ['Add specific examples', 'Define key terms', 'Provide context']\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def analyze_failure_pattern(self, prompt: str, output: str, expected: str) -> Dict:\n",
        "        \"\"\"Analyze what type of failure occurred\"\"\"\n",
        "        \n",
        "        analysis = {\n",
        "            'likely_patterns': [],\n",
        "            'confidence_scores': {},\n",
        "            'recommended_solutions': []\n",
        "        }\n",
        "        \n",
        "        # Simple pattern matching (in production, this would be more sophisticated)\n",
        "        output_lower = output.lower()\n",
        "        prompt_lower = prompt.lower()\n",
        "        \n",
        "        for pattern_name, pattern_info in self.failure_patterns.items():\n",
        "            score = 0\n",
        "            \n",
        "            # Check indicators (this is simplified - real implementation would be more complex)\n",
        "            if pattern_name == 'inconsistent_format':\n",
        "                if len(output.split('\\n')) != len(expected.split('\\n')):\n",
        "                    score += 0.3\n",
        "                if 'format' not in prompt_lower and 'structure' not in prompt_lower:\n",
        "                    score += 0.4\n",
        "            \n",
        "            elif pattern_name == 'instruction_following':\n",
        "                if len(output.split()) < len(expected.split()) * 0.5:\n",
        "                    score += 0.5\n",
        "                if 'must' not in prompt_lower and 'should' not in prompt_lower:\n",
        "                    score += 0.3\n",
        "            \n",
        "            elif pattern_name == 'ambiguity':\n",
        "                if '?' in output:\n",
        "                    score += 0.4\n",
        "                if len(prompt_lower.split()) < 20:  # Very short prompts often ambiguous\n",
        "                    score += 0.3\n",
        "            \n",
        "            analysis['confidence_scores'][pattern_name] = score\n",
        "            \n",
        "            if score > 0.3:\n",
        "                analysis['likely_patterns'].append(pattern_name)\n",
        "                analysis['recommended_solutions'].extend(pattern_info['solutions'])\n",
        "        \n",
        "        return analysis\n",
        "    \n",
        "    def suggest_prompt_improvements(self, analysis: Dict, current_prompt: str) -> List[str]:\n",
        "        \"\"\"Suggest specific prompt improvements based on analysis\"\"\"\n",
        "        \n",
        "        suggestions = []\n",
        "        \n",
        "        # Prioritize suggestions by confidence scores\n",
        "        sorted_patterns = sorted(analysis['confidence_scores'].items(), \n",
        "                               key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        for pattern_name, confidence in sorted_patterns:\n",
        "            if confidence > 0.3:\n",
        "                if pattern_name == 'inconsistent_format':\n",
        "                    suggestions.append(\"Add explicit format template: 'Use this format: [example]'\")\n",
        "                \n",
        "                elif pattern_name == 'instruction_following':\n",
        "                    suggestions.append(\"Strengthen instructions: 'You MUST include...' instead of 'Please include...'\")\n",
        "                \n",
        "                elif pattern_name == 'ambiguity':\n",
        "                    suggestions.append(\"Add specific examples and define key terms clearly\")\n",
        "        \n",
        "        # Remove duplicates while preserving order\n",
        "        seen = set()\n",
        "        unique_suggestions = []\n",
        "        for suggestion in suggestions:\n",
        "            if suggestion not in seen:\n",
        "                seen.add(suggestion)\n",
        "                unique_suggestions.append(suggestion)\n",
        "        \n",
        "        return unique_suggestions[:3]  # Top 3 suggestions\n",
        "\n",
        "# Demo the advanced diagnostics\n",
        "advanced_diagnostics = AdvancedPromptDiagnostics()\n",
        "\n",
        "# Example failure analysis\n",
        "problematic_prompt = \"Write documentation for the code.\"\n",
        "poor_output = \"This is a function.\"\n",
        "expected_output = \"Comprehensive documentation with parameters, returns, examples, and type hints.\"\n",
        "\n",
        "analysis = advanced_diagnostics.analyze_failure_pattern(\n",
        "    problematic_prompt, poor_output, expected_output\n",
        ")\n",
        "\n",
        "suggestions = advanced_diagnostics.suggest_prompt_improvements(analysis, problematic_prompt)\n",
        "\n",
        "console.print(\"\\nüîç [bold blue]Advanced Failure Analysis:[/bold blue]\")\n",
        "console.print(f\"Likely failure patterns: {', '.join(analysis['likely_patterns'])}\")\n",
        "console.print(\"\\nüí° [bold yellow]Improvement Suggestions:[/bold yellow]\")\n",
        "for i, suggestion in enumerate(suggestions, 1):\n",
        "    console.print(f\"{i}. {suggestion}\")\n",
        "\n",
        "print(\"\\nüîß Advanced diagnostics ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways: The Prompt Patch Loop\n",
        "\n",
        "### üîç **Systematic Diagnosis Process**\n",
        "\n",
        "1. **Identify Symptoms**: What exactly is wrong with the output?\n",
        "2. **Categorize Failure Type**: Format, content, instruction-following, etc.\n",
        "3. **Trace Root Cause**: Is it the prompt structure, examples, or constraints?\n",
        "4. **Design Targeted Fix**: Address the specific root cause, not just symptoms\n",
        "5. **Validate Improvement**: Test with multiple examples to ensure fix works\n",
        "\n",
        "### üõ†Ô∏è **Common Failure Patterns & Solutions**\n",
        "\n",
        "| **Failure Pattern** | **Indicators** | **Solutions** |\n",
        "|---------------------|----------------|---------------|\n",
        "| **Inconsistent Format** | Varying structure, different layouts | Add format templates, structured output |\n",
        "| **Missing Information** | Incomplete responses, omitted elements | Explicit requirements list, negative examples |\n",
        "| **Hallucination** | Made-up facts, impossible details | \"Use only provided info\", request citations |\n",
        "| **Instruction Ignored** | Wrong task performed, constraints missed | Stronger language (MUST vs should), step-by-step |\n",
        "| **Ambiguous Output** | Multiple valid interpretations | Specific examples, define key terms |\n",
        "\n",
        "### üìà **Iterative Improvement Strategy**\n",
        "\n",
        "- **Start Simple**: Begin with minimal prompt, add complexity as needed\n",
        "- **One Fix at a Time**: Change one thing per iteration to isolate effects\n",
        "- **Test Edge Cases**: Don't just test the happy path\n",
        "- **Document Changes**: Keep track of what works and what doesn't\n",
        "- **Version Control**: Maintain prompt versions like code\n",
        "\n",
        "### üéØ **Best Practices**\n",
        "\n",
        "1. **Build a Test Suite**: Create diverse test cases covering edge cases\n",
        "2. **Measure Consistently**: Use quantitative metrics where possible\n",
        "3. **Automate Testing**: Run tests after each prompt change\n",
        "4. **Keep a Prompt Library**: Document successful patterns for reuse\n",
        "5. **A/B Test**: Compare prompt versions with statistical significance\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "In Lab 6, you'll build your own prompt library with examples and guardrails, applying these diagnostic techniques to create robust, reliable prompts!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}