{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "e7448dee",
      "metadata": {},
      "source": [
        "\n",
        "# Crossâ€‘Provider Basics\n",
        "\n",
        "This notebook is Colabâ€‘ready and demonstrates:\n",
        "- **OpenAI streaming** output in Python.\n",
        "- Conditional handling for providers that **do not support streaming** (e.g., AskSage)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234aba83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Colab\n",
        "!pip install  requests \n",
        "# If you're on Colab, this will ensure the latest OpenAI SDK is available.\n",
        "!pip -q install --upgrade openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87da9668",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read OPENAI_API_KEY from Colab Secrets (left sidebar â†’ ðŸ”‘ Secrets)\n",
        "# 1) Add a secret named OPENAI_API_KEY\n",
        "# 2) Toggle \"Notebook access\" ON for this notebook\n",
        "# 3) Then run this cell\n",
        "import os\n",
        "try:\n",
        "    from google.colab import userdata  # Colab-only\n",
        "    key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    if key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = key\n",
        "        print(\"âœ“ OPENAI_API_KEY is set from Colab Secrets\")\n",
        "    else:\n",
        "        print(\"âœ— OPENAI_API_KEY not found in Colab Secrets. Add it in the left sidebar (ðŸ”‘).\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ Could not import/use google.colab.userdata. Are you running outside Colab?\")\n",
        "    print(\"Error:\", str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632cd0dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Literal\n",
        "\n",
        "Provider = Literal[\"openai\"]\n",
        "\n",
        "@dataclass\n",
        "class ChatInput:\n",
        "    provider: Provider = \"openai\"\n",
        "    model: str = \"gpt-5-mini\"\n",
        "    system: Optional[str] = None\n",
        "    user: str = \"Say hello and count to 10 slowly.\"\n",
        "    stream: bool = True  # request streaming where supported\n",
        "\n",
        "SUPPORTED_STREAMING: dict[Provider, bool] = {\n",
        "    \"openai\": True,\n",
        "}\n",
        "\n",
        "def provider_supports_streaming(provider: Provider) -> bool:\n",
        "    return SUPPORTED_STREAMING.get(provider, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "475641d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Iterator, Optional\n",
        "from openai import OpenAI\n",
        "from contextlib import contextmanager\n",
        "import sys\n",
        "\n",
        "# Initialize client (ensure OPENAI_API_KEY is set, e.g., via Colab Secrets)\n",
        "client = OpenAI()\n",
        "\n",
        "@dataclass\n",
        "class ChatInput:\n",
        "    model: str = \"gpt-5-mini\"\n",
        "    system: Optional[str] = None\n",
        "    user: str = \"Say hello and count to 10 slowly.\"\n",
        "\n",
        "def print_streamed_text(text_iter: Iterator[str]) -> str:\n",
        "    \"\"\"Print tokens as they stream and return the final concatenated text.\"\"\"\n",
        "    final = []\n",
        "    for chunk in text_iter:\n",
        "        sys.stdout.write(chunk)\n",
        "        sys.stdout.flush()\n",
        "        final.append(chunk)\n",
        "    print()  # newline after stream\n",
        "    return \"\".join(final)\n",
        "\n",
        "def stream_openai(chat: ChatInput) -> str:\n",
        "    \"\"\"\n",
        "    True streaming using the Responses API event stream:\n",
        "      - iterate events\n",
        "      - handle response.output_text.delta\n",
        "      - stop when response.completed\n",
        "      - return the concatenated text (also accessible from get_final_response()).\n",
        "    \"\"\"\n",
        "    # Prepare request payload\n",
        "    request_input = (\n",
        "        chat.user if chat.system is None else\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": chat.system},\n",
        "            {\"role\": \"user\", \"content\": chat.user},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    @contextmanager\n",
        "    def response_stream():\n",
        "        s = client.responses.stream(\n",
        "            model=chat.model,\n",
        "            input=request_input,\n",
        "        )\n",
        "        try:\n",
        "            yield s\n",
        "        finally:\n",
        "            s.close()\n",
        "\n",
        "    deltas: list[str] = []\n",
        "\n",
        "    with response_stream() as s:\n",
        "        for event in s:\n",
        "            if event.type == \"response.output_text.delta\":\n",
        "                deltas.append(event.delta)\n",
        "                # print token as it arrives\n",
        "                sys.stdout.write(event.delta)\n",
        "                sys.stdout.flush()\n",
        "            elif event.type == \"response.completed\":\n",
        "                print()  # newline after final token\n",
        "\n",
        "        # If you want usage/metadata, fetch the final response object:\n",
        "        _final_response = s.get_final_response()\n",
        "        return \"\".join(deltas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5584a478",
      "metadata": {},
      "source": [
        "\n",
        "## Demo â€” OpenAI (streaming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579c7e96",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Demo OpenAI ---\n",
        "chat = ChatInput(\n",
        "    model=\"gpt-5-mini\",\n",
        "    system=\"You are concise and helpful.\",\n",
        "    user=\"Explain in 2 short sentences what streaming is, then emit a tiny haiku.\",\n",
        ")\n",
        "\n",
        "final_text = stream_openai(chat)\n",
        "print(\"\\n---\\nFinal captured text:\\n\", final_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "268ed394",
      "metadata": {},
      "source": [
        "\n",
        "### Notes & References\n",
        "\n",
        "- **OpenAI Responses API** basics and hosted tools examples: see the official Cookbook page.  \n",
        "- **Streaming events** with the Responses API and new features are described in the OpenAI blog post (May 21, 2025).\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}