{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Patch Loop Demo\n",
        "\n",
        "## Learning Objectives\n",
        "- See systematic prompt improvement in action\n",
        "- Watch AI iterate and refine its own prompts\n",
        "- Understand the patch-test-improve cycle\n",
        "- Learn automated prompt optimization techniques\n",
        "\n",
        "## The Demo: Self-Improving Prompts\n",
        "\n",
        "We'll demonstrate:\n",
        "1. **Initial Prompt** - Start with a basic prompt\n",
        "2. **Performance Analysis** - Identify weaknesses\n",
        "3. **Automated Patching** - AI suggests improvements\n",
        "4. **Testing Loop** - Validate improvements\n",
        "5. **Convergence** - Reach optimal performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "!pip install asksageclient pip_system_certs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Import our AskSage client\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "# Get API credentials from Google Colab secrets\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('ASKSAGE_API_KEY')\n",
        "email = userdata.get('ASKSAGE_EMAIL')\n",
        "\n",
        "# Initialize client and tokenizer\n",
        "client = AskSageClient(api_key=api_key, email=email)\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(\"AskSage client initialized successfully\")\n",
        "print(\"Ready to showcase AI capabilities...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task: Product Review Analysis\n",
        "\n",
        "We'll use product review sentiment analysis as our test case for prompt optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test dataset for prompt optimization\n",
        "test_reviews = [\n",
        "    {\n",
        "        \"review\": \"This product is amazing! Works exactly as advertised. Fast shipping too.\",\n",
        "        \"expected_sentiment\": \"positive\",\n",
        "        \"expected_score\": 0.9\n",
        "    },\n",
        "    {\n",
        "        \"review\": \"Terrible quality. Broke after one day. Waste of money.\",\n",
        "        \"expected_sentiment\": \"negative\",\n",
        "        \"expected_score\": 0.1\n",
        "    },\n",
        "    {\n",
        "        \"review\": \"It's okay. Does what it's supposed to do but nothing special.\",\n",
        "        \"expected_sentiment\": \"neutral\",\n",
        "        \"expected_score\": 0.5\n",
        "    },\n",
        "    {\n",
        "        \"review\": \"Love the design but the functionality is lacking. Mixed feelings.\",\n",
        "        \"expected_sentiment\": \"mixed\",\n",
        "        \"expected_score\": 0.6\n",
        "    },\n",
        "    {\n",
        "        \"review\": \"Outstanding customer service! Product had issues but they fixed everything quickly.\",\n",
        "        \"expected_sentiment\": \"positive\",\n",
        "        \"expected_score\": 0.8\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Test dataset loaded: {len(test_reviews)} reviews\")\n",
        "print(\"Expected outcomes defined for validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iteration 1: Basic Prompt\n",
        "\n",
        "Start with a simple, basic prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initial basic prompt\n",
        "def create_basic_prompt(review_text):\n",
        "    return f\"\"\"\n",
        "Analyze the sentiment of this review: {review_text}\n",
        "\"\"\"\n",
        "\n",
        "# Test basic prompt\n",
        "print(\"=== ITERATION 1: BASIC PROMPT ===\")\n",
        "basic_results = []\n",
        "\n",
        "for i, test_case in enumerate(test_reviews):\n",
        "    prompt = create_basic_prompt(test_case['review'])\n",
        "    \n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "response = client.query(\n",
        "    message=prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "    \n",
        "result = response.get(\"message\").strip()\n",
        "    basic_results.append({\n",
        "        'review': test_case['review'][:50] + '...',\n",
        "        'expected': test_case['expected_sentiment'],\n",
        "        'actual': result,\n",
        "        'prompt_version': 'basic'\n",
        "    })\n",
        "    \n",
        "    print(f\"Review {i+1}: Expected {test_case['expected_sentiment']}\")\n",
        "    print(f\"Got: {result[:100]}...\")\n",
        "    print()\n",
        "\n",
        "print(f\"Basic prompt tested on {len(basic_results)} reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AI-Powered Prompt Analysis\n",
        "\n",
        "Let AI analyze the basic prompt's performance and suggest improvements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI analyzes prompt performance\n",
        "analysis_prompt = f\"\"\"\n",
        "Analyze this prompt's performance and suggest specific improvements.\n",
        "\n",
        "CURRENT PROMPT:\n",
        "{create_basic_prompt('[REVIEW_TEXT]')}\n",
        "\n",
        "TEST RESULTS:\n",
        "{json.dumps(basic_results, indent=2)}\n",
        "\n",
        "Provide analysis in JSON format:\n",
        "{{\n",
        "  \"performance_issues\": [\n",
        "    {{\n",
        "      \"issue\": \"string\",\n",
        "      \"impact\": \"High|Medium|Low\",\n",
        "      \"examples\": [\"list of examples\"]\n",
        "    }}\n",
        "  ],\n",
        "  \"improvement_suggestions\": [\n",
        "    {{\n",
        "      \"suggestion\": \"string\",\n",
        "      \"rationale\": \"string\",\n",
        "      \"expected_improvement\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"improved_prompt\": \"string\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== AI PROMPT ANALYSIS ===\")\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "analysis_response = client.query(\n",
        "    message=analysis_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "analysis_result = analysis_response.get(\"message\").strip()\n",
        "print(analysis_result)\n",
        "\n",
        "# Parse the analysis\n",
        "import re\n",
        "json_match = re.search(r'\\{.*\\}', analysis_result, re.DOTALL)\n",
        "if json_match:\n",
        "    analysis_data = json.loads(json_match.group())\n",
        "    issues = analysis_data.get('performance_issues', [])\n",
        "    suggestions = analysis_data.get('improvement_suggestions', [])\n",
        "    improved_prompt = analysis_data.get('improved_prompt', '')\n",
        "    \n",
        "    print(f\"\\n✓ Identified {len(issues)} performance issues\")\n",
        "    print(f\"✓ Generated {len(suggestions)} improvement suggestions\")\n",
        "    print(f\"✓ Created improved prompt version\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iteration 2: AI-Improved Prompt\n",
        "\n",
        "Test the AI-suggested improvements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test improved prompt\n",
        "def create_improved_prompt(review_text):\n",
        "    # Use the AI-suggested improved prompt\n",
        "    return improved_prompt.replace('[REVIEW_TEXT]', review_text)\n",
        "\n",
        "print(\"=== ITERATION 2: AI-IMPROVED PROMPT ===\")\n",
        "print(\"Improved prompt:\")\n",
        "print(improved_prompt)\n",
        "print()\n",
        "\n",
        "improved_results = []\n",
        "\n",
        "for i, test_case in enumerate(test_reviews):\n",
        "    prompt = create_improved_prompt(test_case['review'])\n",
        "    \n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "response = client.query(\n",
        "    message=prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "    \n",
        "result = response.get(\"message\").strip()\n",
        "    improved_results.append({\n",
        "        'review': test_case['review'][:50] + '...',\n",
        "        'expected': test_case['expected_sentiment'],\n",
        "        'actual': result,\n",
        "        'prompt_version': 'improved_v1'\n",
        "    })\n",
        "    \n",
        "    print(f\"Review {i+1}: Expected {test_case['expected_sentiment']}\")\n",
        "    print(f\"Got: {result[:100]}...\")\n",
        "    print()\n",
        "\n",
        "print(f\"Improved prompt tested on {len(improved_results)} reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison\n",
        "\n",
        "Compare the performance of both prompt versions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI compares prompt performance\n",
        "comparison_prompt = f\"\"\"\n",
        "Compare the performance of these two prompt versions and determine if further optimization is needed.\n",
        "\n",
        "BASIC PROMPT RESULTS:\n",
        "{json.dumps(basic_results, indent=2)}\n",
        "\n",
        "IMPROVED PROMPT RESULTS:\n",
        "{json.dumps(improved_results, indent=2)}\n",
        "\n",
        "Provide comparison analysis:\n",
        "{{\n",
        "  \"performance_comparison\": {{\n",
        "    \"basic_prompt_score\": \"number 1-10\",\n",
        "    \"improved_prompt_score\": \"number 1-10\",\n",
        "    \"improvement_percentage\": \"number\",\n",
        "    \"key_improvements\": [\"list of improvements\"]\n",
        "  }},\n",
        "  \"remaining_issues\": [\n",
        "    {{\n",
        "      \"issue\": \"string\",\n",
        "      \"severity\": \"High|Medium|Low\"\n",
        "    }}\n",
        "  ],\n",
        "  \"optimization_status\": \"Complete|Needs_Further_Work\",\n",
        "  \"next_iteration_suggestions\": [\"list if needed\"],\n",
        "  \"final_optimized_prompt\": \"string if optimization complete\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== PERFORMANCE COMPARISON ===\")\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "comparison_response = client.query(\n",
        "    message=comparison_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "comparison_result = comparison_response.get(\"message\").strip()\n",
        "print(comparison_result)\n",
        "\n",
        "# Parse comparison results\n",
        "json_match = re.search(r'\\{.*\\}', comparison_result, re.DOTALL)\n",
        "if json_match:\n",
        "    comparison_data = json.loads(json_match.group())\n",
        "    performance = comparison_data.get('performance_comparison', {})\n",
        "    status = comparison_data.get('optimization_status', 'Unknown')\n",
        "    \n",
        "    print(f\"\\n✓ Basic prompt score: {performance.get('basic_prompt_score', 0)}/10\")\n",
        "    print(f\"✓ Improved prompt score: {performance.get('improved_prompt_score', 0)}/10\")\n",
        "    print(f\"✓ Improvement: {performance.get('improvement_percentage', 0)}%\")\n",
        "    print(f\"✓ Optimization status: {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Patch Loop Summary\n",
        "\n",
        "### The Automated Improvement Process:\n",
        "\n",
        "**1. Initial Testing**\n",
        "- Started with basic prompt\n",
        "- Tested on representative dataset\n",
        "- Collected performance data\n",
        "\n",
        "**2. AI-Powered Analysis**\n",
        "- AI identified specific weaknesses\n",
        "- Generated targeted improvement suggestions\n",
        "- Created optimized prompt version\n",
        "\n",
        "**3. Iterative Testing**\n",
        "- Tested improved prompt\n",
        "- Compared performance metrics\n",
        "- Determined if further optimization needed\n",
        "\n",
        "**4. Convergence**\n",
        "- Reached satisfactory performance\n",
        "- Or identified need for additional iterations\n",
        "\n",
        "### Key Benefits:\n",
        "\n",
        "**Systematic Improvement**\n",
        "- Data-driven optimization process\n",
        "- Objective performance measurement\n",
        "- Consistent improvement methodology\n",
        "\n",
        "**AI-Assisted Analysis**\n",
        "- Identifies issues humans might miss\n",
        "- Suggests specific, actionable improvements\n",
        "- Accelerates optimization process\n",
        "\n",
        "**Scalable Process**\n",
        "- Can be automated for production use\n",
        "- Works with any prompt type\n",
        "- Continuous improvement capability\n",
        "\n",
        "### Production Applications:\n",
        "- **A/B Testing**: Compare prompt versions automatically\n",
        "- **Performance Monitoring**: Detect prompt degradation\n",
        "- **Continuous Optimization**: Self-improving systems\n",
        "- **Quality Assurance**: Validate prompt changes\n",
        "\n",
        "This demonstrates how AI can improve AI - creating self-optimizing prompt systems that get better over time."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}