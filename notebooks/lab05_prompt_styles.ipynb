{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: Prompt Engineering Styles\n",
        "\n",
        "## Learning Objectives\n",
        "- Master different prompt engineering techniques\n",
        "- Compare zero-shot, few-shot, and chain-of-thought approaches\n",
        "- Build a systematic prompt testing framework\n",
        "- Analyze performance trade-offs between different styles\n",
        "\n",
        "## Lab Overview\n",
        "You'll implement and test multiple prompt engineering styles on a classification task, then build a comparison framework to analyze their effectiveness.\n",
        "\n",
        "**Estimated Time:** 60 minutes\n",
        "\n",
        "## Your Mission\n",
        "Build a prompt testing lab that compares different approaches for email classification (spam/not spam)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "!pip install asksageclient pip_system_certs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Import our AskSage client\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "# Get API credentials from Google Colab secrets\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('ASKSAGE_API_KEY')\n",
        "email = userdata.get('ASKSAGE_EMAIL')\n",
        "\n",
        "# Initialize client and tokenizer\n",
        "client = AskSageClient(api_key=api_key, email=email)\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(\"AskSage client initialized successfully\")\n",
        "print(\"Ready to showcase AI capabilities...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, time, csv\n",
        "from typing import Optional, Dict\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "import console\n",
        "\n",
        "ASKSAGE_API_KEY = userdata.get(\"ASKSAGE_API_KEY\")\n",
        "ASKSAGE_BASE_URL = userdata.get(\"ASKSAGE_BASE_URL\")\n",
        "ASKSAGE_EMAIL = userdata.get(\"ASKSAGE_EMAIL\")\n",
        "\n",
        "\n",
        "\n",
        "assert ASKSAGE_API_KEY, \"ASKSAGE_API_KEY not provided.\"\n",
        "assert ASKSAGE_EMAIL, \"ASKSAGE_EMAIL not provided.\"\n",
        "\n",
        "\n",
        "print(\"âœ“ Secrets loaded\")\n",
        "print(\"  â€¢ EMAIL:\", ASKSAGE_EMAIL)\n",
        "print(\"  â€¢ BASE URL:\", ASKSAGE_BASE_URL or \"(default)\")\n",
        "\n",
        "\n",
        "# Pricing (USD per 1,000,000 tokens) â€” exact values requested\n",
        "PRICES_PER_M = {\n",
        "    \"gpt-5\":      {\"input_per_m\": 1.25, \"output_per_m\": 10.00},\n",
        "    \"gpt-5-mini\": {\"input_per_m\": 0.25, \"output_per_m\": 2.00},\n",
        "}\n",
        "\n",
        "\n",
        "# Tokenizer (OpenAI-style)\n",
        "ENCODING_NAME = \"o200k_base\"\n",
        "enc = tiktoken.get_encoding(ENCODING_NAME)\n",
        "\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text or \"\"))\n",
        "\n",
        "\n",
        "def cost_usd(model: str, input_tokens: int, output_tokens: int) -> float:\n",
        "    \"\"\"Exact cost using PRICES_PER_M (input + output).\"\"\"\n",
        "    if model not in PRICES_PER_M:\n",
        "        raise ValueError(f\"Unknown model: {model}\")\n",
        "    r = PRICES_PER_M[model]\n",
        "    return (input_tokens / 1_000_000) * r[\"input_per_m\"] + (output_tokens / 1_000_000) * r[\"output_per_m\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Implement Different Prompt Styles\n",
        "\n",
        "**TODO:** Complete the `PromptStyleTester` class with different prompt engineering approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptStyleTester:\n",
        "    \"\"\"Test different prompt engineering styles for email classification\"\"\"\n",
        "    \n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "        self.results = []\n",
        "        \n",
        "    def zero_shot_prompt(self, email: str) -> str:\n",
        "        \"\"\"Simple zero-shot classification prompt\"\"\"\n",
        "        return f\"\"\"Classify this email as either 'spam' or 'not spam':\n",
        "\n",
        "Email: {email}\n",
        "\n",
        "Classification:\"\"\"\n",
        "    \n",
        "    def few_shot_prompt(self, email: str) -> str:\n",
        "        \"\"\"Few-shot prompt with examples\"\"\"\n",
        "        return f\"\"\"Classify emails as 'spam' or 'not spam' based on these examples:\n",
        "\n",
        "Email: \"Congratulations! You've won $1000! Click here now!\"\n",
        "Classification: spam\n",
        "\n",
        "Email: \"Meeting scheduled for tomorrow at 2pm in conference room A\"\n",
        "Classification: not spam\n",
        "\n",
        "Email: \"URGENT: Your account will be suspended unless you verify now!\"\n",
        "Classification: spam\n",
        "\n",
        "Email: {email}\n",
        "Classification:\"\"\"\n",
        "    \n",
        "    def chain_of_thought_prompt(self, email: str) -> str:\n",
        "        \"\"\"Chain-of-thought reasoning prompt\"\"\"\n",
        "        return f\"\"\"Analyze this email step by step to determine if it's spam:\n",
        "\n",
        "Email: {email}\n",
        "\n",
        "Let me think through this:\n",
        "1. Sender analysis: \n",
        "2. Content analysis: \n",
        "3. Urgency/pressure tactics: \n",
        "4. Suspicious elements: \n",
        "\n",
        "Based on this analysis, classification:\"\"\"\n",
        "    \n",
        "    def system_prompt_style(self, email: str) -> str:\n",
        "        \"\"\"Using system message approach\"\"\"\n",
        "        return f\"\"\"You are an expert email security analyst. Classify this email as 'spam' or 'not spam'.\n",
        "\n",
        "Consider these factors:\n",
        "- Sender credibility\n",
        "- Content quality and grammar\n",
        "- Urgency or pressure tactics\n",
        "- Suspicious links or requests\n",
        "\n",
        "Email: {email}\n",
        "\n",
        "Classification:\"\"\"\n",
        "    \n",
        "    def test_style(self, style_name: str, prompt_func, email: str, expected: str):\n",
        "        \"\"\"Test a single prompt style\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            prompt = prompt_func(email)\n",
        "            input_tokens = count_tokens(prompt)\n",
        "            \n",
        "            # TODO: Add the API call to client.query() here\n",
        "\n",
        "            \n",
        "            # TODO: Extract response text and count output tokens\n",
        "            \n",
        "            # Placeholder for now\n",
        "            response_text = \"not spam\"  # TODO: Replace with actual API response\n",
        "            output_tokens = 10  # TODO: Replace with actual token count\n",
        "            \n",
        "            end_time = time.time()\n",
        "            response_time = end_time - start_time\n",
        "            \n",
        "            # TODO: Calculate cost using cost_usd function\n",
        "            # cost = cost_usd(\"gpt-5-mini\", input_tokens, output_tokens)\n",
        "            cost = 0.001  # TODO: Replace with actual cost calculation\n",
        "            \n",
        "            # Determine if prediction is correct\n",
        "            is_correct = (expected.lower() in response_text) or (response_text in expected.lower())\n",
        "            \n",
        "            result = {\n",
        "                'style': style_name,\n",
        "                'email': email[:50] + '...',\n",
        "                'expected': expected,\n",
        "                'predicted': response_text,\n",
        "                'correct': is_correct,\n",
        "                'response_time': response_time,\n",
        "                'input_tokens': input_tokens,\n",
        "                'output_tokens': output_tokens,\n",
        "                'cost': cost\n",
        "            }\n",
        "            \n",
        "            self.results.append(result)\n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            console.print(f\"[red]Error testing {style_name}: {e}[/red]\")\n",
        "            return None\n",
        "    \n",
        "    def run_comparison(self, test_emails):\n",
        "        \"\"\"Run all prompt styles on test emails\"\"\"\n",
        "        styles = [\n",
        "            ('Zero-shot', self.zero_shot_prompt),\n",
        "            ('Few-shot', self.few_shot_prompt),\n",
        "            ('Chain-of-thought', self.chain_of_thought_prompt),\n",
        "            ('System prompt', self.system_prompt_style)\n",
        "        ]\n",
        "        \n",
        "        console.print(\"\\nðŸ§ª [bold blue]Testing Prompt Styles[/bold blue]\")\n",
        "        \n",
        "        for email_data in test_emails:\n",
        "            email = email_data['email']\n",
        "            expected = email_data['label']\n",
        "            \n",
        "            console.print(f\"\\nðŸ“§ Testing email: {email[:50]}...\")\n",
        "            \n",
        "            for style_name, prompt_func in styles:\n",
        "                result = self.test_style(style_name, prompt_func, email, expected)\n",
        "                if result:\n",
        "                    status = \"âœ…\" if result['correct'] else \"âŒ\"\n",
        "                    console.print(f\"  {status} {style_name}: {result['predicted']} (${result['cost']:.4f})\")\n",
        "        \n",
        "        return self.results\n",
        "\n",
        "# Test data\n",
        "test_emails = [\n",
        "    {'email': 'Congratulations! You have won $10,000! Click here to claim your prize now!', 'label': 'spam'},\n",
        "    {'email': 'Hi John, can we reschedule our meeting to 3pm tomorrow? Thanks, Sarah', 'label': 'not spam'},\n",
        "    {'email': 'URGENT: Your account will be closed in 24 hours. Verify immediately!', 'label': 'spam'},\n",
        "    {'email': 'Quarterly report is attached. Please review before Friday\\'s board meeting.', 'label': 'not spam'}\n",
        "]\n",
        "\n",
        "# Initialize tester\n",
        "tester = PromptStyleTester(client)\n",
        "console.print(\"âœ… PromptStyleTester initialized\")\n",
        "console.print(\"âš ï¸ TODO: Complete the API integration in test_style() method\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Build Performance Analysis Framework\n",
        "\n",
        "**TODO:** Complete the analysis functions to compare prompt performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class StyleMetrics:\n",
        "    style: str\n",
        "    n: int\n",
        "    accuracy: float\n",
        "    avg_response_time: float\n",
        "    avg_tokens: float\n",
        "    total_cost: float\n",
        "    cost_per_correct: Optional[float]\n",
        "    overall_score: float  # user-defined weighted score\n",
        "\n",
        "\n",
        "class PromptAnalyzer:\n",
        "    \"\"\"\n",
        "    Fill-in the blanks where marked. Produces per-style metrics and a simple table.\n",
        "    \"\"\"\n",
        "    def __init__(self, results: List[Dict[str, Any]]):\n",
        "        self.results = results\n",
        "        self.by_style: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
        "        for r in results:\n",
        "            self.by_style[r['style']].append(r)\n",
        "\n",
        "        # ---- BEGIN USER-EDITABLE AREA (weights for overall score) ----\n",
        "        self.weights = {\n",
        "            \"accuracy\": 0.55,\n",
        "            \"speed\":    0.15,  # inverse of response time\n",
        "            \"tokens\":   0.10,  # inverse of token usage\n",
        "            \"cost\":     0.20,  # inverse of cost\n",
        "        }\n",
        "        # ---- END USER-EDITABLE AREA ----\n",
        "\n",
        "    def _normalize_inverse(self, values: List[float], eps: float = 1e-9) -> List[float]:\n",
        "        \"\"\"\n",
        "        Larger-is-better normalization for inverse metrics (lower is better raw).\n",
        "        \"\"\"\n",
        "        if not values: \n",
        "            return []\n",
        "        mx = max(values)\n",
        "        mn = min(values)\n",
        "        # Convert to inverse by flipping: inv = (max - x) range normalized\n",
        "        rng = max(mx - mn, eps)\n",
        "        return [(mx - v) / rng for v in values]\n",
        "\n",
        "    def compute_metrics(self) -> List[StyleMetrics]:\n",
        "        records: List[StyleMetrics] = []\n",
        "        # Collect raw vectors for normalization\n",
        "        styles = list(self.by_style.keys())\n",
        "        accs, times, toks, costs = [], [], [], []\n",
        "\n",
        "        # Pre-pass to compute raw per-style aggregates\n",
        "        aggregates = {}\n",
        "        for s in styles:\n",
        "            rs = self.by_style[s]\n",
        "            n = len(rs)\n",
        "            accuracy = sum(1 for r in rs if r['correct']) / max(1, n)\n",
        "            avg_time = mean(r['response_time'] for r in rs) if n else 0.0\n",
        "            avg_tokens = mean((r['input_tokens'] + r['output_tokens']) for r in rs) if n else 0.0\n",
        "            total_cost = sum(r['cost'] for r in rs)\n",
        "            correct = sum(1 for r in rs if r['correct'])\n",
        "            cost_per_correct = (total_cost / correct) if correct > 0 else None\n",
        "\n",
        "            aggregates[s] = (n, accuracy, avg_time, avg_tokens, total_cost, cost_per_correct)\n",
        "            accs.append(accuracy); times.append(avg_time); toks.append(avg_tokens); costs.append(total_cost)\n",
        "\n",
        "        # Normalize inverse metrics (time, tokens, cost)\n",
        "        norm_speed  = self._normalize_inverse(times)\n",
        "        norm_tokens = self._normalize_inverse(toks)\n",
        "        norm_costs  = self._normalize_inverse(costs)\n",
        "\n",
        "        # Build final records with weighted score\n",
        "        for idx, s in enumerate(styles):\n",
        "            n, accuracy, avg_time, avg_tokens, total_cost, cost_per_correct = aggregates[s]\n",
        "            score = (\n",
        "                self.weights[\"accuracy\"] * accuracy\n",
        "                + self.weights[\"speed\"]  * norm_speed[idx]\n",
        "                + self.weights[\"tokens\"] * norm_tokens[idx]\n",
        "                + self.weights[\"cost\"]   * norm_costs[idx]\n",
        "            )\n",
        "            records.append(StyleMetrics(\n",
        "                style=s,\n",
        "                n=n,\n",
        "                accuracy=accuracy,\n",
        "                avg_response_time=avg_time,\n",
        "                avg_tokens=avg_tokens,\n",
        "                total_cost=total_cost,\n",
        "                cost_per_correct=cost_per_correct,\n",
        "                overall_score=score\n",
        "            ))\n",
        "        return sorted(records, key=lambda m: m.overall_score, reverse=True)\n",
        "\n",
        "    def print_table(self, metrics: List[StyleMetrics]) -> None:\n",
        "        table = Table(title=\"Prompt Style Comparison\")\n",
        "        table.add_column(\"Style\")\n",
        "        table.add_column(\"N\", justify=\"right\")\n",
        "        table.add_column(\"Accuracy\", justify=\"right\")\n",
        "        table.add_column(\"Avg Time (s)\", justify=\"right\")\n",
        "        table.add_column(\"Avg Tokens\", justify=\"right\")\n",
        "        table.add_column(\"Total Cost ($)\", justify=\"right\")\n",
        "        table.add_column(\"Cost/Correct ($)\", justify=\"right\")\n",
        "        table.add_column(\"Score\", justify=\"right\")\n",
        "\n",
        "        for m in metrics:\n",
        "            table.add_row(\n",
        "                m.style,\n",
        "                str(m.n),\n",
        "                f\"{m.accuracy:.2%}\",\n",
        "                f\"{m.avg_response_time:.3f}\",\n",
        "                f\"{m.avg_tokens:.1f}\",\n",
        "                f\"{m.total_cost:.4f}\",\n",
        "                (\"â€”\" if m.cost_per_correct is None else f\"{m.cost_per_correct:.4f}\"),\n",
        "                f\"{m.overall_score:.3f}\",\n",
        "            )\n",
        "        console.print(table)\n",
        "\n",
        "    def recommend(self, metrics: List[StyleMetrics]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Return a small recommendation dict.\n",
        "        \"\"\"\n",
        "        best = metrics[0] if metrics else None\n",
        "        fastest = min(metrics, key=lambda m: m.avg_response_time) if metrics else None\n",
        "        cheapest = min(metrics, key=lambda m: m.total_cost) if metrics else None\n",
        "        most_accurate = max(metrics, key=lambda m: m.accuracy) if metrics else None\n",
        "        return {\n",
        "            \"best_overall\": best.style if best else None,\n",
        "            \"most_accurate\": most_accurate.style if most_accurate else None,\n",
        "            \"fastest\": fastest.style if fastest else None,\n",
        "            \"cheapest_total\": cheapest.style if cheapest else None,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Run the Comparison Experiment\n",
        "\n",
        "**TODO:** Execute your prompt style comparison and analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== Task 3 â€” PromptAnalyzer (drop-in) =====\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional\n",
        "from statistics import mean\n",
        "from collections import defaultdict\n",
        "\n",
        "try:\n",
        "    from rich.console import Console\n",
        "    from rich.table import Table\n",
        "    _console = Console()\n",
        "except Exception:\n",
        "    class _Dummy:\n",
        "        def print(self, *a, **k): print(*a)\n",
        "    _console = _Dummy()\n",
        "    class Table:\n",
        "        def __init__(self, title=\"\"): self.rows=[[\"(install rich for formatted table)\"]]\n",
        "        def add_column(self, *a, **k): pass\n",
        "        def add_row(self, *a, **k): print(\"\\t\".join(map(str, a)))\n",
        "\n",
        "@dataclass\n",
        "class StyleMetrics:\n",
        "    style: str\n",
        "    n: int\n",
        "    accuracy: float\n",
        "    avg_response_time: float\n",
        "    avg_tokens: float\n",
        "    total_cost: float\n",
        "    cost_per_correct: Optional[float]\n",
        "    overall_score: float  # weighted composite\n",
        "\n",
        "class PromptAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyze a list of PromptStyleTester results (dicts) and compute per-style metrics.\n",
        "    Usage:\n",
        "      analyzer = PromptAnalyzer(results)\n",
        "      metrics = analyzer.compute_metrics()  # List[StyleMetrics], sorted best-first\n",
        "      analyzer.print_table(metrics)\n",
        "      rec = analyzer.recommend(metrics)\n",
        "    \"\"\"\n",
        "    def __init__(self, results: List[Dict[str, Any]],\n",
        "                 weights: Optional[Dict[str, float]] = None):\n",
        "        self.results = results\n",
        "        self.by_style: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
        "        for r in results:\n",
        "            self.by_style[r['style']].append(r)\n",
        "\n",
        "        # Weight knobs (tune to taste)\n",
        "        self.weights = weights or {\n",
        "            \"accuracy\": 0.55,  # higher is better\n",
        "            \"speed\":    0.15,  # lower time is better (normalized inverse)\n",
        "            \"tokens\":   0.10,  # lower tokens is better (normalized inverse)\n",
        "            \"cost\":     0.20,  # lower total cost is better (normalized inverse)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_inverse(values: List[float], eps: float = 1e-9) -> List[float]:\n",
        "        \"\"\"\n",
        "        Normalize where lower-is-better into higher-is-better [0..1].\n",
        "        \"\"\"\n",
        "        if not values:\n",
        "            return []\n",
        "        mx, mn = max(values), min(values)\n",
        "        rng = max(mx - mn, eps)\n",
        "        return [(mx - v) / rng for v in values]\n",
        "\n",
        "    def compute_metrics(self) -> List[StyleMetrics]:\n",
        "        styles = list(self.by_style.keys())\n",
        "        aggregates = {}\n",
        "        accs, times, toks, costs = [], [], [], []\n",
        "\n",
        "        # aggregate raw stats\n",
        "        for s in styles:\n",
        "            rs = self.by_style[s]\n",
        "            n = len(rs) or 1\n",
        "            accuracy = sum(1 for r in rs if r.get('correct')) / n\n",
        "            avg_time = mean(r['response_time'] for r in rs) if rs else 0.0\n",
        "            avg_tokens = mean((r['input_tokens'] + r['output_tokens']) for r in rs) if rs else 0.0\n",
        "            total_cost = sum(r['cost'] for r in rs) if rs else 0.0\n",
        "            correct = sum(1 for r in rs if r.get('correct'))\n",
        "            cost_per_correct = (total_cost / correct) if correct else None\n",
        "\n",
        "            aggregates[s] = (n, accuracy, avg_time, avg_tokens, total_cost, cost_per_correct)\n",
        "            accs.append(accuracy); times.append(avg_time); toks.append(avg_tokens); costs.append(total_cost)\n",
        "\n",
        "        # normalize inverse metrics\n",
        "        norm_speed  = self._normalize_inverse(times)\n",
        "        norm_tokens = self._normalize_inverse(toks)\n",
        "        norm_costs  = self._normalize_inverse(costs)\n",
        "\n",
        "        # compute weighted score\n",
        "        metrics: List[StyleMetrics] = []\n",
        "        for i, s in enumerate(styles):\n",
        "            n, accuracy, avg_time, avg_tokens, total_cost, cost_per_correct = aggregates[s]\n",
        "            score = (\n",
        "                self.weights[\"accuracy\"] * accuracy +\n",
        "                self.weights[\"speed\"]    * norm_speed[i] +\n",
        "                self.weights[\"tokens\"]   * norm_tokens[i] +\n",
        "                self.weights[\"cost\"]     * norm_costs[i]\n",
        "            )\n",
        "            metrics.append(StyleMetrics(\n",
        "                style=s,\n",
        "                n=n,\n",
        "                accuracy=accuracy,\n",
        "                avg_response_time=avg_time,\n",
        "                avg_tokens=avg_tokens,\n",
        "                total_cost=total_cost,\n",
        "                cost_per_correct=cost_per_correct,\n",
        "                overall_score=score\n",
        "            ))\n",
        "        return sorted(metrics, key=lambda m: m.overall_score, reverse=True)\n",
        "\n",
        "    def print_table(self, metrics: List[StyleMetrics]) -> None:\n",
        "        tbl = Table(title=\"Prompt Style Comparison\")\n",
        "        tbl.add_column(\"Style\")\n",
        "        tbl.add_column(\"N\", justify=\"right\")\n",
        "        tbl.add_column(\"Accuracy\", justify=\"right\")\n",
        "        tbl.add_column(\"Avg Time (s)\", justify=\"right\")\n",
        "        tbl.add_column(\"Avg Tokens\", justify=\"right\")\n",
        "        tbl.add_column(\"Total Cost ($)\", justify=\"right\")\n",
        "        tbl.add_column(\"Cost/Correct ($)\", justify=\"right\")\n",
        "        tbl.add_column(\"Score\", justify=\"right\")\n",
        "\n",
        "        for m in metrics:\n",
        "            tbl.add_row(\n",
        "                m.style,\n",
        "                str(m.n),\n",
        "                f\"{m.accuracy:.2%}\",\n",
        "                f\"{m.avg_response_time:.3f}\",\n",
        "                f\"{m.avg_tokens:.1f}\",\n",
        "                f\"{m.total_cost:.4f}\",\n",
        "                \"â€”\" if m.cost_per_correct is None else f\"{m.cost_per_correct:.4f}\",\n",
        "                f\"{m.overall_score:.3f}\",\n",
        "            )\n",
        "        _console.print(tbl)\n",
        "\n",
        "    def recommend(self, metrics: List[StyleMetrics]) -> Dict[str, Optional[str]]:\n",
        "        if not metrics:\n",
        "            return {\"best_overall\": None, \"most_accurate\": None, \"fastest\": None, \"cheapest_total\": None}\n",
        "        best_overall = metrics[0]\n",
        "        most_accurate = max(metrics, key=lambda m: m.accuracy)\n",
        "        fastest = min(metrics, key=lambda m: m.avg_response_time)\n",
        "        cheapest = min(metrics, key=lambda m: m.total_cost)\n",
        "        return {\n",
        "            \"best_overall\": best_overall.style,\n",
        "            \"most_accurate\": most_accurate.style,\n",
        "            \"fastest\": fastest.style,\n",
        "            \"cheapest_total\": cheapest.style,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Design Your Own Prompt Style\n",
        "\n",
        "**TODO:** Create and test your own innovative prompt engineering approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "!pip install asksageclient pip_system_certs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Import our AskSage client\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "# Get API credentials from Google Colab secrets\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('ASKSAGE_API_KEY')\n",
        "email = userdata.get('ASKSAGE_EMAIL')\n",
        "\n",
        "# Initialize client and tokenizer\n",
        "client = AskSageClient(api_key=api_key, email=email)\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(\"AskSage client initialized successfully\")\n",
        "print(\"Ready to showcase AI capabilities...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Exit Ticket\n",
        "\n",
        "Before completing this lab, make sure you can answer:\n",
        "\n",
        "### âœ… Deliverables Checklist\n",
        "\n",
        "- [ ] **Implemented 4 prompt styles**: Zero-shot, few-shot, chain-of-thought, and system prompt\n",
        "- [ ] **Built performance analyzer**: Accuracy, response time, and token usage metrics\n",
        "- [ ] **Generated comparison results**: Table and visualizations showing style performance\n",
        "- [ ] **Created custom prompt style**: Your own innovative approach with analysis\n",
        "\n",
        "### ðŸ§  Knowledge Check\n",
        "\n",
        "1. **Which prompt style worked best for accuracy?** Why do you think that is?\n",
        "\n",
        "2. **What trade-offs did you observe** between accuracy, speed, and token usage?\n",
        "\n",
        "3. **When would you use each style** in a production system?\n",
        "\n",
        "4. **What made your custom approach unique?** What problem does it solve?\n",
        "\n",
        "### ðŸš€ Extensions (Optional)\n",
        "\n",
        "- **Multi-model comparison**: Test styles across different models (GPT vs Claude)\n",
        "- **Domain adaptation**: Try styles on different classification tasks\n",
        "- **Prompt optimization**: A/B test variations of your best-performing style\n",
        "- **Cost analysis**: Calculate actual costs for different approaches\n",
        "\n",
        "### ðŸ“Š Success Metrics\n",
        "\n",
        "- Built working prompt comparison framework\n",
        "- Achieved >80% accuracy on at least one style\n",
        "- Identified clear trade-offs between approaches\n",
        "- Designed innovative custom prompt approach\n",
        "\n",
        "**Time Check:** This lab should take about 60 minutes. If you're running over, focus on getting the basic comparison working first, then add custom styles.\n",
        "\n",
        "Ready for Lab 6: Prompt Library? Let's build a reusable collection of battle-tested prompts! ðŸš€"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}