{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced RAG with AskSage and GPT-5-Mini\n",
        "\n",
        "**Focus**: Advanced Retrieval-Augmented Generation using AskSageClient and GPT-5-Mini\n",
        "\n",
        "This notebook demonstrates how to efficiently build a RAG system using AskSageClient with GPT-5-Mini model and NVIDIA's NV-Embed-v2 for embeddings.\n",
        "\n",
        "## Learning Objectives\n",
        "- Use AskSageClient with GPT-5-Mini for LLM interactions\n",
        "- Implement NVIDIA NV-Embed-v2 for high-quality embeddings\n",
        "- Build an efficient RAG pipeline with semantic search\n",
        "- Handle document indexing and retrieval with embeddings\n",
        "- Optimize API usage and manage costs effectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install requests\n",
        "!pip install asksageclient\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentence-transformers\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install faiss-cpu\n",
        "!pip install pandas\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "from asksageclient import AskSageClient\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"✅ All modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load credentials from a JSON file\n",
        "def load_credentials(filename):\n",
        "    try:\n",
        "        with open(filename) as file:\n",
        "            return json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The credentials file was not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Failed to decode JSON from the credentials file.\")\n",
        "\n",
        "# Load the credentials\n",
        "credentials = load_credentials('../../credentials.json')\n",
        "\n",
        "# Extract the API key and email from the credentials\n",
        "api_key = credentials['credentials']['api_key']\n",
        "email = credentials['credentials']['Ask_sage_user_info']['username']\n",
        "\n",
        "# Initialize AskSageClient\n",
        "ask_sage_client = AskSageClient(email, api_key)\n",
        "\n",
        "print(\"✅ AskSage client initialized successfully!\")\n",
        "print(f\"📧 Connected as: {email}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPT-5-Mini model availability\n",
        "print(\"🔍 Checking available models...\")\n",
        "try:\n",
        "    models_response = ask_sage_client.get_models()\n",
        "    available_models = models_response.get('response', [])\n",
        "    \n",
        "    # Look for GPT-5-Mini variants\n",
        "    gpt5_models = [model for model in available_models if 'gpt-o3' in model.lower() or 'gpt-5' in model.lower()]\n",
        "    \n",
        "    if gpt5_models:\n",
        "        selected_model = gpt5_models[0]  # Use the first available GPT-5/O3 model\n",
        "        print(f\"✅ Found GPT-5/O3 model: {selected_model}\")\n",
        "    else:\n",
        "        # Fallback to GPT-4o-mini if GPT-5 not available\n",
        "        selected_model = 'gpt-4o-mini'\n",
        "        print(f\"⚠️ GPT-5-Mini not found, using fallback: {selected_model}\")\n",
        "        \n",
        "    print(f\"📋 Total available models: {len(available_models)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error checking models: {e}\")\n",
        "    selected_model = 'gpt-4o-mini'  # Safe fallback\n",
        "    \n",
        "print(f\"🎯 Selected model: {selected_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize NVIDIA NV-Embed-v2 embedding model\n",
        "print(\"🔄 Loading NVIDIA NV-Embed-v2 model...\")\n",
        "embedding_model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
        "print(\"✅ NVIDIA NV-Embed-v2 model loaded successfully!\")\n",
        "\n",
        "def get_embeddings(texts: List[str]) -> np.ndarray:\n",
        "    \"\"\"Generate embeddings using NVIDIA NV-Embed-v2 model\"\"\"\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    \n",
        "    embeddings = embedding_model.encode(texts, normalize_embeddings=True)\n",
        "    return embeddings\n",
        "\n",
        "# Test the embedding model\n",
        "test_text = \"This is a test sentence for embedding generation with GPT-5-Mini.\"\n",
        "test_embedding = get_embeddings([test_text])\n",
        "print(f\"✅ Test embedding shape: {test_embedding.shape}\")\n",
        "print(f\"✅ Embedding dimension: {test_embedding.shape[1]}\")\n",
        "print(f\"✅ Embedding model working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced RAG System with GPT-5-Mini\n",
        "\n",
        "### Key Components\n",
        "- **Document Indexing**: Process documents and create embeddings\n",
        "- **Vector Storage**: Use FAISS for efficient similarity search\n",
        "- **Retrieval**: Find most relevant chunks based on query embeddings\n",
        "- **Generation**: Use GPT-5-Mini via AskSage for contextual responses\n",
        "\n",
        "### Advantages of GPT-5-Mini\n",
        "- **Superior Performance**: Advanced reasoning capabilities\n",
        "- **Cost Efficiency**: Optimized for production use\n",
        "- **Better Context Understanding**: Enhanced contextual awareness\n",
        "- **Improved Accuracy**: More reliable and consistent outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    \"\"\"Represents a document chunk with metadata\"\"\"\n",
        "    content: str\n",
        "    source: str\n",
        "    chunk_id: int\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "    metadata: Optional[Dict] = None\n",
        "\n",
        "class EnhancedRAGSystem:\n",
        "    \"\"\"Enhanced RAG system using AskSage GPT-5-Mini and NV-Embed-v2\"\"\"\n",
        "    \n",
        "    def __init__(self, ask_sage_client, embedding_model, model_name: str, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
        "        self.client = ask_sage_client\n",
        "        self.embedding_model = embedding_model\n",
        "        self.model_name = model_name\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.document_chunks: List[DocumentChunk] = []\n",
        "        self.vector_index = None\n",
        "        self.embedding_dimension = None\n",
        "        \n",
        "    def chunk_text(self, text: str, source: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        chunk_id = 0\n",
        "        \n",
        "        while start < len(text):\n",
        "            end = start + self.chunk_size\n",
        "            chunk_content = text[start:end]\n",
        "            \n",
        "            # Try to break at sentence boundary\n",
        "            if end < len(text):\n",
        "                last_sentence = chunk_content.rfind('.')\n",
        "                if last_sentence > self.chunk_size * 0.7:  # Don't make chunks too small\n",
        "                    chunk_content = chunk_content[:last_sentence + 1]\n",
        "                    end = start + last_sentence + 1\n",
        "            \n",
        "            chunk = DocumentChunk(\n",
        "                content=chunk_content.strip(),\n",
        "                source=source,\n",
        "                chunk_id=chunk_id,\n",
        "                metadata={'start': start, 'end': end}\n",
        "            )\n",
        "            chunks.append(chunk)\n",
        "            \n",
        "            start = end - self.chunk_overlap\n",
        "            chunk_id += 1\n",
        "            \n",
        "        return chunks\n",
        "    \n",
        "    def add_document(self, text: str, source: str):\n",
        "        \"\"\"Add a document to the RAG system\"\"\"\n",
        "        print(f\"📄 Processing document: {source}\")\n",
        "        \n",
        "        # Create chunks\n",
        "        chunks = self.chunk_text(text, source)\n",
        "        \n",
        "        # Generate embeddings for chunks\n",
        "        chunk_texts = [chunk.content for chunk in chunks]\n",
        "        embeddings = get_embeddings(chunk_texts)\n",
        "        \n",
        "        # Assign embeddings to chunks\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.embedding = embeddings[i]\n",
        "            \n",
        "        self.document_chunks.extend(chunks)\n",
        "        print(f\"✅ Added {len(chunks)} chunks from {source}\")\n",
        "        \n",
        "    def build_vector_index(self):\n",
        "        \"\"\"Build FAISS vector index for efficient similarity search\"\"\"\n",
        "        if not self.document_chunks:\n",
        "            raise ValueError(\"No documents added to the system\")\n",
        "            \n",
        "        print(\"🔨 Building vector index...\")\n",
        "        \n",
        "        # Get all embeddings\n",
        "        embeddings = np.array([chunk.embedding for chunk in self.document_chunks])\n",
        "        self.embedding_dimension = embeddings.shape[1]\n",
        "        \n",
        "        # Create FAISS index\n",
        "        self.vector_index = faiss.IndexFlatIP(self.embedding_dimension)  # Inner product for normalized vectors\n",
        "        self.vector_index.add(embeddings.astype('float32'))\n",
        "        \n",
        "        print(f\"✅ Vector index built with {len(self.document_chunks)} chunks\")\n",
        "        print(f\"📏 Embedding dimension: {self.embedding_dimension}\")\n",
        "        \n",
        "    def retrieve_relevant_chunks(self, query: str, top_k: int = 5) -> List[Tuple[DocumentChunk, float]]:\n",
        "        \"\"\"Retrieve most relevant chunks for a query\"\"\"\n",
        "        if self.vector_index is None:\n",
        "            raise ValueError(\"Vector index not built. Call build_vector_index() first.\")\n",
        "            \n",
        "        # Generate query embedding\n",
        "        query_embedding = get_embeddings([query])[0]\n",
        "        \n",
        "        # Search for similar chunks\n",
        "        scores, indices = self.vector_index.search(\n",
        "            query_embedding.reshape(1, -1).astype('float32'), \n",
        "            top_k\n",
        "        )\n",
        "        \n",
        "        # Return chunks with scores\n",
        "        results = []\n",
        "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "            if idx < len(self.document_chunks):  # Valid index\n",
        "                chunk = self.document_chunks[idx]\n",
        "                results.append((chunk, float(score)))\n",
        "                \n",
        "        return results\n",
        "    \n",
        "    def generate_response(self, query: str, context_chunks: List[DocumentChunk], max_context_length: int = 4000) -> str:\n",
        "        \"\"\"Generate response using GPT-5-Mini with retrieved context\"\"\"\n",
        "        # Prepare context from retrieved chunks\n",
        "        context_parts = []\n",
        "        current_length = 0\n",
        "        \n",
        "        for chunk in context_chunks:\n",
        "            chunk_text = f\"Source: {chunk.source}\\n{chunk.content}\\n\\n\"\n",
        "            if current_length + len(chunk_text) > max_context_length:\n",
        "                break\n",
        "            context_parts.append(chunk_text)\n",
        "            current_length += len(chunk_text)\n",
        "            \n",
        "        context = \"\".join(context_parts)\n",
        "        \n",
        "        # Create prompt for GPT-5-Mini\n",
        "        prompt = f\"\"\"You are an expert assistant with access to relevant documentation. Use the provided context to answer the user's question accurately and comprehensively.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Instructions:\n",
        "1. Base your answer primarily on the provided context\n",
        "2. If the context doesn't contain enough information, clearly state this\n",
        "3. Cite sources when referencing specific information\n",
        "4. Be concise but comprehensive\n",
        "5. If you're unsure about something, express appropriate uncertainty\n",
        "\n",
        "Answer:\"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Use AskSage to query GPT-5-Mini\n",
        "            response = self.client.query(\n",
        "                question=prompt,\n",
        "                model=self.model_name,\n",
        "                temperature=0.3,  # Lower temperature for more consistent responses\n",
        "                max_tokens=1500\n",
        "            )\n",
        "            \n",
        "            # Extract the response text\n",
        "            if isinstance(response, dict) and 'response' in response:\n",
        "                return response['response']\n",
        "            else:\n",
        "                return str(response)\n",
        "                \n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "    \n",
        "    def query(self, question: str, top_k: int = 5) -> Dict:\n",
        "        \"\"\"Complete RAG pipeline: retrieve and generate\"\"\"\n",
        "        print(f\"🔍 Processing query: {question[:100]}{'...' if len(question) > 100 else ''}\")\n",
        "        \n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = self.retrieve_relevant_chunks(question, top_k)\n",
        "        \n",
        "        print(f\"📚 Retrieved {len(relevant_chunks)} relevant chunks\")\n",
        "        for i, (chunk, score) in enumerate(relevant_chunks):\n",
        "            print(f\"  {i+1}. {chunk.source} (score: {score:.3f})\")\n",
        "        \n",
        "        # Generate response\n",
        "        chunks_only = [chunk for chunk, score in relevant_chunks]\n",
        "        response = self.generate_response(question, chunks_only)\n",
        "        \n",
        "        return {\n",
        "            'question': question,\n",
        "            'answer': response,\n",
        "            'retrieved_chunks': relevant_chunks,\n",
        "            'model_used': self.model_name\n",
        "        }\n",
        "    \n",
        "    def get_system_stats(self) -> Dict:\n",
        "        \"\"\"Get system statistics\"\"\"\n",
        "        return {\n",
        "            'total_chunks': len(self.document_chunks),\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'chunk_size': self.chunk_size,\n",
        "            'chunk_overlap': self.chunk_overlap,\n",
        "            'model_name': self.model_name,\n",
        "            'sources': list(set(chunk.source for chunk in self.document_chunks))\n",
        "        }\n",
        "\n",
        "print(\"✅ EnhancedRAGSystem class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo: RAG System with GPT-5-Mini\n",
        "\n",
        "Let's demonstrate the enhanced RAG system using real documents and GPT-5-Mini for generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the RAG system\n",
        "print(\"🚀 Initializing Enhanced RAG System with GPT-5-Mini\")\n",
        "rag_system = EnhancedRAGSystem(\n",
        "    ask_sage_client=ask_sage_client,\n",
        "    embedding_model=embedding_model,\n",
        "    model_name=selected_model,\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "print(f\"✅ RAG system initialized with model: {selected_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents for demonstration\n",
        "sample_documents = {\n",
        "    \"AI_Introduction.txt\": \"\"\"\n",
        "Artificial Intelligence (AI) is a branch of computer science that focuses on creating systems capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and language understanding.\n",
        "\n",
        "Machine Learning is a subset of AI that enables systems to automatically learn and improve from experience without being explicitly programmed. Deep Learning, a subset of Machine Learning, uses neural networks with multiple layers to model and understand complex patterns in data.\n",
        "\n",
        "Natural Language Processing (NLP) is another important area of AI that deals with the interaction between computers and human language. It enables machines to read, understand, and generate human language in a valuable way.\n",
        "\n",
        "Large Language Models (LLMs) like GPT-5-Mini represent the latest advancement in NLP, capable of understanding context, generating coherent text, and performing various language tasks with remarkable accuracy.\n",
        "\"\"\",\n",
        "    \n",
        "    \"RAG_Concepts.txt\": \"\"\"\n",
        "Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of retrieval-based and generation-based approaches for natural language processing tasks. RAG enhances language models by providing them with access to external knowledge through a retrieval mechanism.\n",
        "\n",
        "The RAG process consists of two main components: a retriever and a generator. The retriever searches through a knowledge base or document collection to find relevant information based on the input query. The generator, typically a language model like GPT-5-Mini, then uses both the original query and the retrieved information to generate a comprehensive response.\n",
        "\n",
        "Key advantages of RAG include:\n",
        "1. Access to up-to-date information not present in the model's training data\n",
        "2. Improved factual accuracy by grounding responses in retrieved documents\n",
        "3. Transparency through the ability to trace answers back to source documents\n",
        "4. Reduced hallucination compared to pure generative approaches\n",
        "\n",
        "Vector embeddings play a crucial role in RAG systems by enabling semantic similarity search between queries and documents.\n",
        "\"\"\",\n",
        "    \n",
        "    \"GPT5_Features.txt\": \"\"\"\n",
        "GPT-5-Mini represents a significant advancement in language model technology, offering improved performance while maintaining efficiency. Key features include:\n",
        "\n",
        "Enhanced Reasoning: GPT-5-Mini demonstrates superior logical reasoning capabilities, making it excellent for complex problem-solving tasks and multi-step reasoning.\n",
        "\n",
        "Better Context Understanding: The model shows improved ability to maintain context over longer conversations and documents, enabling more coherent and relevant responses.\n",
        "\n",
        "Reduced Hallucination: Advanced training techniques have significantly reduced the model's tendency to generate factually incorrect information.\n",
        "\n",
        "Efficiency Optimizations: Despite its advanced capabilities, GPT-5-Mini is optimized for faster inference and lower computational costs compared to larger models.\n",
        "\n",
        "Improved Safety: Enhanced safety measures and alignment techniques make GPT-5-Mini more reliable for production use cases.\n",
        "\n",
        "Multimodal Capabilities: The model can process and understand various types of input beyond just text, including structured data and code.\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# Add documents to the RAG system\n",
        "print(\"📚 Adding sample documents to RAG system...\")\n",
        "for source, content in sample_documents.items():\n",
        "    rag_system.add_document(content.strip(), source)\n",
        "\n",
        "print(f\"\\n✅ Added {len(sample_documents)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the vector index\n",
        "print(\"🔨 Building vector index for efficient retrieval...\")\n",
        "rag_system.build_vector_index()\n",
        "\n",
        "# Display system statistics\n",
        "stats = rag_system.get_system_stats()\n",
        "print(\"\\n📊 RAG System Statistics:\")\n",
        "print(f\"  Total chunks: {stats['total_chunks']}\")\n",
        "print(f\"  Embedding dimension: {stats['embedding_dimension']}\")\n",
        "print(f\"  Chunk size: {stats['chunk_size']}\")\n",
        "print(f\"  Model: {stats['model_name']}\")\n",
        "print(f\"  Sources: {', '.join(stats['sources'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test queries with the RAG system\n",
        "test_queries = [\n",
        "    \"What are the key advantages of RAG systems?\",\n",
        "    \"How does GPT-5-Mini improve upon previous language models?\",\n",
        "    \"What is the relationship between AI, Machine Learning, and Deep Learning?\",\n",
        "    \"How do vector embeddings work in RAG systems?\"\n",
        "]\n",
        "\n",
        "print(\"🧪 Testing RAG system with sample queries...\\n\")\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Query {i}: {query}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    try:\n",
        "        result = rag_system.query(query, top_k=3)\n",
        "        \n",
        "        print(f\"\\n🤖 GPT-5-Mini Response:\")\n",
        "        print(result['answer'])\n",
        "        \n",
        "        print(f\"\\n📋 Retrieved Sources:\")\n",
        "        for j, (chunk, score) in enumerate(result['retrieved_chunks'], 1):\n",
        "            print(f\"  {j}. {chunk.source} (similarity: {score:.3f})\")\n",
        "            print(f\"     Preview: {chunk.content[:100]}...\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing query: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive RAG Query Interface\n",
        "\n",
        "Use this cell to ask your own questions to the RAG system powered by GPT-5-Mini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive query interface\n",
        "def ask_rag_system(question: str, detailed: bool = True):\n",
        "    \"\"\"Ask a question to the RAG system\"\"\"\n",
        "    try:\n",
        "        result = rag_system.query(question, top_k=5)\n",
        "        \n",
        "        print(f\"🤖 GPT-5-Mini Answer:\")\n",
        "        print(f\"{result['answer']}\")\n",
        "        \n",
        "        if detailed:\n",
        "            print(f\"\\n📚 Sources Used:\")\n",
        "            for i, (chunk, score) in enumerate(result['retrieved_chunks'], 1):\n",
        "                print(f\"  {i}. {chunk.source} (relevance: {score:.3f})\")\n",
        "                \n",
        "            print(f\"\\n⚙️ Model: {result['model_used']}\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage - modify the question below\n",
        "your_question = \"Explain how GPT-5-Mini's enhanced reasoning capabilities benefit RAG systems.\"\n",
        "\n",
        "print(f\"❓ Question: {your_question}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "result = ask_rag_system(your_question)\n",
        "\n",
        "# Try asking your own questions by modifying the 'your_question' variable above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Analysis and Best Practices\n",
        "\n",
        "### GPT-5-Mini Integration Benefits\n",
        "- **Superior Context Understanding**: Better comprehension of retrieved documents\n",
        "- **Enhanced Reasoning**: More logical and coherent responses\n",
        "- **Reduced Hallucination**: More accurate and fact-based answers\n",
        "- **Efficient Processing**: Optimized performance for production use\n",
        "\n",
        "### RAG System Optimization Tips\n",
        "1. **Chunk Size**: Balance between context and specificity (800-1200 tokens)\n",
        "2. **Overlap**: Ensure continuity between chunks (10-20% overlap)\n",
        "3. **Embedding Quality**: Use high-quality models like NV-Embed-v2\n",
        "4. **Retrieval Strategy**: Experiment with different similarity thresholds\n",
        "5. **Prompt Engineering**: Craft clear instructions for the generation model\n",
        "\n",
        "### Production Considerations\n",
        "- **Caching**: Cache embeddings and frequent queries\n",
        "- **Scaling**: Use vector databases for large document collections\n",
        "- **Monitoring**: Track query performance and answer quality\n",
        "- **Updates**: Regularly refresh the document index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance analysis\n",
        "print(\"📊 RAG System Performance Analysis\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# System statistics\n",
        "stats = rag_system.get_system_stats()\n",
        "print(f\"📈 Total document chunks: {stats['total_chunks']}\")\n",
        "print(f\"🔢 Embedding dimensions: {stats['embedding_dimension']}\")\n",
        "print(f\"📏 Average chunk size: ~{stats['chunk_size']} characters\")\n",
        "print(f\"🤖 Model used: {stats['model_name']}\")\n",
        "\n",
        "# Calculate some metrics\n",
        "total_content_length = sum(len(chunk.content) for chunk in rag_system.document_chunks)\n",
        "avg_chunk_length = total_content_length / len(rag_system.document_chunks) if rag_system.document_chunks else 0\n",
        "\n",
        "print(f\"\\n📊 Content Analysis:\")\n",
        "print(f\"  Total content length: {total_content_length:,} characters\")\n",
        "print(f\"  Average chunk length: {avg_chunk_length:.0f} characters\")\n",
        "print(f\"  Sources processed: {len(stats['sources'])}\")\n",
        "\n",
        "print(f\"\\n✅ RAG system ready for production use with GPT-5-Mini!\")\n",
        "print(f\"💡 Try asking complex questions that require reasoning across multiple sources.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}