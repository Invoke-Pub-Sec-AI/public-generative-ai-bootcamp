{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tool-Calling Foundations - Demo (AskSage Version with gpt-5-mini)\n",
        "\n",
        "**Focus**: tool schemas, read vs write safety, logging tool calls\n",
        "\n",
        "This notebook demonstrates the fundamentals of tool calling in AI applications, covering proper tool design patterns, safety considerations, and observability practices using AskSage API with gpt-5-mini and HuggingFace embeddings.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand tool schema design and validation\n",
        "- Distinguish between read and write operations for safety\n",
        "- Implement proper logging and tracing for tool calls\n",
        "- Build a foundation for safe tool integration with AskSage using gpt-5-mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Google Colab\n",
        "!pip install pydantic requests typing-extensions transformers torch huggingface-hub asksageclient\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import traceback\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field, validator\n",
        "import requests\n",
        "\n",
        "# AskSage client imports\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "# HuggingFace imports for embeddings\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "# Set up logging for tool call tracing\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Function to load credentials from a JSON file\n",
        "def load_credentials(filename):\n",
        "    try:\n",
        "        with open(filename) as file:\n",
        "            return json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The credentials file was not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Failed to decode JSON from the credentials file.\")\n",
        "\n",
        "# Load the credentials\n",
        "credentials = load_credentials('../../credentials.json')\n",
        "\n",
        "# Extract the API key and email from the credentials\n",
        "api_key = credentials['credentials']['api_key']\n",
        "email = credentials['credentials']['Ask_sage_user_info']['username']\n",
        "\n",
        "# Initialize AskSage client\n",
        "ask_sage_client = AskSageClient(email, api_key)\n",
        "\n",
        "# Initialize HuggingFace embedding model (nvidia/NV-Embed-v2)\n",
        "embedding_model_name = \"nvidia/NV-Embed-v2\"\n",
        "try:\n",
        "    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name, trust_remote_code=True)\n",
        "    embedding_model = AutoModel.from_pretrained(embedding_model_name, trust_remote_code=True)\n",
        "    print(\"âœ… HuggingFace nvidia/NV-Embed-v2 model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Could not load nvidia/NV-Embed-v2 model: {e}\")\n",
        "    print(\"Will use mock embeddings for demonstration purposes\")\n",
        "    embedding_tokenizer = None\n",
        "    embedding_model = None\n",
        "\n",
        "print(\"âœ… All packages installed, AskSage client initialized, and logging configured!\")\n",
        "print(\"ðŸš€ This notebook is configured to use gpt-5-mini model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Tool Schema Design\n",
        "\n",
        "Well-defined tool schemas are crucial for reliable AI tool calling. They define the interface, validate inputs, and provide clear documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define tool schemas using Pydantic for validation\n",
        "\n",
        "class QueryEmbeddingTool(BaseModel):\n",
        "    \"\"\"Tool for generating embeddings from text queries using HuggingFace nvidia/NV-Embed-v2 - READ OPERATION\"\"\"\n",
        "    text: str = Field(..., description=\"Text to generate embedding for\", max_length=8000)\n",
        "    model: str = Field(default=\"nvidia/NV-Embed-v2\", description=\"HuggingFace embedding model to use\")\n",
        "    \n",
        "    class Config:\n",
        "        schema_extra = {\n",
        "            \"example\": {\n",
        "                \"text\": \"What is machine learning?\",\n",
        "                \"model\": \"nvidia/NV-Embed-v2\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "class WeatherQueryTool(BaseModel):\n",
        "    \"\"\"Tool for querying weather information - READ OPERATION\"\"\"\n",
        "    location: str = Field(..., description=\"City or location name\", min_length=1)\n",
        "    units: str = Field(default=\"metric\", description=\"Temperature units: metric, imperial, kelvin\")\n",
        "    \n",
        "    @validator('units')\n",
        "    def validate_units(cls, v):\n",
        "        allowed = ['metric', 'imperial', 'kelvin']\n",
        "        if v not in allowed:\n",
        "            raise ValueError(f'Units must be one of {allowed}')\n",
        "        return v\n",
        "\n",
        "class AskSageQueryTool(BaseModel):\n",
        "    \"\"\"Tool for querying AskSage AI with gpt-5-mini - READ OPERATION\"\"\"\n",
        "    message: str = Field(..., description=\"Query message for the AI model\")\n",
        "    model: str = Field(default=\"gpt-5-mini\", description=\"AI model to use\")\n",
        "    system_prompt: str = Field(default=\"You are a helpful AI assistant\", description=\"System prompt\")\n",
        "    \n",
        "    class Config:\n",
        "        schema_extra = {\n",
        "            \"example\": {\n",
        "                \"message\": \"Explain machine learning concepts\",\n",
        "                \"model\": \"gpt-5-mini\",\n",
        "                \"system_prompt\": \"You are a helpful AI assistant\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "class FileWriteTool(BaseModel):\n",
        "    \"\"\"Tool for writing files - WRITE OPERATION (HIGH RISK)\"\"\"\n",
        "    filename: str = Field(..., description=\"Name of file to write\")\n",
        "    content: str = Field(..., description=\"Content to write to file\")\n",
        "    overwrite: bool = Field(default=False, description=\"Whether to overwrite existing file\")\n",
        "    \n",
        "    @validator('filename')\n",
        "    def validate_filename(cls, v):\n",
        "        import os\n",
        "        # Basic security check - no path traversal\n",
        "        if '..' in v or v.startswith('/'):\n",
        "            raise ValueError('Invalid filename: path traversal detected')\n",
        "        return v\n",
        "\n",
        "print(\"Tool schemas defined with validation rules\")\n",
        "print(\"âœ… READ tools: QueryEmbeddingTool (HuggingFace), WeatherQueryTool, AskSageQueryTool (gpt-5-mini)\")\n",
        "print(\"âš ï¸  WRITE tools: FileWriteTool (requires safety measures)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Read vs Write Safety Classification\n",
        "\n",
        "Critical distinction: READ operations are generally safe (query data), while WRITE operations can modify state and require additional safety measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToolSafetyClassifier:\n",
        "    \"\"\"Classify tools by their safety risk level\"\"\"\n",
        "    \n",
        "    READ_OPERATIONS = {\n",
        "        'query_embedding': 'Generate text embeddings using HuggingFace',\n",
        "        'weather_query': 'Query weather data',\n",
        "        'web_search': 'Search web content',\n",
        "        'database_read': 'Read from database',\n",
        "        'api_get': 'GET requests to APIs',\n",
        "        'asksage_query': 'Query AskSage AI model with gpt-5-mini'\n",
        "    }\n",
        "    \n",
        "    WRITE_OPERATIONS = {\n",
        "        'file_write': 'Write or modify files',\n",
        "        'database_write': 'Insert/update database records',\n",
        "        'api_post': 'POST/PUT requests to APIs',\n",
        "        'system_command': 'Execute system commands',\n",
        "        'email_send': 'Send emails or messages'\n",
        "    }\n",
        "    \n",
        "    @classmethod\n",
        "    def classify_tool(cls, tool_name: str, tool_description: str) -> Dict[str, Any]:\n",
        "        \"\"\"Classify a tool's safety level\"\"\"\n",
        "        is_read = any(keyword in tool_description.lower() \n",
        "                     for keyword in ['query', 'read', 'get', 'search', 'fetch'])\n",
        "        is_write = any(keyword in tool_description.lower() \n",
        "                      for keyword in ['write', 'create', 'update', 'delete', 'send', 'execute'])\n",
        "        \n",
        "        if is_write:\n",
        "            risk_level = 'HIGH'\n",
        "            requires_confirmation = True\n",
        "            operation_type = 'WRITE'\n",
        "        elif is_read:\n",
        "            risk_level = 'LOW'\n",
        "            requires_confirmation = False\n",
        "            operation_type = 'READ'\n",
        "        else:\n",
        "            risk_level = 'MEDIUM'\n",
        "            requires_confirmation = True\n",
        "            operation_type = 'UNKNOWN'\n",
        "        \n",
        "        return {\n",
        "            'tool_name': tool_name,\n",
        "            'operation_type': operation_type,\n",
        "            'risk_level': risk_level,\n",
        "            'requires_confirmation': requires_confirmation,\n",
        "            'safety_notes': f\"{'âš ï¸' if is_write else 'âœ…'} {operation_type} operation\"\n",
        "        }\n",
        "\n",
        "# Test the classifier\n",
        "test_tools = [\n",
        "    ('embed_query', 'Generate embeddings for text queries using nvidia/NV-Embed-v2'),\n",
        "    ('write_file', 'Write content to a file on disk'),\n",
        "    ('get_weather', 'Fetch current weather for a location'),\n",
        "    ('asksage_chat', 'Query AskSage AI model with gpt-5-mini for responses')\n",
        "]\n",
        "\n",
        "print(\"Tool Safety Classification Results:\")\n",
        "print(\"-\" * 50)\n",
        "for tool_name, description in test_tools:\n",
        "    result = ToolSafetyClassifier.classify_tool(tool_name, description)\n",
        "    print(f\"Tool: {result['tool_name']}\")\n",
        "    print(f\"  Type: {result['operation_type']} | Risk: {result['risk_level']}\")\n",
        "    print(f\"  Confirmation needed: {result['requires_confirmation']}\")\n",
        "    print(f\"  {result['safety_notes']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tool Call Logging and Tracing\n",
        "\n",
        "Proper logging is essential for debugging, monitoring, and auditing tool usage in production systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToolCallLogger:\n",
        "    \"\"\"Enhanced logging for tool calls with tracing and metrics\"\"\"\n",
        "    \n",
        "    def __init__(self, logger_name: str = \"tool_calls\"):\n",
        "        self.logger = logging.getLogger(logger_name)\n",
        "        self.call_history = []\n",
        "        self.metrics = {\n",
        "            'total_calls': 0,\n",
        "            'successful_calls': 0,\n",
        "            'failed_calls': 0,\n",
        "            'read_operations': 0,\n",
        "            'write_operations': 0\n",
        "        }\n",
        "    \n",
        "    def log_tool_call(self, tool_name: str, parameters: Dict, operation_type: str = \"READ\"):\n",
        "        \"\"\"Log the start of a tool call\"\"\"\n",
        "        call_id = f\"{tool_name}_{int(time.time() * 1000)}\"\n",
        "        \n",
        "        call_record = {\n",
        "            'call_id': call_id,\n",
        "            'tool_name': tool_name,\n",
        "            'operation_type': operation_type,\n",
        "            'parameters': parameters,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'status': 'STARTED',\n",
        "            'duration_ms': None,\n",
        "            'error': None\n",
        "        }\n",
        "        \n",
        "        self.call_history.append(call_record)\n",
        "        self.metrics['total_calls'] += 1\n",
        "        self.metrics[f'{operation_type.lower()}_operations'] += 1\n",
        "        \n",
        "        self.logger.info(f\"ðŸ”§ TOOL_CALL_START | {call_id} | {tool_name} | {operation_type}\")\n",
        "        self.logger.debug(f\"Parameters: {json.dumps(parameters, default=str)}\")\n",
        "        \n",
        "        return call_id\n",
        "    \n",
        "    def log_tool_success(self, call_id: str, result: Any, duration_ms: float):\n",
        "        \"\"\"Log successful tool completion\"\"\"\n",
        "        for record in self.call_history:\n",
        "            if record['call_id'] == call_id:\n",
        "                record['status'] = 'SUCCESS'\n",
        "                record['duration_ms'] = duration_ms\n",
        "                record['result_size'] = len(str(result)) if result else 0\n",
        "                break\n",
        "        \n",
        "        self.metrics['successful_calls'] += 1\n",
        "        self.logger.info(f\"âœ… TOOL_CALL_SUCCESS | {call_id} | {duration_ms:.2f}ms\")\n",
        "    \n",
        "    def log_tool_error(self, call_id: str, error: Exception, duration_ms: float):\n",
        "        \"\"\"Log tool call failure\"\"\"\n",
        "        for record in self.call_history:\n",
        "            if record['call_id'] == call_id:\n",
        "                record['status'] = 'ERROR'\n",
        "                record['duration_ms'] = duration_ms\n",
        "                record['error'] = str(error)\n",
        "                break\n",
        "        \n",
        "        self.metrics['failed_calls'] += 1\n",
        "        self.logger.error(f\"âŒ TOOL_CALL_ERROR | {call_id} | {duration_ms:.2f}ms | {error}\")\n",
        "    \n",
        "    def get_metrics_summary(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get summary of tool call metrics\"\"\"\n",
        "        success_rate = (self.metrics['successful_calls'] / self.metrics['total_calls'] * 100) if self.metrics['total_calls'] > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            **self.metrics,\n",
        "            'success_rate_percent': round(success_rate, 2),\n",
        "            'recent_calls': self.call_history[-5:] if self.call_history else []\n",
        "        }\n",
        "\n",
        "# Initialize the logger\n",
        "tool_logger = ToolCallLogger()\n",
        "print(\"ðŸ“Š Tool call logging system initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace embedding function using nvidia/NV-Embed-v2\n",
        "def get_huggingface_embedding(text: str, model_name: str = \"nvidia/NV-Embed-v2\") -> List[float]:\n",
        "    \"\"\"Generate embeddings using HuggingFace nvidia/NV-Embed-v2 model\"\"\"\n",
        "    global embedding_model, embedding_tokenizer\n",
        "    \n",
        "    if embedding_model is None or embedding_tokenizer is None:\n",
        "        # Fallback to mock embedding if model not available\n",
        "        import random\n",
        "        embedding_dim = 4096  # nvidia/NV-Embed-v2 dimension\n",
        "        return [random.uniform(-1, 1) for _ in range(embedding_dim)]\n",
        "    \n",
        "    try:\n",
        "        # Tokenize the input text\n",
        "        inputs = embedding_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        \n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = embedding_model(**inputs)\n",
        "            \n",
        "        # Extract embeddings (usually from last_hidden_state)\n",
        "        # Mean pooling across sequence length\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "        \n",
        "        # Convert to list\n",
        "        return embeddings.tolist()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error generating embedding: {e}\")\n",
        "        # Fallback to mock embedding\n",
        "        import random\n",
        "        embedding_dim = 4096  # nvidia/NV-Embed-v2 dimension\n",
        "        return [random.uniform(-1, 1) for _ in range(embedding_dim)]\n",
        "\n",
        "# Updated embed_query function with HuggingFace and logging\n",
        "def embed_query(text: str, model: str = \"nvidia/NV-Embed-v2\") -> List[float]:\n",
        "    \"\"\"Embedding function using HuggingFace nvidia/NV-Embed-v2 with logging\"\"\"\n",
        "    \n",
        "    # Validate input using our schema\n",
        "    tool_params = QueryEmbeddingTool(text=text, model=model)\n",
        "    \n",
        "    # Log tool call start\n",
        "    call_id = tool_logger.log_tool_call(\n",
        "        tool_name=\"embed_query\",\n",
        "        parameters=tool_params.dict(),\n",
        "        operation_type=\"READ\"\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Generate embedding using HuggingFace model\n",
        "        embedding = get_huggingface_embedding(text, model)\n",
        "        \n",
        "        # Log success\n",
        "        duration_ms = (time.time() - start_time) * 1000\n",
        "        tool_logger.log_tool_success(call_id, embedding, duration_ms)\n",
        "        \n",
        "        return embedding\n",
        "        \n",
        "    except Exception as e:\n",
        "        duration_ms = (time.time() - start_time) * 1000\n",
        "        tool_logger.log_tool_error(call_id, e, duration_ms)\n",
        "        raise\n",
        "\n",
        "# Test the function\n",
        "print(\"Testing embed_query with HuggingFace nvidia/NV-Embed-v2 and logging:\")\n",
        "test_embedding = embed_query(\"What is artificial intelligence?\")\n",
        "print(f\"Generated embedding with {len(test_embedding)} dimensions\")\n",
        "print(f\"First 5 values: {test_embedding[:5]}\")\n",
        "\n",
        "# Show metrics\n",
        "print(\"\\nCurrent metrics:\")\n",
        "metrics = tool_logger.get_metrics_summary()\n",
        "for key, value in metrics.items():\n",
        "    if key != 'recent_calls':\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: AskSage query function with gpt-5-mini and logging\n",
        "def query_asksage(prompt: str, model: str = \"gpt-5-mini\", system_prompt: Optional[str] = None) -> str:\n",
        "    \"\"\"Query AskSage AI model with gpt-5-mini and logging\"\"\"\n",
        "    \n",
        "    # Validate input using our schema\n",
        "    tool_params = AskSageQueryTool(message=prompt, model=model, system_prompt=system_prompt)\n",
        "    \n",
        "    # Log tool call start\n",
        "    call_id = tool_logger.log_tool_call(\n",
        "        tool_name=\"query_asksage\",\n",
        "        parameters=tool_params.dict(),\n",
        "        operation_type=\"READ\"\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Use AskSage client to get response with gpt-5-mini\n",
        "        query_params = {\n",
        "            'message': prompt,\n",
        "            'model': model\n",
        "        }\n",
        "        \n",
        "        if system_prompt:\n",
        "            query_params['system_prompt'] = system_prompt\n",
        "            \n",
        "        response = ask_sage_client.query(**query_params)\n",
        "        \n",
        "        # Extract the actual response text based on AskSage API response format\n",
        "        if isinstance(response, dict) and 'response' in response:\n",
        "            response_text = response['response']\n",
        "        elif isinstance(response, dict) and 'ret' in response:\n",
        "            response_text = response['ret']\n",
        "        else:\n",
        "            response_text = str(response)\n",
        "        \n",
        "        # Log success\n",
        "        duration_ms = (time.time() - start_time) * 1000\n",
        "        tool_logger.log_tool_success(call_id, response_text, duration_ms)\n",
        "        \n",
        "        return response_text\n",
        "        \n",
        "    except Exception as e:\n",
        "        duration_ms = (time.time() - start_time) * 1000\n",
        "        tool_logger.log_tool_error(call_id, e, duration_ms)\n",
        "        \n",
        "        # Return a fallback response for demo purposes\n",
        "        fallback_response = f\"AskSage query with gpt-5-mini failed: {str(e)}. This is a demo fallback response for prompt: '{prompt}'\"\n",
        "        return fallback_response\n",
        "\n",
        "# Test AskSage query with gpt-5-mini\n",
        "print(\"Testing AskSage query with gpt-5-mini and logging:\")\n",
        "test_response = query_asksage(\n",
        "    prompt=\"Explain the concept of machine learning in simple terms.\",\n",
        "    model=\"gpt-5-mini\",\n",
        "    system_prompt=\"You are a helpful AI assistant that explains complex topics in simple terms.\"\n",
        ")\n",
        "print(f\"AskSage gpt-5-mini response: {test_response[:200]}{'...' if len(test_response) > 200 else ''}\")\n",
        "\n",
        "# Show updated metrics\n",
        "print(\"\\nUpdated metrics after AskSage gpt-5-mini query:\")\n",
        "metrics = tool_logger.get_metrics_summary()\n",
        "for key, value in metrics.items():\n",
        "    if key != 'recent_calls':\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# Test with a more complex query\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Testing gpt-5-mini with a more complex reasoning task:\")\n",
        "complex_response = query_asksage(\n",
        "    prompt=\"Compare the advantages and disadvantages of supervised vs unsupervised learning. Provide specific examples.\",\n",
        "    model=\"gpt-5-mini\",\n",
        "    system_prompt=\"You are an expert in machine learning. Provide detailed, structured responses with clear examples.\"\n",
        ")\n",
        "print(f\"Complex query response: {complex_response[:300]}{'...' if len(complex_response) > 300 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this demo, we covered the foundations of tool calling with AskSage gpt-5-mini and HuggingFace:\n",
        "\n",
        "1. **Tool Schema Design**: Using Pydantic for validation and clear interfaces\n",
        "2. **Safety Classification**: Distinguishing READ (safe) vs WRITE (risky) operations\n",
        "3. **Logging & Tracing**: Comprehensive monitoring for debugging and auditing\n",
        "4. **AskSage gpt-5-mini Integration**: Using AskSageClient with gpt-5-mini for AI model queries\n",
        "5. **HuggingFace Embeddings**: Using nvidia/NV-Embed-v2 for text embeddings\n",
        "\n",
        "These patterns provide a solid foundation for building reliable, observable, and safe AI tool systems.\n",
        "\n",
        "### Key Takeaways\n",
        "- Always validate tool inputs with proper schemas\n",
        "- Classify operations by risk level and apply appropriate safety measures\n",
        "- Implement comprehensive logging for observability and debugging\n",
        "- READ operations are generally safe; WRITE operations require extra caution\n",
        "- AskSage with gpt-5-mini provides an alternative to OpenAI for advanced AI model queries\n",
        "- gpt-5-mini offers improved reasoning capabilities for complex tasks\n",
        "- HuggingFace models like nvidia/NV-Embed-v2 offer powerful embedding capabilities\n",
        "\n",
        "### Setup Requirements\n",
        "- Ensure you have `../../credentials.json` with proper AskSage API credentials\n",
        "- Install required packages: `asksageclient`, `transformers`, `torch`, `pydantic`\n",
        "- The notebook will fallback to mock embeddings if nvidia/NV-Embed-v2 cannot be loaded\n",
        "- gpt-5-mini model must be available in your AskSage account\n",
        "\n",
        "### gpt-5-mini Specific Notes\n",
        "- gpt-5-mini is configured as the default model for all AskSage queries\n",
        "- The model provides enhanced reasoning capabilities compared to earlier versions\n",
        "- All tool functions have been updated to explicitly use gpt-5-mini\n",
        "- System prompts are properly integrated for better model guidance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}