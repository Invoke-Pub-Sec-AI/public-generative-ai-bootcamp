{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Constrained Tool Use - GPT-5-Mini Demo\n",
        "\n",
        "**Focus**: plan constrained tool use; answer repo questions with ‚â§3 calls using GPT-5-Mini via AskSage\n",
        "\n",
        "This notebook demonstrates how to efficiently use a limited set of tools to explore and understand codebases with GPT-5-Mini LLM calls via AskSage. We'll work with strict constraints to maximize the value of each tool call.\n",
        "\n",
        "## Learning Objectives\n",
        "- Plan tool usage strategically with call limits\n",
        "- Use `list_dir`, `search_code`, and `read_text` effectively with GPT-5-Mini\n",
        "- Answer repository questions within constraint budgets\n",
        "- Develop efficient exploration patterns\n",
        "- Handle safe byte caps and minimize API costs\n",
        "- Leverage GPT-5-Mini's advanced reasoning capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install requests\n",
        "!pip install asksageclient\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "print(\"‚úÖ All packages installed and modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load credentials from a JSON file\n",
        "def load_credentials(filename):\n",
        "    try:\n",
        "        with open(filename) as file:\n",
        "            return json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The credentials file was not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Failed to decode JSON from the credentials file.\")\n",
        "\n",
        "# Load the credentials\n",
        "credentials = load_credentials('../../credentials.json')\n",
        "\n",
        "# Extract the API key and email from the credentials\n",
        "api_key = credentials['credentials']['api_key']\n",
        "email = credentials['credentials']['Ask_sage_user_info']['username']\n",
        "\n",
        "# Create an instance of the AskSageClient\n",
        "ask_sage_client = AskSageClient(email, api_key)\n",
        "\n",
        "print(\"‚úÖ AskSage client initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPT-5-Mini is available and set as default model\n",
        "try:\n",
        "    models_response = ask_sage_client.get_models()\n",
        "    if 'response' in models_response:\n",
        "        available_models = models_response['response']\n",
        "        models_df = pd.DataFrame(available_models, columns=['Model Name'])\n",
        "        print(\"üìã Available Models:\")\n",
        "        print(models_df)\n",
        "        \n",
        "        # Check if gpt-5-mini is available\n",
        "        if 'gpt-5-mini' in available_models:\n",
        "            print(\"\\n‚úÖ GPT-5-Mini is available!\")\n",
        "            DEFAULT_MODEL = 'gpt-5-mini'\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è  GPT-5-Mini not found. Available alternatives:\")\n",
        "            gpt_models = [m for m in available_models if 'gpt' in m.lower()]\n",
        "            for model in gpt_models[:5]:\n",
        "                print(f\"   - {model}\")\n",
        "            # Fallback to a similar model\n",
        "            DEFAULT_MODEL = 'gpt-4o-mini' if 'gpt-4o-mini' in available_models else available_models[0]\n",
        "            print(f\"\\nüîÑ Using fallback model: {DEFAULT_MODEL}\")\n",
        "    else:\n",
        "        print(\"‚ùå Unable to fetch models\")\n",
        "        DEFAULT_MODEL = 'gpt-5-mini'  # Assume it exists\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error checking models: {e}\")\n",
        "    DEFAULT_MODEL = 'gpt-5-mini'  # Assume it exists\n",
        "    \n",
        "print(f\"\\nü§ñ Using model: {DEFAULT_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constraint Framework\n",
        "\n",
        "### Available Tools\n",
        "- `list_dir`: Explore directory structure using real file system\n",
        "- `search_code`: Find specific patterns in code using GPT-5-Mini analysis\n",
        "- `read_text`: Read file contents with GPT-5-Mini interpretation\n",
        "\n",
        "### Constraints\n",
        "- **Call Limit**: Maximum 3 tool calls per question\n",
        "- **LLM Budget**: Efficient prompt usage to minimize API costs\n",
        "- **Safe Byte Caps**: Limited file reading to prevent memory issues\n",
        "- **Strategic Planning**: Each call must maximize information gain\n",
        "- **GPT-5-Mini Optimization**: Leverage advanced reasoning for better insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ToolCall:\n",
        "    \"\"\"Track tool usage for constraint compliance\"\"\"\n",
        "    tool_name: str\n",
        "    parameters: dict\n",
        "    result_summary: str\n",
        "    bytes_read: int = 0\n",
        "    model_used: str = DEFAULT_MODEL\n",
        "\n",
        "class GPT5MiniConstrainedExplorer:\n",
        "    \"\"\"GPT-5-Mini powered constrained tool explorer with AskSage\"\"\"\n",
        "    \n",
        "    def __init__(self, ask_sage_client, max_calls: int = 3, max_bytes_per_read: int = 5000, model: str = DEFAULT_MODEL):\n",
        "        self.client = ask_sage_client\n",
        "        self.max_calls = max_calls\n",
        "        self.max_bytes_per_read = max_bytes_per_read\n",
        "        self.model = model\n",
        "        self.calls_made: List[ToolCall] = []\n",
        "        \n",
        "    def can_make_call(self) -> bool:\n",
        "        return len(self.calls_made) < self.max_calls\n",
        "    \n",
        "    def _query_llm(self, prompt: str, system_prompt: str = None) -> str:\n",
        "        \"\"\"Query GPT-5-Mini via AskSage with optimized parameters\"\"\"\n",
        "        try:\n",
        "            response = self.client.query(\n",
        "                user_input=prompt,\n",
        "                model=self.model,\n",
        "                system_prompt=system_prompt,\n",
        "                temperature=0.1,  # Low temperature for consistent, focused responses\n",
        "                max_tokens=2000   # Reasonable limit for cost control\n",
        "            )\n",
        "            \n",
        "            if isinstance(response, dict) and 'response' in response:\n",
        "                return response['response']\n",
        "            elif isinstance(response, dict) and 'ret' in response:\n",
        "                return response['ret']\n",
        "            else:\n",
        "                return str(response)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"LLM query failed: {e}\")\n",
        "            return f\"Error: {str(e)}\"\n",
        "    \n",
        "    def list_dir(self, path: str) -> List[str]:\n",
        "        \"\"\"List directory contents using real file system\"\"\"\n",
        "        if not self.can_make_call():\n",
        "            raise Exception(f\"Call limit exceeded: {self.max_calls}\")\n",
        "            \n",
        "        try:\n",
        "            result = os.listdir(path)\n",
        "        except FileNotFoundError:\n",
        "            result = []\n",
        "        \n",
        "        call = ToolCall(\n",
        "            tool_name=\"list_dir\",\n",
        "            parameters={\"path\": path},\n",
        "            result_summary=f\"Found {len(result)} items: {', '.join(result[:3])}{'...' if len(result) > 3 else ''}\",\n",
        "            model_used=self.model\n",
        "        )\n",
        "        self.calls_made.append(call)\n",
        "        \n",
        "        print(f\"üîç Call {len(self.calls_made)}/{self.max_calls}: list_dir('{path}') using {self.model}\")\n",
        "        print(f\"   Result: {result}\")\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def search_code(self, pattern: str, path: str = \"/content/sample_data\") -> List[Tuple[str, int]]:\n",
        "        \"\"\"Search for code patterns using GPT-5-Mini analysis via AskSage\"\"\"\n",
        "        if not self.can_make_call():\n",
        "            raise Exception(f\"Call limit exceeded: {self.max_calls}\")\n",
        "        \n",
        "        # Get file list to search\n",
        "        files_to_search = []\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for file in files:\n",
        "                if file.endswith(('.py', '.txt', '.md', '.json', '.yaml', '.yml', '.csv')):\n",
        "                    files_to_search.append(os.path.join(root, file))\n",
        "        \n",
        "        # Use GPT-5-Mini to analyze which files likely contain the pattern\n",
        "        system_prompt = \"You are a code analysis expert. Analyze file paths and predict which files likely contain specific patterns.\"\n",
        "        \n",
        "        prompt = f\"\"\"Given this search pattern: '{pattern}'\n",
        "Files available: {files_to_search[:15]}\n",
        "\n",
        "Analyze which files are most likely to contain code or content related to '{pattern}'.\n",
        "Consider file names, extensions, and typical software project structures.\n",
        "Return only the TOP 3 most relevant file paths, one per line, with no explanations.\"\"\"\n",
        "        \n",
        "        llm_response = self._query_llm(prompt, system_prompt)\n",
        "        relevant_files = [line.strip() for line in llm_response.split('\\n') if line.strip() and ('/' in line.strip() or '\\\\' in line.strip())]\n",
        "        \n",
        "        if not relevant_files:\n",
        "            relevant_files = files_to_search[:3]  # Fallback to first 3 files\n",
        "        \n",
        "        # Simple grep-like search in the identified files\n",
        "        results = []\n",
        "        for file_path in relevant_files[:5]:\n",
        "            try:\n",
        "                if os.path.exists(file_path):\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        lines = f.readlines()\n",
        "                        for i, line in enumerate(lines, 1):\n",
        "                            if pattern.lower() in line.lower():\n",
        "                                results.append((file_path, i))\n",
        "            except Exception:\n",
        "                continue\n",
        "        \n",
        "        call = ToolCall(\n",
        "            tool_name=\"search_code\",\n",
        "            parameters={\"pattern\": pattern, \"path\": path},\n",
        "            result_summary=f\"GPT-5-Mini found {len(results)} matches for '{pattern}'\",\n",
        "            model_used=self.model\n",
        "        )\n",
        "        self.calls_made.append(call)\n",
        "        \n",
        "        print(f\"üîç Call {len(self.calls_made)}/{self.max_calls}: search_code('{pattern}', '{path}') using {self.model}\")\n",
        "        print(f\"   Result: {results}\")\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    def read_text(self, file_path: str, max_bytes: Optional[int] = None) -> str:\n",
        "        \"\"\"Read file contents with GPT-5-Mini interpretation via AskSage\"\"\"\n",
        "        if not self.can_make_call():\n",
        "            raise Exception(f\"Call limit exceeded: {self.max_calls}\")\n",
        "            \n",
        "        max_bytes = max_bytes or self.max_bytes_per_read\n",
        "        \n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read(max_bytes)\n",
        "        except Exception as e:\n",
        "            content = f\"Error reading file: {str(e)}\"\n",
        "        \n",
        "        bytes_read = len(content)\n",
        "        \n",
        "        # Use GPT-5-Mini to analyze and summarize the content\n",
        "        if len(content) > 100:\n",
        "            system_prompt = \"You are a technical document analyzer. Provide concise, accurate summaries focusing on key functionality and purpose.\"\n",
        "            \n",
        "            summary_prompt = f\"\"\"Analyze this file content and provide a comprehensive 2-3 sentence summary:\n",
        "\n",
        "File: {file_path}\n",
        "Content:\n",
        "{content[:2000]}{'...(truncated)' if len(content) > 2000 else ''}\n",
        "\n",
        "Focus on:\n",
        "1. Main purpose and functionality\n",
        "2. Key components or data structures\n",
        "3. Notable patterns or technologies used\n",
        "\n",
        "Provide only the summary, no preamble.\"\"\"\n",
        "            \n",
        "            summary = self._query_llm(summary_prompt, system_prompt)\n",
        "        else:\n",
        "            summary = content\n",
        "        \n",
        "        call = ToolCall(\n",
        "            tool_name=\"read_text\",\n",
        "            parameters={\"file_path\": file_path, \"max_bytes\": max_bytes},\n",
        "            result_summary=f\"GPT-5-Mini analyzed {bytes_read} bytes from {file_path}. Summary: {summary[:100]}...\",\n",
        "            bytes_read=bytes_read,\n",
        "            model_used=self.model\n",
        "        )\n",
        "        self.calls_made.append(call)\n",
        "        \n",
        "        print(f\"üîç Call {len(self.calls_made)}/{self.max_calls}: read_text('{file_path}', max_bytes={max_bytes}) using {self.model}\")\n",
        "        print(f\"   Result: {bytes_read} bytes read\")\n",
        "        print(f\"   GPT-5-Mini Summary: {summary}\")\n",
        "        \n",
        "        return content\n",
        "    \n",
        "    def get_call_summary(self) -> str:\n",
        "        \"\"\"Generate summary of all calls made\"\"\"\n",
        "        summary = f\"\\n=== GPT-5-MINI CALL SUMMARY ({len(self.calls_made)}/{self.max_calls}) ===\\n\"\n",
        "        for i, call in enumerate(self.calls_made, 1):\n",
        "            summary += f\"{i}. {call.tool_name} ({call.model_used}): {call.result_summary}\\n\"\n",
        "        return summary\n",
        "\n",
        "print(\"‚úÖ GPT5MiniConstrainedExplorer class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo: Strategic Repository Exploration with GPT-5-Mini\n",
        "\n",
        "Let's demonstrate how to answer repository questions efficiently using our 3-call limit with GPT-5-Mini's advanced analysis capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo Question: \"What files are in the sample data?\"\n",
        "print(\"üéØ QUESTION: What files are in the sample data?\")\n",
        "print(\"üìã STRATEGY: 1) List sample_data directory, 2) Search for interesting patterns, 3) Read a key file\")\n",
        "print(f\"ü§ñ USING MODEL: {DEFAULT_MODEL}\")\n",
        "print()\n",
        "\n",
        "explorer = GPT5MiniConstrainedExplorer(ask_sage_client, max_calls=3, model=DEFAULT_MODEL)\n",
        "\n",
        "# Call 1: Strategic directory exploration\n",
        "print(\"üìç CALL 1 STRATEGY: Explore sample_data directory structure\")\n",
        "sample_contents = explorer.list_dir(\"/content/sample_data\")\n",
        "print(f\"‚úÖ Found {len(sample_contents)} files/directories\")\n",
        "print()\n",
        "\n",
        "# Call 2: GPT-5-Mini guided search for data patterns\n",
        "print(\"üìç CALL 2 STRATEGY: Use GPT-5-Mini to search for data or CSV files\")\n",
        "data_matches = explorer.search_code(\"data\")\n",
        "print(f\"‚úÖ GPT-5-Mini found {len(data_matches)} data-related references\")\n",
        "print()\n",
        "\n",
        "# Call 3: GPT-5-Mini analysis of the most interesting file\n",
        "print(\"üìç CALL 3 STRATEGY: GPT-5-Mini analysis of a sample data file\")\n",
        "if sample_contents:\n",
        "    target_file = f\"/content/sample_data/{sample_contents[0]}\"\n",
        "    file_content = explorer.read_text(target_file, max_bytes=1000)\n",
        "    print(f\"‚úÖ GPT-5-Mini successfully analyzed {target_file}\")\n",
        "\n",
        "print(explorer.get_call_summary())\n",
        "print(\"\\nüéâ EXPLORATION COMPLETE: Used GPT-5-Mini to efficiently analyze sample data within constraints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo: GPT-5-Mini Guided File Analysis\n",
        "\n",
        "Now let's show how to find specific content using GPT-5-Mini's advanced reasoning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo Question: \"Find CSV files and analyze their structure using GPT-5-Mini\"\n",
        "print(\"üéØ QUESTION: Find CSV files and analyze their structure using GPT-5-Mini\")\n",
        "print(\"üìã STRATEGY: 1) GPT-5-Mini search for CSV patterns, 2) List directory for .csv files, 3) GPT-5-Mini analysis of CSV structure\")\n",
        "print(f\"ü§ñ USING MODEL: {DEFAULT_MODEL}\")\n",
        "print()\n",
        "\n",
        "explorer = GPT5MiniConstrainedExplorer(ask_sage_client, max_calls=3, model=DEFAULT_MODEL)\n",
        "\n",
        "# Call 1: GPT-5-Mini guided search for CSV patterns\n",
        "print(\"üìç CALL 1 STRATEGY: Use GPT-5-Mini to search codebase for CSV-related content\")\n",
        "csv_matches = explorer.search_code(\"csv\")\n",
        "print(f\"‚úÖ GPT-5-Mini found CSV references in {len(set(match[0] for match in csv_matches))} files\")\n",
        "print()\n",
        "\n",
        "# Call 2: Explore sample data directory\n",
        "print(\"üìç CALL 2 STRATEGY: List files in sample_data directory\")\n",
        "data_files = explorer.list_dir(\"/content/sample_data\")\n",
        "csv_files = [f for f in data_files if f.endswith('.csv')]\n",
        "print(f\"‚úÖ Found {len(csv_files)} CSV files: {csv_files}\")\n",
        "print()\n",
        "\n",
        "# Call 3: GPT-5-Mini analysis of CSV file structure\n",
        "print(\"üìç CALL 3 STRATEGY: GPT-5-Mini deep analysis of CSV file structure\")\n",
        "if csv_files:\n",
        "    csv_path = f\"/content/sample_data/{csv_files[0]}\"\n",
        "    csv_content = explorer.read_text(csv_path, max_bytes=500)\n",
        "    print(f\"‚úÖ GPT-5-Mini successfully analyzed {csv_files[0]} structure\")\n",
        "else:\n",
        "    # Fallback to any available file\n",
        "    if data_files:\n",
        "        fallback_file = f\"/content/sample_data/{data_files[0]}\"\n",
        "        content = explorer.read_text(fallback_file, max_bytes=500)\n",
        "        print(f\"‚úÖ GPT-5-Mini analyzed available file: {data_files[0]}\")\n",
        "\n",
        "print(explorer.get_call_summary())\n",
        "print(\"\\nüéâ ANALYSIS COMPLETE: GPT-5-Mini guided us to efficiently find and analyze data files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced GPT-5-Mini Strategic Planning Framework\n",
        "\n",
        "### Key Advantages of GPT-5-Mini for Constrained Tool Use\n",
        "\n",
        "1. **Enhanced Reasoning**: GPT-5-Mini's advanced reasoning helps predict most relevant files/locations\n",
        "2. **Pattern Recognition**: Superior pattern matching for complex code analysis\n",
        "3. **Context Understanding**: Better comprehension of file relationships and project structure\n",
        "4. **Efficient Summarization**: More accurate and concise file content analysis\n",
        "5. **Cost Optimization**: Balanced performance and API efficiency\n",
        "6. **Multi-step Planning**: Enhanced ability to plan complex exploration strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gpt5_mini_plan_exploration_strategy(question: str, ask_sage_client, max_calls: int = 3, model: str = DEFAULT_MODEL) -> List[str]:\n",
        "    \"\"\"Generate GPT-5-Mini guided strategic plan for repository exploration\"\"\"\n",
        "    \n",
        "    system_prompt = \"You are an expert software architect and code analyst. Create optimal exploration strategies for repository analysis within strict constraints.\"\n",
        "    \n",
        "    planning_prompt = f\"\"\"Given this repository analysis question: \"{question}\"\n",
        "\n",
        "Available tools:\n",
        "- list_dir(path): Lists files and directories\n",
        "- search_code(pattern, path): Searches for patterns in code files using AI analysis\n",
        "- read_text(file_path, max_bytes): Reads and analyzes file contents with AI interpretation\n",
        "\n",
        "CONSTRAINTS: You have exactly {max_calls} tool calls to answer this question efficiently.\n",
        "\n",
        "Create an optimal 3-step exploration strategy. Each step should specify:\n",
        "1. Tool to use\n",
        "2. Specific parameters\n",
        "3. Expected outcome and information gain\n",
        "4. How it builds toward answering the question\n",
        "\n",
        "Prioritize steps that maximize information gain and build upon each other.\n",
        "\n",
        "Format as: \"Step N: tool_name(specific_parameters) - expected outcome and strategic value\"\n",
        "Return only the 3 steps, one per line.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = ask_sage_client.query(\n",
        "            user_input=planning_prompt,\n",
        "            model=model,\n",
        "            system_prompt=system_prompt,\n",
        "            temperature=0.1,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "        \n",
        "        # Extract response text\n",
        "        if isinstance(response, dict) and 'response' in response:\n",
        "            response_text = response['response']\n",
        "        elif isinstance(response, dict) and 'ret' in response:\n",
        "            response_text = response['ret']\n",
        "        else:\n",
        "            response_text = str(response)\n",
        "            \n",
        "        strategy = [line.strip() for line in response_text.split('\\n') if line.strip() and 'Step' in line]\n",
        "        return strategy[:3]  # Ensure exactly 3 steps\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"GPT-5-Mini planning failed: {e}\")\n",
        "        # Fallback strategy\n",
        "        return [\n",
        "            \"Step 1: list_dir(/content/sample_data) - Understand directory structure and file types\",\n",
        "            \"Step 2: search_code(relevant_terms) - Find relevant code patterns using AI analysis\", \n",
        "            \"Step 3: read_text(target_file) - Examine specific implementation with AI interpretation\"\n",
        "        ]\n",
        "\n",
        "# Test the GPT-5-Mini planning function\n",
        "questions = [\n",
        "    \"What data formats and file types are available in the sample data?\",\n",
        "    \"Find any machine learning, AI, or data science related files and analyze their purpose\", \n",
        "    \"What is the structure and content of the largest or most complex file in the repository?\",\n",
        "    \"Identify configuration files and understand the project's technology stack\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\n‚ùì Question: {q}\")\n",
        "    strategy = gpt5_mini_plan_exploration_strategy(q, ask_sage_client, model=DEFAULT_MODEL)\n",
        "    print(f\"üìã GPT-5-Mini Generated Strategy (using {DEFAULT_MODEL}):\")\n",
        "    for step in strategy:\n",
        "        print(f\"   {step}\")\n",
        "\n",
        "print(\"\\n‚úÖ GPT-5-Mini guided strategic planning framework complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token Usage and Cost Monitoring\n",
        "\n",
        "Monitor GPT-5-Mini usage to optimize costs while maintaining quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def monitor_gpt5_mini_usage(ask_sage_client):\n",
        "    \"\"\"Monitor GPT-5-Mini token usage and provide cost insights\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Get monthly token count\n",
        "        token_response = ask_sage_client.count_monthly_tokens()\n",
        "        \n",
        "        if isinstance(token_response, dict) and 'response' in token_response:\n",
        "            token_data = token_response['response']\n",
        "            \n",
        "            print(\"üîç GPT-5-Mini Usage Report:\")\n",
        "            print(\"=\"*50)\n",
        "            \n",
        "            if isinstance(token_data, dict):\n",
        "                for key, value in token_data.items():\n",
        "                    print(f\"{key}: {value:,} tokens\")\n",
        "            else:\n",
        "                print(f\"Total tokens used: {token_data:,}\")\n",
        "                \n",
        "            print(\"=\"*50)\n",
        "            \n",
        "            # Efficiency tips for GPT-5-Mini\n",
        "            print(\"\\nüí° GPT-5-Mini Optimization Tips:\")\n",
        "            print(\"‚Ä¢ Use temperature=0.1 for consistent, focused responses\")\n",
        "            print(\"‚Ä¢ Set appropriate max_tokens limits (1000-2000 for analysis)\")\n",
        "            print(\"‚Ä¢ Leverage system prompts for better context efficiency\")\n",
        "            print(\"‚Ä¢ Batch similar queries when possible\")\n",
        "            print(\"‚Ä¢ Use specific, targeted prompts for better cost per insight\")\n",
        "            \n",
        "        else:\n",
        "            print(\"Unable to retrieve token usage data\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error monitoring usage: {e}\")\n",
        "\n",
        "# Run usage monitoring\n",
        "monitor_gpt5_mini_usage(ask_sage_client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices Summary for GPT-5-Mini\n",
        "\n",
        "### Maximizing GPT-5-Mini Information Gain\n",
        "- **AI-Guided Discovery**: Leverage GPT-5-Mini's reasoning to predict most relevant files before exploring\n",
        "- **Intelligent Filtering**: Use advanced context understanding to prioritize which files to examine\n",
        "- **Smart Summarization**: Leverage superior language understanding for extracting key insights from large files\n",
        "- **Multi-step Reasoning**: Utilize enhanced planning capabilities for complex exploration tasks\n",
        "\n",
        "### Managing GPT-5-Mini and System Constraints\n",
        "- **Call Budgeting**: Plan all 3 calls strategically, considering GPT-5-Mini's enhanced capabilities\n",
        "- **Prompt Efficiency**: Use targeted, specific prompts optimized for GPT-5-Mini's reasoning abilities\n",
        "- **Temperature Control**: Use low temperatures (0.1) for consistent, focused analytical outputs\n",
        "- **Token Management**: Balance quality insights with cost efficiency\n",
        "\n",
        "### Real-World Implementation\n",
        "- **AskSage Integration**: Seamless integration with enterprise-grade AI infrastructure\n",
        "- **File Type Awareness**: Enhanced understanding of various file formats (.py, .csv, .txt, .json, .md)\n",
        "- **Error Handling**: Robust operations with intelligent fallback strategies\n",
        "- **Scalability**: Optimized for both small projects and large codebases\n",
        "\n",
        "This framework enables efficient repository exploration within tight constraints while leveraging GPT-5-Mini's advanced reasoning capabilities to maximize the value and accuracy of each tool call."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}