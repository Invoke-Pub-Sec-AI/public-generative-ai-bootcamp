{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "4c732dc06511448d8a9ffaa6692bf162",
      "metadata": {},
      "source": [
        "# Lab 04: Running, Monitoring, and Evaluating Fine-Tuning Jobs\n",
        "\n",
        "## Learning Objectives\n",
        "- Structure a fine-tuning job into a reusable pipeline\n",
        "- Monitor and log training metrics like loss and accuracy\n",
        "- Implement advanced evaluation metrics for text generation (e.g., BLEU score)\n",
        "- Save and load model checkpoints\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44455bbe0f8b44beb24e310d8a73f0ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple, Any\n",
        "from collections import Counter\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa1fde0f6c19417a8d1a5fdd5ff5bf41",
      "metadata": {},
      "source": [
        "## Part 1: Structuring the Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f897657eb484ee58be5dccc9f30ccc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll reuse and build upon the mock model from previous labs\n",
        "class MockModel:\n",
        "    def __init__(self, vocab_size=256, dim=32):\n",
        "        self.weights = np.random.randn(dim, vocab_size) * 0.1\n",
        "\n",
        "    def save_checkpoint(self, path: str):\n",
        "        np.save(path, self.weights)\n",
        "\n",
        "    def load_checkpoint(self, path: str):\n",
        "        self.weights = np.load(path)\n",
        "\n",
        "class TrainingPipeline:\n",
        "    def __init__(self, model, dataset, lr=0.01):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.lr = lr\n",
        "        self.history = {'loss': [], 'accuracy': []} # For monitoring\n",
        "\n",
        "    def run(self, epochs: int):\n",
        "        print(f'Starting training for {epochs} epochs...')\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = self._train_one_epoch()\n",
        "            avg_loss = total_loss / len(self.dataset)\n",
        "            \n",
        "            # Mock evaluation for accuracy\n",
        "            accuracy = self._evaluate()\n",
        "            \n",
        "            # Log metrics\n",
        "            self.history['loss'].append(avg_loss)\n",
        "            self.history['accuracy'].append(accuracy)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2%}')\n",
        "            \n",
        "            # Save a checkpoint periodically\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                self.model.save_checkpoint(f'model_epoch_{epoch+1}.npy')\n",
        "                print(f'Saved checkpoint at epoch {epoch+1}')\n",
        "\n",
        "    def _train_one_epoch(self) -> float:\n",
        "        # Simplified training logic from previous labs\n",
        "        total_loss = 0\n",
        "        for _ in self.dataset:\n",
        "            # Mock forward/backward pass\n",
        "            loss_grad = np.random.randn(*self.model.weights.shape) * 0.01\n",
        "            self.model.weights -= self.lr * loss_grad\n",
        "            total_loss += np.mean(loss_grad**2) # Mock loss\n",
        "        return total_loss\n",
        "\n",
        "    def _evaluate(self) -> float:\n",
        "        # Mock evaluation: accuracy improves as training progresses\n",
        "        # This simulates the model getting better over time\n",
        "        base_accuracy = 0.1\n",
        "        improvement = (len(self.history['loss']) / 100) # Simple linear improvement\n",
        "        return min(base_accuracy + improvement, 0.95) + random.uniform(-0.05, 0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda7da1d07f04879afe6f5fe1015aa15",
      "metadata": {},
      "source": [
        "## Part 2: Monitoring and Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae68d57e90a248fbad2748adfeb05b3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history: Dict[str, List[float]]):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Plot training loss\n",
        "    ax1.plot(history['loss'], label='Loss')\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Plot training accuracy\n",
        "    ax2.plot(history['accuracy'], label='Accuracy', color='orange')\n",
        "    ax2.set_title('Training Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a9b94f2e2ff46e8868f82390b89fc98",
      "metadata": {},
      "source": [
        "## Part 3: Advanced Evaluation Metrics (BLEU Score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9979e204371c49d9b1c3be200b240dbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_bleu(reference: str, candidate: str, n: int = 4) -> float:\n",
        "    \"\"\"A simplified implementation of the BLEU score.\"\"\"\n",
        "    ref_tokens = reference.lower().split()\n",
        "    cand_tokens = candidate.lower().split()\n",
        "    \n",
        "    clipped_counts = 0\n",
        "    candidate_ngram_counts = 0\n",
        "    \n",
        "    for i in range(1, n + 1):\n",
        "        # N-grams for candidate\n",
        "        cand_ngrams = Counter([' '.join(cand_tokens[j:j+i]) for j in range(len(cand_tokens) - i + 1)])\n",
        "        if not cand_ngrams: continue\n",
        "        \n",
        "        # N-grams for reference\n",
        "        ref_ngrams = Counter([' '.join(ref_tokens[j:j+i]) for j in range(len(ref_tokens) - i + 1)])\n",
        "        \n",
        "        # Clip counts\n",
        "        for ngram, count in cand_ngrams.items():\n",
        "            clipped_counts += min(count, ref_ngrams.get(ngram, 0))\n",
        "            candidate_ngram_counts += count\n",
        "\n",
        "    if candidate_ngram_counts == 0:\n",
        "        return 0.0\n",
        "        \n",
        "    precision = clipped_counts / candidate_ngram_counts\n",
        "    \n",
        "    # Brevity Penalty\n",
        "    len_cand, len_ref = len(cand_tokens), len(ref_tokens)\n",
        "    if len_cand > len_ref:\n",
        "        bp = 1.0\n",
        "    else:\n",
        "        bp = math.exp(1 - len_ref / len_cand) if len_cand > 0 else 0.0\n",
        "        \n",
        "    return bp * precision\n",
        "\n",
        "# Example BLEU score calculation\n",
        "reference_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "candidate_1 = \"The fast brown fox jumps over the lazy dog\" # High similarity\n",
        "candidate_2 = \"A cat sits on the mat\" # Low similarity\n",
        "\n",
        "bleu_1 = simple_bleu(reference_text, candidate_1)\n",
        "bleu_2 = simple_bleu(reference_text, candidate_2)\n",
        "\n",
        "print(f'--- BLEU Score Example ---\")\n",
        "print(f'BLEU for candidate 1: {bleu_1:.4f}')\n",
        "print(f'BLEU for candidate 2: {bleu_2:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0b5ffc140e4b3b91c63378891cc69d",
      "metadata": {},
      "source": [
        "## Part 4: Running the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c27c811b2fe470983b027f1b759932b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model and pipeline\n",
        "model = MockModel()\n",
        "mock_dataset = [1] * 20 # 20 dummy examples\n",
        "pipeline = TrainingPipeline(model, mock_dataset, lr=0.01)\n",
        "\n",
        "# Run training\n",
        "pipeline.run(epochs=20)\n",
        "\n",
        "# Plot the results\n",
        "plot_history(pipeline.history)\n",
        "\n",
        "# Now, let's load a checkpoint and inspect it\n",
        "checkpoint_path = 'model_epoch_20.npy'\n",
        "if os.path.exists(checkpoint_path):\n",
        "    new_model = MockModel()\n",
        "    new_model.load_checkpoint(checkpoint_path)\n",
        "    print(f'\n",
        "Successfully loaded model from {checkpoint_path}')\n",
        "    print(f'Weights are the same: {np.allclose(model.weights, new_model.weights)}')\n",
        "    \n",
        "    # Clean up checkpoint files\n",
        "    for f in os.listdir():\n",
        "        if f.endswith('.npy'):\n",
        "            os.remove(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87063ae724342dab065b09899130610",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Implement ROUGE Score**: ROUGE is another popular metric that focuses on recall (while BLEU focuses on precision). Implement a simple version of ROUGE-N, which counts the overlap of n-grams between the reference and candidate.\n",
        "2. **Add Learning Rate Scheduling**: Modify the `TrainingPipeline` to include a simple learning rate scheduler, such as one that decreases the learning rate by a factor of 10 every 5 epochs. How does this affect the training curve?\n",
        "3. **Save Best Checkpoint Only**: Modify the checkpointing logic to only save the model if its evaluation accuracy has improved since the last save. This is a common strategy to avoid filling up disk space.\n",
        "\n",
        "## Summary\n",
        "\n",
        "You learned:\n",
        "- How to create a structured pipeline for running fine-tuning jobs.\n",
        "- The importance of monitoring metrics and how to visualize them.\n",
        "- How to implement a text generation evaluation metric like BLEU from scratch.\n",
        "- A practical approach to saving and loading model checkpoints during training."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}