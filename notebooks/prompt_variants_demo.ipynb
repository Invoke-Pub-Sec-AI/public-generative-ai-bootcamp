{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Engineering: Variants Comparison Demo\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand different prompt engineering techniques\n",
        "- Compare zero-shot, few-shot, and chain-of-thought approaches\n",
        "- Analyze quality vs latency trade-offs\n",
        "- Learn systematic prompt evaluation methods\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Prompt engineering is both art and science. The same task can be accomplished with vastly different prompts, each with different trade-offs in:\n",
        "- **Quality**: Accuracy and relevance of responses\n",
        "- **Latency**: Speed of response generation\n",
        "- **Consistency**: Reliability across different inputs\n",
        "- **Cost**: Token usage and associated costs\n",
        "\n",
        "This demo will systematically compare different prompting approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install asksageclient pip_system_certs rich pandas matplotlib seaborn tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔐 Cell 1 — Load secrets (Colab) + pricing + token utils\n",
        "# ================================\n",
        "import os, time, csv\n",
        "from typing import Optional, Dict\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "ASKSAGE_API_KEY = userdata.get(\"ASKSAGE_API_KEY\")\n",
        "ASKSAGE_BASE_URL = userdata.get(\"ASKSAGE_BASE_URL\")\n",
        "ASKSAGE_EMAIL = userdata.get(\"ASKSAGE_EMAIL\")\n",
        "\n",
        "assert ASKSAGE_API_KEY, \"ASKSAGE_API_KEY not provided.\"\n",
        "assert ASKSAGE_EMAIL, \"ASKSAGE_EMAIL not provided.\"\n",
        "\n",
        "print(\"✓ Secrets loaded\")\n",
        "print(\"  • EMAIL:\", ASKSAGE_EMAIL)\n",
        "print(\"  • BASE URL:\", ASKSAGE_BASE_URL or \"(default)\")\n",
        "\n",
        "# Pricing (USD per 1,000,000 tokens)\n",
        "PRICES_PER_M = {\n",
        "    \"gpt-5\": {\"input_per_m\": 1.25, \"output_per_m\": 10.00},\n",
        "    \"gpt-5-mini\": {\"input_per_m\": 0.25, \"output_per_m\": 2.00},\n",
        "}\n",
        "\n",
        "# Tokenizer\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text or \"\"))\n",
        "\n",
        "def cost_usd(model: str, input_tokens: int, output_tokens: int) -> float:\n",
        "    if model not in PRICES_PER_M:\n",
        "        raise ValueError(f\"Unknown model: {model}\")\n",
        "    r = PRICES_PER_M[model]\n",
        "    return (input_tokens / 1_000_000) * r[\"input_per_m\"] + (output_tokens / 1_000_000) * r[\"output_per_m\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔧 Cell 2 — Import bootcamp_common and setup AskSage client\n",
        "# ================================\n",
        "import sys\n",
        "sys.path.append('../../../')  # Adjust path to reach bootcamp_common\n",
        "\n",
        "from bootcamp_common.ask_sage import AskSageClient\n",
        "import json, random\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.panel import Panel\n",
        "from rich.progress import track\n",
        "\n",
        "# Initialize AskSage client\n",
        "client = AskSageClient(\n",
        "    api_key=ASKSAGE_API_KEY,\n",
        "    base_url=ASKSAGE_BASE_URL\n",
        ")\n",
        "\n",
        "console = Console()\n",
        "print(\"✓ AskSage client initialized\")\n",
        "print(\"✅ Libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task: Email Classification\n",
        "\n",
        "We'll use email classification as our test task - classifying emails as 'urgent', 'normal', or 'spam'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PromptResult:\n",
        "    prompt_type: str\n",
        "    prompt_text: str\n",
        "    response: str\n",
        "    latency_ms: float\n",
        "    input_tokens: int\n",
        "    output_tokens: int\n",
        "    total_tokens: int\n",
        "    timestamp: str\n",
        "    success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "class PromptVariantTester:\n",
        "    \"\"\"Test different prompt engineering approaches\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.setup_client()\n",
        "        self.test_emails = [\n",
        "            \"URGENT: Server down! All systems failing. Need immediate action!\",\n",
        "            \"Hi team, please review the quarterly report when you have time.\",\n",
        "            \"🎉 CONGRATULATIONS! You've WON $1,000,000! Click here NOW!\",\n",
        "            \"Meeting moved to 3pm tomorrow. Let me know if that works.\",\n",
        "            \"SECURITY ALERT: Suspicious login detected. Verify immediately!\"\n",
        "        ]\n",
        "    \n",
        "    def setup_client(self):\n",
        "        \"\"\"Setup OpenAI client (fallback to mock if not available)\"\"\"\n",
        "        if os.getenv('OPENAI_API_KEY'):\n",
        "            try:\n",
        "                self.client = openai.OpenAI()\n",
        "                self.has_api = True\n",
        "                console.print(\"✅ OpenAI client configured\")\n",
        "            except Exception as e:\n",
        "                self.has_api = False\n",
        "                console.print(f\"⚠️ OpenAI setup failed, using mock responses: {e}\")\n",
        "        else:\n",
        "            self.has_api = False\n",
        "            console.print(\"💡 No OpenAI API key found, using mock responses\")\n",
        "    \n",
        "    def create_prompt_variants(self, email_text: str) -> Dict[str, str]:\n",
        "        \"\"\"Create different prompt variants for the same task\"\"\"\n",
        "        \n",
        "        variants = {\n",
        "            'zero_shot_simple': f\"\"\"\n",
        "Classify this email as urgent, normal, or spam:\n",
        "\n",
        "{email_text}\n",
        "\n",
        "Classification:\"\"\",\n",
        "            \n",
        "            'zero_shot_detailed': f\"\"\"\n",
        "You are an email classification system. Analyze the following email and classify it into one of these categories:\n",
        "- urgent: Requires immediate attention (security issues, system failures, deadlines)\n",
        "- normal: Regular business communication that can be handled during normal workflow\n",
        "- spam: Unwanted promotional content, scams, or irrelevant messages\n",
        "\n",
        "Email to classify:\n",
        "{email_text}\n",
        "\n",
        "Classification (respond with just the category):\"\"\",\n",
        "            \n",
        "            'few_shot': f\"\"\"\n",
        "Classify emails as urgent, normal, or spam. Here are examples:\n",
        "\n",
        "Email: \"Server outage affecting all customers. Need immediate fix!\"\n",
        "Classification: urgent\n",
        "\n",
        "Email: \"Weekly team meeting scheduled for Friday at 2pm.\"\n",
        "Classification: normal\n",
        "\n",
        "Email: \"WIN BIG! Click here for amazing deals!\"\n",
        "Classification: spam\n",
        "\n",
        "Email: {email_text}\n",
        "Classification:\"\"\",\n",
        "            \n",
        "            'chain_of_thought': f\"\"\"\n",
        "Classify this email as urgent, normal, or spam. Think through your reasoning step by step:\n",
        "\n",
        "Email: {email_text}\n",
        "\n",
        "Analysis:\n",
        "1. What is the main topic/purpose of this email?\n",
        "2. What language patterns indicate urgency or spam?\n",
        "3. What is the likely sender and context?\n",
        "4. Based on these factors, what is the appropriate classification?\n",
        "\n",
        "Final Classification:\"\"\",\n",
        "            \n",
        "            'system_prompt': f\"\"\"\n",
        "System: You are a professional email classifier with years of experience. You excel at quickly identifying urgent communications and filtering out spam.\n",
        "\n",
        "User: Classify this email as urgent, normal, or spam:\n",
        "\n",
        "{email_text}\n",
        "\n",
        "Assistant:\"\"\"\n",
        "        }\n",
        "        \n",
        "        return variants\n",
        "    \n",
        "    def test_prompt(self, prompt_type: str, prompt_text: str, model: str = \"gpt-3.5-turbo\") -> PromptResult:\n",
        "        \"\"\"Test a single prompt variant\"\"\"\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        if self.has_api:\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "                    max_tokens=150,\n",
        "                    temperature=0.1  # Low temperature for consistent classification\n",
        "                )\n",
        "                \n",
        "                latency_ms = (time.time() - start_time) * 1000\n",
        "                \n",
        "                return PromptResult(\n",
        "                    prompt_type=prompt_type,\n",
        "                    prompt_text=prompt_text,\n",
        "                    response=response.choices[0].message.content.strip(),\n",
        "                    latency_ms=round(latency_ms, 2),\n",
        "                    input_tokens=response.usage.prompt_tokens,\n",
        "                    output_tokens=response.usage.completion_tokens,\n",
        "                    total_tokens=response.usage.total_tokens,\n",
        "                    timestamp=datetime.now().isoformat(),\n",
        "                    success=True\n",
        "                )\n",
        "                \n",
        "            except Exception as e:\n",
        "                return PromptResult(\n",
        "                    prompt_type=prompt_type,\n",
        "                    prompt_text=prompt_text,\n",
        "                    response=\"\",\n",
        "                    latency_ms=0,\n",
        "                    input_tokens=0,\n",
        "                    output_tokens=0,\n",
        "                    total_tokens=0,\n",
        "                    timestamp=datetime.now().isoformat(),\n",
        "                    success=False,\n",
        "                    error_message=str(e)\n",
        "                )\n",
        "        else:\n",
        "            # Mock responses for demo\n",
        "            mock_responses = {\n",
        "                'zero_shot_simple': 'urgent',\n",
        "                'zero_shot_detailed': 'urgent',\n",
        "                'few_shot': 'urgent', \n",
        "                'chain_of_thought': '1. This email mentions server/system issues\\n2. Uses urgent language\\n3. Requests immediate action\\n4. Classification: urgent',\n",
        "                'system_prompt': 'urgent'\n",
        "            }\n",
        "            \n",
        "            # Simulate varying latencies\n",
        "            mock_latency = {\n",
        "                'zero_shot_simple': 850,\n",
        "                'zero_shot_detailed': 1200,\n",
        "                'few_shot': 1500,\n",
        "                'chain_of_thought': 2100,\n",
        "                'system_prompt': 950\n",
        "            }\n",
        "            \n",
        "            # Simulate token counts\n",
        "            input_tokens = len(prompt_text.split()) * 1.3\n",
        "            output_tokens = len(mock_responses[prompt_type].split()) * 1.3\n",
        "            \n",
        "            time.sleep(mock_latency[prompt_type] / 1000)  # Simulate latency\n",
        "            \n",
        "            return PromptResult(\n",
        "                prompt_type=prompt_type,\n",
        "                prompt_text=prompt_text,\n",
        "                response=mock_responses[prompt_type],\n",
        "                latency_ms=mock_latency[prompt_type],\n",
        "                input_tokens=int(input_tokens),\n",
        "                output_tokens=int(output_tokens),\n",
        "                total_tokens=int(input_tokens + output_tokens),\n",
        "                timestamp=datetime.now().isoformat(),\n",
        "                success=True\n",
        "            )\n",
        "    \n",
        "    def run_comparison(self, email_index: int = 0) -> List[PromptResult]:\n",
        "        \"\"\"Run comparison across all prompt variants\"\"\"\n",
        "        \n",
        "        email_text = self.test_emails[email_index]\n",
        "        console.print(f\"\\n📧 Testing email: '{email_text}'\\n\")\n",
        "        \n",
        "        variants = self.create_prompt_variants(email_text)\n",
        "        results = []\n",
        "        \n",
        "        for prompt_type, prompt_text in variants.items():\n",
        "            console.print(f\"🔄 Testing {prompt_type.replace('_', ' ').title()}...\")\n",
        "            \n",
        "            result = self.test_prompt(prompt_type, prompt_text)\n",
        "            results.append(result)\n",
        "            \n",
        "            if result.success:\n",
        "                console.print(f\"  ✅ Response: {result.response[:100]}...\")\n",
        "                console.print(f\"  ⏱️ Latency: {result.latency_ms}ms, Tokens: {result.total_tokens}\")\n",
        "            else:\n",
        "                console.print(f\"  ❌ Failed: {result.error_message}\")\n",
        "            \n",
        "            time.sleep(0.5)  # Brief pause between requests\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Initialize the tester\n",
        "tester = PromptVariantTester()\n",
        "print(\"🔧 Prompt variant tester ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the Comparison\n",
        "\n",
        "Let's test different prompt variants on the same email and compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the comparison\n",
        "results = tester.run_comparison(email_index=0)  # Test the urgent email\n",
        "\n",
        "# Create comparison table\n",
        "if results:\n",
        "    table = Table(title=\"Prompt Variant Comparison\")\n",
        "    table.add_column(\"Prompt Type\")\n",
        "    table.add_column(\"Response\")\n",
        "    table.add_column(\"Latency (ms)\")\n",
        "    table.add_column(\"Input Tokens\")\n",
        "    table.add_column(\"Output Tokens\")\n",
        "    table.add_column(\"Total Tokens\")\n",
        "    \n",
        "    for result in results:\n",
        "        if result.success:\n",
        "            table.add_row(\n",
        "                result.prompt_type.replace('_', ' ').title(),\n",
        "                result.response[:50] + '...' if len(result.response) > 50 else result.response,\n",
        "                str(result.latency_ms),\n",
        "                str(result.input_tokens),\n",
        "                str(result.output_tokens),\n",
        "                str(result.total_tokens)\n",
        "            )\n",
        "    \n",
        "    console.print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis and Visualization\n",
        "\n",
        "Let's analyze the performance characteristics of each approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results to DataFrame for analysis\n",
        "if results:\n",
        "    df = pd.DataFrame([asdict(r) for r in results if r.success])\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. Latency comparison\n",
        "    axes[0, 0].bar(df['prompt_type'], df['latency_ms'], alpha=0.7, color='skyblue')\n",
        "    axes[0, 0].set_title('Latency by Prompt Type')\n",
        "    axes[0, 0].set_ylabel('Latency (ms)')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 2. Token usage comparison\n",
        "    axes[0, 1].bar(df['prompt_type'], df['total_tokens'], alpha=0.7, color='lightcoral')\n",
        "    axes[0, 1].set_title('Total Tokens by Prompt Type')\n",
        "    axes[0, 1].set_ylabel('Total Tokens')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 3. Input vs Output tokens\n",
        "    x = range(len(df))\n",
        "    axes[1, 0].bar(x, df['input_tokens'], alpha=0.7, label='Input Tokens', color='lightgreen')\n",
        "    axes[1, 0].bar(x, df['output_tokens'], bottom=df['input_tokens'], alpha=0.7, \n",
        "                   label='Output Tokens', color='orange')\n",
        "    axes[1, 0].set_title('Input vs Output Tokens')\n",
        "    axes[1, 0].set_ylabel('Tokens')\n",
        "    axes[1, 0].set_xticks(x)\n",
        "    axes[1, 0].set_xticklabels(df['prompt_type'], rotation=45)\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # 4. Efficiency (tokens per ms)\n",
        "    df['efficiency'] = df['total_tokens'] / df['latency_ms']\n",
        "    axes[1, 1].bar(df['prompt_type'], df['efficiency'], alpha=0.7, color='gold')\n",
        "    axes[1, 1].set_title('Efficiency (Tokens per ms)')\n",
        "    axes[1, 1].set_ylabel('Tokens/ms')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Summary statistics\n",
        "    console.print(\"\\n📊 Summary Statistics:\\n\")\n",
        "    summary_table = Table()\n",
        "    summary_table.add_column(\"Metric\")\n",
        "    summary_table.add_column(\"Fastest\")\n",
        "    summary_table.add_column(\"Most Efficient\")\n",
        "    summary_table.add_column(\"Most Detailed\")\n",
        "    \n",
        "    fastest = df.loc[df['latency_ms'].idxmin()]\n",
        "    most_efficient = df.loc[df['efficiency'].idxmax()]\n",
        "    most_detailed = df.loc[df['output_tokens'].idxmax()]\n",
        "    \n",
        "    summary_table.add_row(\n",
        "        \"Prompt Type\",\n",
        "        fastest['prompt_type'],\n",
        "        most_efficient['prompt_type'],\n",
        "        most_detailed['prompt_type']\n",
        "    )\n",
        "    \n",
        "    summary_table.add_row(\n",
        "        \"Latency (ms)\",\n",
        "        str(fastest['latency_ms']),\n",
        "        str(most_efficient['latency_ms']),\n",
        "        str(most_detailed['latency_ms'])\n",
        "    )\n",
        "    \n",
        "    console.print(summary_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insights from Prompt Variant Testing\n",
        "\n",
        "### 🚀 **Zero-Shot Simple**\n",
        "- **Pros**: Fastest, lowest token usage, direct\n",
        "- **Cons**: Less reliable, minimal context\n",
        "- **Best for**: Simple, well-defined tasks with clear categories\n",
        "\n",
        "### 📋 **Zero-Shot Detailed**  \n",
        "- **Pros**: Clear instructions, better reliability\n",
        "- **Cons**: Higher token usage, moderate latency\n",
        "- **Best for**: Tasks requiring specific criteria or edge case handling\n",
        "\n",
        "### 📚 **Few-Shot**\n",
        "- **Pros**: Demonstrates desired format, handles ambiguous cases better\n",
        "- **Cons**: Higher input tokens, requires good examples\n",
        "- **Best for**: Tasks where format matters, complex classification\n",
        "\n",
        "### 🧠 **Chain-of-Thought**\n",
        "- **Pros**: Shows reasoning, more accurate for complex decisions\n",
        "- **Cons**: Highest latency and token usage\n",
        "- **Best for**: Complex reasoning, when explanation is needed\n",
        "\n",
        "### 🤖 **System Prompt**\n",
        "- **Pros**: Sets clear role and expectations\n",
        "- **Cons**: May not work consistently across all models\n",
        "- **Best for**: Conversational interfaces, role-specific tasks\n",
        "\n",
        "## Choosing the Right Approach\n",
        "\n",
        "Consider these factors:\n",
        "\n",
        "1. **Task Complexity**: Simple tasks → Zero-shot; Complex tasks → Chain-of-thought\n",
        "2. **Latency Requirements**: Real-time apps → Simple prompts; Batch processing → Detailed prompts\n",
        "3. **Accuracy Needs**: High stakes → Few-shot or CoT; General use → Zero-shot\n",
        "4. **Cost Constraints**: Budget-sensitive → Simple prompts; Quality-focused → Detailed prompts\n",
        "5. **Consistency Requirements**: Variable inputs → Few-shot; Stable inputs → Zero-shot\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "In Lab 5, you'll implement and test your own prompt variants with a systematic evaluation framework!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}