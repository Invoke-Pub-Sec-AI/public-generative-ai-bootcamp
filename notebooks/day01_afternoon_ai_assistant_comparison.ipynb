{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Assistant Comparison\n",
        "\n",
        "## Learning Objectives\n",
        "- See different AI models tackle the same real-world problem\n",
        "- Compare their approaches, speed, and quality\n",
        "- Understand how streaming affects user experience\n",
        "- Learn about token budgets and cost optimization\n",
        "\n",
        "## The Demo: Model Comparison in Action\n",
        "\n",
        "We'll test multiple AI models on the same challenging task:\n",
        "1. **Problem Setup** - A complex business scenario\n",
        "2. **Model Testing** - Different AI assistants tackle the problem\n",
        "3. **Performance Analysis** - Speed, quality, and cost comparison\n",
        "4. **Streaming Demo** - Real-time vs batch processing\n",
        "5. **Budget Impact** - Cost implications of different approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "!pip install asksageclient pip_system_certs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Import our AskSage client\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "# Get API credentials from Google Colab secrets\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('ASKSAGE_API_KEY')\n",
        "email = userdata.get('ASKSAGE_EMAIL')\n",
        "\n",
        "# Initialize client and tokenizer\n",
        "client = AskSageClient(api_key=api_key, email=email)\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(\"AskSage client initialized successfully\")\n",
        "print(\"Ready to showcase AI capabilities...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Scenario: Market Analysis Challenge\n",
        "\n",
        "**Business Problem**: A startup needs to analyze market opportunities for a new product\n",
        "\n",
        "**Requirements**:\n",
        "- Analyze market size and growth potential\n",
        "- Identify key competitors and their strategies\n",
        "- Recommend go-to-market approach\n",
        "- Assess risks and mitigation strategies\n",
        "- Provide actionable next steps\n",
        "\n",
        "Let's see how different models handle this complex analysis..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the test prompt\n",
        "market_analysis_prompt = \"\"\"\n",
        "MARKET ANALYSIS REQUEST:\n",
        "\n",
        "Our startup is developing an AI-powered personal finance app targeting millennials and Gen Z. \n",
        "We need a comprehensive market analysis including:\n",
        "\n",
        "1. MARKET SIZE & OPPORTUNITY\n",
        "   - Total addressable market (TAM)\n",
        "   - Serviceable addressable market (SAM)\n",
        "   - Market growth trends and drivers\n",
        "\n",
        "2. COMPETITIVE LANDSCAPE\n",
        "   - Direct and indirect competitors\n",
        "   - Their strengths, weaknesses, and market positioning\n",
        "   - Pricing strategies and business models\n",
        "\n",
        "3. TARGET CUSTOMER ANALYSIS\n",
        "   - Customer segments and personas\n",
        "   - Pain points and unmet needs\n",
        "   - Buying behavior and decision factors\n",
        "\n",
        "4. GO-TO-MARKET STRATEGY\n",
        "   - Recommended market entry approach\n",
        "   - Distribution channels and partnerships\n",
        "   - Marketing and customer acquisition strategies\n",
        "\n",
        "5. RISK ASSESSMENT\n",
        "   - Market risks and challenges\n",
        "   - Regulatory considerations\n",
        "   - Mitigation strategies\n",
        "\n",
        "Provide actionable insights with specific recommendations and next steps.\n",
        "\"\"\"\n",
        "\n",
        "prompt_tokens = len(tokenizer.encode(market_analysis_prompt))\n",
        "print(f\"Test prompt: {prompt_tokens} tokens\")\n",
        "print(\"\\nPreparing to test multiple AI models...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1: GPT-4o-mini (Speed Optimized)\n",
        "\n",
        "Testing the fastest model for quick iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "response_mini = client.query(\n",
        "    message=market_analysis_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.2,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "mini_time = time.time() - start_time\n",
        "mini_response = mini_response.get(\"message\").strip()\n",
        "mini_tokens = len(tokenizer.encode(mini_response))\n",
        "\n",
        "mini_tokens = len(tokenizer.encode(mini_response))\n",
        "\n",
        "mini_response = response_mini.get(\"message\").strip()\n",
        "mini_tokens = len(tokenizer.encode(mini_response))\n",
        "\n",
        "print(f\"Response time: {mini_time:.2f} seconds\")\n",
        "print(f\"Output tokens: {mini_tokens}\")\n",
        "print(f\"Tokens per second: {mini_tokens/mini_time:.1f}\")\n",
        "print(\"\\nResponse preview:\")\n",
        "print(mini_response[:500] + \"...\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 2: GPT-4o (Balanced Performance)\n",
        "\n",
        "Testing the balanced model for quality and speed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "response_4o = client.query(\n",
        "    message=market_analysis_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.2,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "gpt4o_time = time.time() - start_time\n",
        "gpt4o_response = gpt4o_response.get(\"message\").strip()\n",
        "gpt4o_tokens = len(tokenizer.encode(gpt4o_response))\n",
        "\n",
        "gpt4o_tokens = len(tokenizer.encode(gpt4o_response))\n",
        "\n",
        "gpt4o_response = response_4o.get(\"message\").strip()\n",
        "gpt4o_tokens = len(tokenizer.encode(gpt4o_response))\n",
        "\n",
        "print(f\"Response time: {gpt4o_time:.2f} seconds\")\n",
        "print(f\"Output tokens: {gpt4o_tokens}\")\n",
        "print(f\"Tokens per second: {gpt4o_tokens/gpt4o_time:.1f}\")\n",
        "print(\"\\nResponse preview:\")\n",
        "print(gpt4o_response[:500] + \"...\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 3: Claude (Alternative Approach)\n",
        "\n",
        "Testing Claude for different reasoning patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GPT-5-mini (if available)\n",
        "print(\"=== TESTING CLAUDE ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "response_gpt-5-mini = client.query(\n",
        "    message=market_analysis_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.2,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "    \n",
        "    gpt-5-mini_time = time.time() - start_time\n",
        "gpt-5-mini_response = response_gpt-5-mini.get(\"message\").strip()\n",
        "    gpt-5-mini_tokens = len(tokenizer.encode(gpt-5-mini_response))\n",
        "    \n",
        "    print(f\"Response time: {gpt-5-mini_time:.2f} seconds\")\n",
        "    print(f\"Output tokens: {gpt-5-mini_tokens}\")\n",
        "    print(f\"Tokens per second: {gpt-5-mini_tokens/gpt-5-mini_time:.1f}\")\n",
        "    print(\"\\nResponse preview:\")\n",
        "    print(gpt-5-mini_response[:500] + \"...\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"GPT-5-mini not available: {e}\")\n",
        "    gpt-5-mini_time = 0\n",
        "    gpt-5-mini_tokens = 0\n",
        "    gpt-5-mini_response = \"Model not available\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming vs Non-Streaming Comparison\n",
        "\n",
        "Let's see how streaming affects the user experience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate streaming vs non-streaming experience\n",
        "print(\"=== STREAMING VS NON-STREAMING DEMO ===\")\n",
        "\n",
        "# Non-streaming (what we did above)\n",
        "print(f\"\\nNON-STREAMING EXPERIENCE:\")\n",
        "print(f\"  User waits: {mini_time:.2f} seconds\")\n",
        "print(f\"  Then receives: {mini_tokens} tokens instantly\")\n",
        "print(f\"  User experience: Wait... then flood of information\")\n",
        "\n",
        "# Simulate streaming experience\n",
        "print(f\"\\nSTREAMING EXPERIENCE (simulated):\")\n",
        "print(f\"  First token appears: ~0.5 seconds\")\n",
        "print(f\"  Tokens stream at: ~{mini_tokens/mini_time:.0f} tokens/second\")\n",
        "print(f\"  User experience: Immediate feedback, progressive revelation\")\n",
        "\n",
        "# Demonstrate with a shorter example\n",
        "short_prompt = \"Explain the key benefits of AI-powered personal finance apps in 3 bullet points.\"\n",
        "\n",
        "print(\"\\n=== STREAMING SIMULATION ===\")\n",
        "print(\"Sending short request to demonstrate streaming...\")\n",
        "\n",
        "start_time = time.time()\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "stream_response = client.query(\n",
        "    message=short_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "stream_time = time.time() - start_time\n",
        "stream_response = stream_response.get(\"message\").strip()\n",
        "stream_tokens = len(tokenizer.encode(stream_response))\n",
        "\n",
        "stream_tokens = len(tokenizer.encode(stream_response))\n",
        "\n",
        "\n",
        "stream_content = stream_response.get(\"message\").strip()\n",
        "stream_content = len(tokenizer.encode(stream_content))\n",
        "\n",
        "print(f\"\\nComplete response ({stream_time:.2f}s):\")\n",
        "print(stream_content)\n",
        "\n",
        "# Simulate how this would appear with streaming\n",
        "print(\"\\nHow this would appear with streaming:\")\n",
        "words = stream_content.split()\n",
        "for i, word in enumerate(words[:20]):  # Show first 20 words\n",
        "    if i % 5 == 0 and i > 0:\n",
        "        print(f\"\\n[{i*0.1:.1f}s] \", end=\"\")\n",
        "    print(word, end=\" \")\n",
        "    time.sleep(0.05)  # Simulate streaming delay\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Budget and Cost Analysis\n",
        "\n",
        "Let's analyze the cost implications of different models and approaches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cost analysis (using approximate pricing)\n",
        "print(\"=== COST ANALYSIS ===\")\n",
        "\n",
        "# Approximate pricing per 1K tokens (as of 2024)\n",
        "pricing = {\n",
        "    \"gpt-5-mini\": {\"input\": 0.00015, \"output\": 0.0006},\n",
        "    \"gpt-5-mini\": {\"input\": 0.0025, \"output\": 0.01},\n",
        "    \"gpt-5-mini-3-sonnet\": {\"input\": 0.003, \"output\": 0.015}\n",
        "}\n",
        "\n",
        "def calculate_cost(model, input_tokens, output_tokens):\n",
        "    if model not in pricing:\n",
        "        return 0\n",
        "    input_cost = (input_tokens / 1000) * pricing[model][\"input\"]\n",
        "    output_cost = (output_tokens / 1000) * pricing[model][\"output\"]\n",
        "    return input_cost + output_cost\n",
        "\n",
        "# Calculate costs for our test\n",
        "models_tested = [\n",
        "    (\"gpt-5-mini\", mini_time, mini_tokens),\n",
        "    (\"gpt-5-mini\", gpt4o_time, gpt4o_tokens),\n",
        "]\n",
        "\n",
        "if gpt-5-mini_tokens > 0:\n",
        "    models_tested.append((\"gpt-5-mini-3-sonnet\", gpt-5-mini_time, gpt-5-mini_tokens))\n",
        "\n",
        "print(f\"\\nCOST COMPARISON (per request):\")\n",
        "print(f\"Input tokens: {prompt_tokens}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for model, response_time, output_tokens in models_tested:\n",
        "    cost = calculate_cost(model, prompt_tokens, output_tokens)\n",
        "    print(f\"{model:15} | {response_time:5.2f}s | {output_tokens:4d} tokens | ${cost:.4f}\")\n",
        "\n",
        "print(\"\\nSCALE ANALYSIS (1000 requests/month):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for model, response_time, output_tokens in models_tested:\n",
        "    cost_per_request = calculate_cost(model, prompt_tokens, output_tokens)\n",
        "    monthly_cost = cost_per_request * 1000\n",
        "    monthly_time = (response_time * 1000) / 3600  # Convert to hours\n",
        "    print(f\"{model:15} | {monthly_time:5.1f}h | ${monthly_cost:6.2f}/month\")\n",
        "\n",
        "print(\"\\nBUDGET RECOMMENDATIONS:\")\n",
        "print(\"- Development/Testing: Use gpt-5-mini for speed and cost\")\n",
        "print(\"- Production (Quality): Use gpt-5-mini for balanced performance\")\n",
        "print(\"- High-stakes Analysis: Consider premium models for critical decisions\")\n",
        "print(\"- Streaming: Improves UX with minimal cost impact\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quality Assessment\n",
        "\n",
        "Let's analyze the quality differences between model responses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality analysis using AI to evaluate responses\n",
        "quality_prompt = f\"\"\"\n",
        "Evaluate these AI responses to a market analysis request on a scale of 1-10 for:\n",
        "1. Completeness (covers all required sections)\n",
        "2. Accuracy (realistic data and insights)\n",
        "3. Actionability (specific, implementable recommendations)\n",
        "4. Structure (clear organization and flow)\n",
        "5. Business Value (practical utility for decision-making)\n",
        "\n",
        "ORIGINAL REQUEST:\n",
        "{market_analysis_prompt[:300]}...\n",
        "\n",
        "RESPONSE 1 (GPT-5-mini):\n",
        "{mini_response[:400]}...\n",
        "\n",
        "RESPONSE 2 (GPT-5-mini):\n",
        "{gpt4o_response[:400]}...\n",
        "\n",
        "Provide scores and brief justification for each criterion.\n",
        "\"\"\"\n",
        "\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "quality_response = client.query(\n",
        "    message=quality_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "quality_analysis = quality_response.get(\"message\").strip()\n",
        "quality_analysis = len(tokenizer.encode(quality_analysis))\n",
        "\n",
        "print(\"=== QUALITY ASSESSMENT ===\")\n",
        "print(quality_analysis)\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison Summary\n",
        "\n",
        "### Performance Metrics:\n",
        "\n",
        "**Speed Comparison:**\n",
        "- GPT-4o-mini: Fastest response time, good for iterations\n",
        "- GPT-4o: Balanced speed and quality\n",
        "- Claude: Alternative reasoning approach (when available)\n",
        "\n",
        "**Cost Efficiency:**\n",
        "- GPT-4o-mini: Most cost-effective for development\n",
        "- GPT-4o: Higher cost but better quality\n",
        "- Premium models: Best for critical business decisions\n",
        "\n",
        "**User Experience:**\n",
        "- Non-streaming: Wait time then complete response\n",
        "- Streaming: Immediate feedback, progressive revelation\n",
        "- Streaming improves perceived performance significantly\n",
        "\n",
        "### Choosing the Right Model:\n",
        "\n",
        "**Development Phase:**\n",
        "- Use fast, cheap models for rapid iteration\n",
        "- Focus on prompt engineering and workflow\n",
        "\n",
        "**Production Deployment:**\n",
        "- Balance cost, speed, and quality requirements\n",
        "- Consider user experience and business impact\n",
        "\n",
        "**Critical Applications:**\n",
        "- Use highest quality models for important decisions\n",
        "- Implement multiple model validation for accuracy\n",
        "\n",
        "### Next Steps:\n",
        "You'll learn to optimize these trade-offs through:\n",
        "- Prompt engineering techniques\n",
        "- Token budget management\n",
        "- Quality assurance patterns\n",
        "- Cost optimization strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final comparison summary\n",
        "print(\"=== MODEL COMPARISON SUMMARY ===\")\n",
        "print(\"\\nPerformance Results:\")\n",
        "for model, response_time, output_tokens in models_tested:\n",
        "    cost = calculate_cost(model, prompt_tokens, output_tokens)\n",
        "    efficiency = output_tokens / response_time\n",
        "    print(f\"  {model:15}: {response_time:5.2f}s | {efficiency:5.0f} tok/s | ${cost:.4f}\")\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"  - Speed vs Quality: Clear trade-offs exist\")\n",
        "print(\"  - Cost scales significantly with model choice\")\n",
        "print(\"  - Streaming improves UX without major cost impact\")\n",
        "print(\"  - Model selection should match use case requirements\")\n",
        "\n",
        "print(\"\\nRecommendations:\")\n",
        "print(\"  - Development: Fast, cheap models for iteration\")\n",
        "print(\"  - Production: Balanced models for reliability\")\n",
        "print(\"  - Critical tasks: Premium models for accuracy\")\n",
        "print(\"  - Always consider streaming for better UX\")\n",
        "\n",
        "print(\"\\nNext: Learn prompt engineering to maximize model performance\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}