{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "f55a7fc75b6e4fc9b4277d33229e94dd",
      "metadata": {},
      "source": [
        "# Lab 01: Advanced Prompting Techniques\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand and implement Chain-of-Thought (CoT) prompting\n",
        "- Apply Self-Consistency to improve the reliability of CoT\n",
        "- Explore the conceptual framework of Tree of Thoughts (ToT)\n",
        "- Learn how to structure prompts to elicit more complex reasoning from LLMs\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1fa45b0742d468695c9530559a297c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List, Dict, Any\n",
        "from collections import Counter\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ceb3a7a271341d889354501340bb1a8",
      "metadata": {},
      "source": [
        "## Part 1: Chain-of-Thought (CoT) Prompting\n",
        "\n",
        "Chain-of-Thought prompting encourages the LLM to break down a problem into intermediate steps, mimicking a human-like reasoning process. This is particularly effective for arithmetic, commonsense, and symbolic reasoning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f2234eebe004234b291cd08e85b086d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockLLM:\n",
        "    \"\"\"A mock LLM to simulate responses based on prompt structure.\"\"\"\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        # If the prompt asks for steps, the LLM is more likely to produce them.\n",
        "        if 'step by step' in prompt.lower():\n",
        "            return (\n",
        "                'Step 1: Identify the number of apples. There are 5 apples.\n",
        "'\n",
        "                'Step 2: Identify the number of apples given away. 2 apples were given away.\n",
        "'\n",
        "                'Step 3: Subtract the given apples from the initial amount. 5 - 2 = 3.\n",
        "'\n",
        "                'The answer is 3.'\n",
        "            )\n",
        "        else:\n",
        "            # Without CoT, the model might guess incorrectly.\n",
        "            return 'The answer is 4.'\n",
        "\n",
        "llm = MockLLM()\n",
        "\n",
        "# Standard Prompt\n",
        "problem = 'Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?'\n",
        "standard_prompt = f'{problem}\\nA:'\n",
        "\n",
        "# Chain-of-Thought Prompt\n",
        "cot_prompt = f'{problem}\\nA: Let\\'s think step by step.'\n",
        "\n",
        "print(\"--- Standard Prompt Response ---\")\n",
        "print(llm.generate(standard_prompt))\n",
        "\n",
        "print(\"\n",
        "--- Chain-of-Thought Prompt Response ---\")\n",
        "print(llm.generate(cot_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7641d63a1daf42cca9061f347645b547",
      "metadata": {},
      "source": [
        "## Part 2: Self-Consistency\n",
        "\n",
        "Self-Consistency improves on CoT by generating multiple reasoning paths and then taking a majority vote on the final answer. This makes the model more robust to individual reasoning errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b622d965184d46a0a9c35b33cadca0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockLLMWithRandomness(MockLLM):\n",
        "    \"\"\"A mock LLM that can produce slightly different reasoning paths.\"\"\"\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if 'step by step' in prompt.lower():\n",
        "            # Simulate different reasoning paths with some randomness\n",
        "            path_1 = 'Step 1: 5 initial balls. Step 2: 2 cans * 3 balls/can = 6 balls. Step 3: 5 + 6 = 11. The answer is 11.'\n",
        "            path_2 = 'Step 1: 2 cans * 3 balls = 6. Step 2: 6 + 5 = 11. The answer is 11.'\n",
        "            path_3 = 'Step 1: Roger started with 5. Step 2: He added 2 cans, so 5+2=7. The answer is 7.' # A reasoning error\n",
        "            return random.choice([path_1, path_1, path_2, path_3]) # Skew towards the correct answer\n",
        "        return 'The answer is 8.'\n",
        "\n",
        "def self_consistency_decode(prompt: str, llm: MockLLMWithRandomness, num_paths: int) -> str:\n",
        "    responses = [llm.generate(prompt) for _ in range(num_paths)]\n",
        "    \n",
        "    # Extract the final answer from each reasoning path\n",
        "    answers = []\n",
        "    for res in responses:\n",
        "        match = re.search(r'The answer is (\\d+)', res)\n",
        "        if match:\n",
        "            answers.append(match.group(1))\n",
        "            \n",
        "    if not answers:\n",
        "        return 'Could not determine an answer.'\n",
        "        \n",
        "    # Take the majority vote\n",
        "    final_answer = Counter(answers).most_common(1)[0][0]\n",
        "    return final_answer\n",
        "\n",
        "sc_llm = MockLLMWithRandomness()\n",
        "final_answer = self_consistency_decode(cot_prompt, sc_llm, num_paths=5)\n",
        "\n",
        "print(\"--- Self-Consistency Result ---\")\n",
        "print(f'The final answer after majority vote is: {final_answer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86502e1aae474359b49f82ebe7816ff9",
      "metadata": {},
      "source": [
        "## Part 3: Tree of Thoughts (ToT) - Conceptual\n",
        "\n",
        "Tree of Thoughts extends CoT by exploring multiple reasoning paths in a tree-like structure. At each step, the model generates multiple possible \"thoughts\" and uses a deliberate search algorithm (like breadth-first or depth-first search) to explore the most promising paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f90e0fd0394afdb231de2e43eead39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToT is an algorithm that uses an LLM, not just a prompt. We'll represent it conceptually.\n",
        "\n",
        "@dataclass\n",
        "class ThoughtNode:\n",
        "    state: str  # The current state of the problem (e.g., 'Roger has 5 balls')\n",
        "    children: List['ThoughtNode'] = field(default_factory=list)\n",
        "    evaluation: float = 0.0 # How promising this path is\n",
        "\n",
        "def tot_search(problem: str, llm: MockLLM, max_depth: int, breadth: int) -> str:\n",
        "    \"\"\"A conceptual implementation of a Tree of Thoughts search.\"\"\"\n",
        "    root = ThoughtNode(state=problem)\n",
        "    frontier = [root]\n",
        "    \n",
        "    for depth in range(max_depth):\n",
        "        next_frontier = []\n",
        "        for node in frontier:\n",
        "            # 1. Generate multiple next steps (thoughts)\n",
        "            for _ in range(breadth):\n",
        "                # In a real system, the prompt would ask for the next logical step\n",
        "                next_step_prompt = f'Current state: {node.state}. What are возможные next steps?'\n",
        "                # Mocking the generation of next steps\n",
        "                next_thought = random.choice([\n",
        "                    'Calculate balls in cans (2*3=6)', \n",
        "                    'Add cans to balls (5+2=7)', \n",
        "                    'Sum everything (5+2+3=10)'\n",
        "                ])\n",
        "                child = ThoughtNode(state=f'{node.state}, then {next_thought}')\n",
        "                \n",
        "                # 2. Evaluate the new state (heuristic)\n",
        "                if '11' in child.state or '6' in child.state: # Good heuristic\n",
        "                    child.evaluation = 0.8\n",
        "                elif '7' in child.state: # Bad heuristic\n",
        "                    child.evaluation = 0.2\n",
        "                else:\n",
        "                    child.evaluation = 0.5\n",
        "                \n",
        "                node.children.append(child)\n",
        "                next_frontier.append(child)\n",
        "        \n",
        "        # 3. Prune the tree, keeping only the most promising nodes\n",
        "        frontier = sorted(next_frontier, key=lambda n: n.evaluation, reverse=True)[:breadth]\n",
        "\n",
        "    # Return the state of the best leaf node\n",
        "    best_path = sorted(frontier, key=lambda n: n.evaluation, reverse=True)[0]\n",
        "    return best_path.state\n",
        "\n",
        "print(\"--- Tree of Thoughts (Conceptual Search) ---\")\n",
        "best_reasoning_path = tot_search(problem, llm, max_depth=2, breadth=2)\n",
        "print(f'Most promising reasoning path found:\n",
        "{best_reasoning_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d723be9f324204bbbac5284f14aef1",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Create a Zero-Shot CoT Prompt**: Modify the CoT prompt to be \"zero-shot\" by simply appending \"Let's think step by step\" to the question without providing any examples. How does the mock LLM handle this? (Our current one does, but in reality, performance might vary).\n",
        "2. **Adjust Self-Consistency Parameters**: Change the `num_paths` in the `self_consistency_decode` function. What happens if you use a small number (e.g., 2) versus a large number (e.g., 20)?\n",
        "3. **Improve the ToT Evaluator**: The `evaluation` heuristic in our `tot_search` is very simple. Propose a more sophisticated way to evaluate a thought. For example, you could use the LLM itself to score how logical a step is.\n",
        "\n",
        "## Summary\n",
        "\n",
        "You learned:\n",
        "- How **Chain-of-Thought (CoT)** prompting guides an LLM to produce a reasoning process, improving accuracy.\n",
        "- How **Self-Consistency** builds on CoT by sampling multiple reasoning paths and using a majority vote to find a more reliable answer.\n",
        "- The conceptual framework of **Tree of Thoughts (ToT)**, a deliberate search algorithm that explores a tree of reasoning steps to solve more complex problems."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}