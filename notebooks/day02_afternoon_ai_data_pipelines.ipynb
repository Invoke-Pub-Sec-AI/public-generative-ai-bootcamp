{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Powered Data Pipelines\n",
        "\n",
        "## Learning Objectives\n",
        "- See AI transform raw data into business insights automatically\n",
        "- Watch intelligent data processing, cleaning, and analysis\n",
        "- Understand AI-driven data quality and validation\n",
        "- Learn about scalable, self-healing data workflows\n",
        "\n",
        "## The Demo: Intelligent Data Processing\n",
        "\n",
        "We'll demonstrate how AI can:\n",
        "1. **Analyze** raw data and understand its structure\n",
        "2. **Clean** and standardize messy datasets\n",
        "3. **Validate** data quality and detect anomalies\n",
        "4. **Transform** data for business analysis\n",
        "5. **Generate** insights and recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "!pip install asksageclient pip_system_certs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Import our AskSage client\n",
        "from asksageclient import AskSageClient\n",
        "\n",
        "# Get API credentials from Google Colab secrets\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('ASKSAGE_API_KEY')\n",
        "email = userdata.get('ASKSAGE_EMAIL')\n",
        "\n",
        "# Initialize client and tokenizer\n",
        "client = AskSageClient(api_key=api_key, email=email)\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(\"AskSage client initialized successfully\")\n",
        "print(\"Ready to showcase AI capabilities...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Dataset: Messy Sales Data\n",
        "\n",
        "Let's start with a realistic messy dataset that needs intelligent processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample messy sales data\n",
        "messy_data = [\n",
        "    {\"date\": \"2024-01-15\", \"customer\": \"ACME Corp\", \"product\": \"Widget A\", \"amount\": \"$1,250.00\", \"region\": \"North\", \"sales_rep\": \"John Smith\"},\n",
        "    {\"date\": \"01/16/2024\", \"customer\": \"acme corp\", \"product\": \"widget-a\", \"amount\": \"1250\", \"region\": \"NORTH\", \"sales_rep\": \"J. Smith\"},\n",
        "    {\"date\": \"2024-1-17\", \"customer\": \"Beta Inc.\", \"product\": \"Widget B\", \"amount\": \"$2,500.50\", \"region\": \"South\", \"sales_rep\": \"Sarah Johnson\"},\n",
        "    {\"date\": \"invalid_date\", \"customer\": \"Gamma LLC\", \"product\": \"Widget C\", \"amount\": \"ERROR\", \"region\": \"East\", \"sales_rep\": \"Mike Davis\"},\n",
        "    {\"date\": \"2024-01-19\", \"customer\": \"Delta Systems\", \"product\": \"Widget A\", \"amount\": \"$999.99\", \"region\": \"West\", \"sales_rep\": \"Lisa Brown\"},\n",
        "    {\"date\": \"2024/01/20\", \"customer\": \"EPSILON TECH\", \"product\": \"widget_b\", \"amount\": \"3000.00\", \"region\": \"north\", \"sales_rep\": \"Tom Wilson\"},\n",
        "    {\"date\": \"2024-01-21\", \"customer\": \"\", \"product\": \"Widget D\", \"amount\": \"$0.00\", \"region\": \"Central\", \"sales_rep\": \"\"}\n",
        "]\n",
        "\n",
        "# Convert to DataFrame for display\n",
        "df_raw = pd.DataFrame(messy_data)\n",
        "print(\"Raw Messy Sales Data:\")\n",
        "print(df_raw.to_string(index=False))\n",
        "print(f\"\\nDataset Issues:\")\n",
        "print(f\"- Inconsistent date formats\")\n",
        "print(f\"- Mixed case customer names\")\n",
        "print(f\"- Inconsistent product naming\")\n",
        "print(f\"- Various amount formats\")\n",
        "print(f\"- Missing/empty values\")\n",
        "print(f\"- Invalid data entries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: AI Data Analysis\n",
        "\n",
        "First, let's have AI analyze the data structure and identify issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI analyzes the raw data\n",
        "analysis_prompt = f\"\"\"\n",
        "Analyze this sales dataset and identify data quality issues, patterns, and cleaning requirements.\n",
        "\n",
        "Raw Data:\n",
        "{json.dumps(messy_data, indent=2)}\n",
        "\n",
        "Provide analysis in JSON format:\n",
        "{{\n",
        "  \"data_summary\": {{\n",
        "    \"total_records\": \"number\",\n",
        "    \"columns\": [\"list of columns\"],\n",
        "    \"date_range\": \"string\"\n",
        "  }},\n",
        "  \"quality_issues\": [\n",
        "    {{\n",
        "      \"column\": \"string\",\n",
        "      \"issue\": \"string\",\n",
        "      \"severity\": \"High|Medium|Low\",\n",
        "      \"affected_records\": \"number\"\n",
        "    }}\n",
        "  ],\n",
        "  \"cleaning_recommendations\": [\n",
        "    {{\n",
        "      \"action\": \"string\",\n",
        "      \"column\": \"string\",\n",
        "      \"method\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"business_insights\": [\"list of observations\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== AI DATA ANALYSIS ===\")\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "analysis_response = client.query(\n",
        "    message=analysis_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "analysis_result = analysis_response.get(\"message\").strip()\n",
        "print(analysis_result)\n",
        "\n",
        "# Parse the analysis\n",
        "import re\n",
        "json_match = re.search(r'\\{.*\\}', analysis_result, re.DOTALL)\n",
        "if json_match:\n",
        "    analysis_data = json.loads(json_match.group())\n",
        "    print(\"\\n✓ AI successfully analyzed the dataset\")\n",
        "    print(f\"✓ Identified {len(analysis_data.get('quality_issues', []))} quality issues\")\n",
        "    print(f\"✓ Generated {len(analysis_data.get('cleaning_recommendations', []))} cleaning recommendations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: AI Data Cleaning\n",
        "\n",
        "Now let's have AI generate a cleaning strategy and clean the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI generates cleaning transformations\n",
        "cleaning_prompt = f\"\"\"\n",
        "Based on the messy sales data, generate Python code to clean and standardize it.\n",
        "\n",
        "Raw Data:\n",
        "{json.dumps(messy_data, indent=2)}\n",
        "\n",
        "Requirements:\n",
        "1. Standardize date formats to YYYY-MM-DD\n",
        "2. Normalize customer names (title case, consistent)\n",
        "3. Standardize product names\n",
        "4. Convert amounts to float values\n",
        "5. Standardize region names\n",
        "6. Handle missing/invalid data\n",
        "\n",
        "Provide the cleaned data in JSON format with the same structure.\n",
        "Also include a summary of transformations applied.\n",
        "\n",
        "Format:\n",
        "{{\n",
        "  \"cleaned_data\": [cleaned records],\n",
        "  \"transformations\": [\n",
        "    {{\n",
        "      \"field\": \"string\",\n",
        "      \"action\": \"string\",\n",
        "      \"records_affected\": \"number\"\n",
        "    }}\n",
        "  ],\n",
        "  \"data_quality_score\": \"number 1-10\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== AI DATA CLEANING ===\")\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "cleaning_response = client.query(\n",
        "    message=cleaning_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "cleaning_result = cleaning_response.get(\"message\").strip()\n",
        "print(cleaning_result[:1000] + \"...\" if len(cleaning_result) > 1000 else cleaning_result)\n",
        "\n",
        "# Parse cleaned data\n",
        "json_match = re.search(r'\\{.*\\}', cleaning_result, re.DOTALL)\n",
        "if json_match:\n",
        "    cleaned_response = json.loads(json_match.group())\n",
        "    cleaned_data = cleaned_response.get('cleaned_data', [])\n",
        "    transformations = cleaned_response.get('transformations', [])\n",
        "    quality_score = cleaned_response.get('data_quality_score', 0)\n",
        "    \n",
        "    print(f\"\\n✓ Data cleaning completed\")\n",
        "    print(f\"✓ Quality score improved to: {quality_score}/10\")\n",
        "    print(f\"✓ Applied {len(transformations)} transformations\")\n",
        "    \n",
        "    # Display cleaned data\n",
        "    df_clean = pd.DataFrame(cleaned_data)\n",
        "    print(\"\\nCleaned Data:\")\n",
        "    print(df_clean.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: AI Data Validation\n",
        "\n",
        "Let's have AI validate the cleaned data and detect any remaining issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI validates the cleaned data\n",
        "validation_prompt = f\"\"\"\n",
        "Validate this cleaned sales dataset for business rules and data quality.\n",
        "\n",
        "Cleaned Data:\n",
        "{json.dumps(cleaned_data, indent=2)}\n",
        "\n",
        "Validation Rules:\n",
        "- Dates should be valid and recent\n",
        "- Customer names should not be empty\n",
        "- Product names should follow standard format\n",
        "- Amounts should be positive numbers\n",
        "- Regions should be valid business regions\n",
        "- Sales reps should be assigned\n",
        "\n",
        "Provide validation results:\n",
        "{{\n",
        "  \"validation_summary\": {{\n",
        "    \"total_records\": \"number\",\n",
        "    \"valid_records\": \"number\",\n",
        "    \"invalid_records\": \"number\",\n",
        "    \"overall_quality\": \"Excellent|Good|Fair|Poor\"\n",
        "  }},\n",
        "  \"validation_issues\": [\n",
        "    {{\n",
        "      \"record_index\": \"number\",\n",
        "      \"field\": \"string\",\n",
        "      \"issue\": \"string\",\n",
        "      \"severity\": \"Critical|Warning|Info\"\n",
        "    }}\n",
        "  ],\n",
        "  \"business_metrics\": {{\n",
        "    \"total_sales\": \"number\",\n",
        "    \"average_deal_size\": \"number\",\n",
        "    \"top_region\": \"string\",\n",
        "    \"date_range\": \"string\"\n",
        "  }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== AI DATA VALIDATION ===\")\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "validation_response = client.query(\n",
        "    message=validation_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.1,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "validation_result = validation_response.get(\"message\").strip()\n",
        "print(validation_result)\n",
        "\n",
        "# Parse validation results\n",
        "json_match = re.search(r'\\{.*\\}', validation_result, re.DOTALL)\n",
        "if json_match:\n",
        "    validation_data = json.loads(json_match.group())\n",
        "    summary = validation_data.get('validation_summary', {})\n",
        "    issues = validation_data.get('validation_issues', [])\n",
        "    metrics = validation_data.get('business_metrics', {})\n",
        "    \n",
        "    print(f\"\\n✓ Validation completed\")\n",
        "    print(f\"✓ Data quality: {summary.get('overall_quality', 'Unknown')}\")\n",
        "    print(f\"✓ Valid records: {summary.get('valid_records', 0)}/{summary.get('total_records', 0)}\")\n",
        "    print(f\"✓ Issues found: {len(issues)}\")\n",
        "    \n",
        "    if metrics:\n",
        "        print(f\"\\nBusiness Metrics:\")\n",
        "        print(f\"- Total Sales: ${metrics.get('total_sales', 0):,.2f}\")\n",
        "        print(f\"- Average Deal: ${metrics.get('average_deal_size', 0):,.2f}\")\n",
        "        print(f\"- Top Region: {metrics.get('top_region', 'Unknown')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: AI Business Intelligence\n",
        "\n",
        "Finally, let's have AI generate business insights and recommendations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI generates business insights\n",
        "insights_prompt = f\"\"\"\n",
        "Analyze this cleaned sales data and generate business insights and recommendations.\n",
        "\n",
        "Sales Data:\n",
        "{json.dumps(cleaned_data, indent=2)}\n",
        "\n",
        "Generate comprehensive business intelligence:\n",
        "{{\n",
        "  \"executive_summary\": \"string\",\n",
        "  \"key_insights\": [\n",
        "    {{\n",
        "      \"insight\": \"string\",\n",
        "      \"impact\": \"High|Medium|Low\",\n",
        "      \"data_supporting\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"performance_analysis\": {{\n",
        "    \"top_performers\": [\"list\"],\n",
        "    \"growth_opportunities\": [\"list\"],\n",
        "    \"risk_areas\": [\"list\"]\n",
        "  }},\n",
        "  \"recommendations\": [\n",
        "    {{\n",
        "      \"action\": \"string\",\n",
        "      \"priority\": \"High|Medium|Low\",\n",
        "      \"expected_impact\": \"string\",\n",
        "      \"timeline\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"next_steps\": [\"list of immediate actions\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== AI BUSINESS INTELLIGENCE ===\")\n",
        "# Test GPT-5-mini\n",
        "print(\"=== TESTING GPT-5-mini ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "insights_response = client.query(\n",
        "    message=insights_prompt,\n",
        "    system_prompt=\"You are concise.\",\n",
        "    temperature=0.2,\n",
        "    model=\"gpt-5-mini\",\n",
        "    live=0,\n",
        "    limit_references=0,\n",
        ")\n",
        "\n",
        "\n",
        "insights_result = insights_response.get(\"message\").strip()\n",
        "print(insights_result)\n",
        "\n",
        "# Parse insights\n",
        "json_match = re.search(r'\\{.*\\}', insights_result, re.DOTALL)\n",
        "if json_match:\n",
        "    insights_data = json.loads(json_match.group())\n",
        "    \n",
        "    print(f\"\\n✓ Business intelligence generated\")\n",
        "    print(f\"✓ Key insights: {len(insights_data.get('key_insights', []))}\")\n",
        "    print(f\"✓ Recommendations: {len(insights_data.get('recommendations', []))}\")\n",
        "    print(f\"✓ Next steps: {len(insights_data.get('next_steps', []))}\")\n",
        "    \n",
        "    # Show high-priority recommendations\n",
        "    high_priority = [r for r in insights_data.get('recommendations', []) if r.get('priority') == 'High']\n",
        "    if high_priority:\n",
        "        print(f\"\\nHigh-Priority Actions:\")\n",
        "        for rec in high_priority:\n",
        "            print(f\"- {rec.get('action', 'Unknown action')}\")\n",
        "            print(f\"  Impact: {rec.get('expected_impact', 'Not specified')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Summary: The AI Transformation\n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "**1. Intelligent Data Analysis**\n",
        "- AI automatically identified data quality issues\n",
        "- Recognized patterns and inconsistencies\n",
        "- Generated cleaning recommendations\n",
        "\n",
        "**2. Automated Data Cleaning**\n",
        "- Standardized date formats across multiple variations\n",
        "- Normalized customer and product names\n",
        "- Converted currency strings to numeric values\n",
        "- Handled missing and invalid data intelligently\n",
        "\n",
        "**3. Comprehensive Validation**\n",
        "- Applied business rules automatically\n",
        "- Calculated data quality scores\n",
        "- Generated business metrics\n",
        "\n",
        "**4. Business Intelligence Generation**\n",
        "- Extracted actionable insights from clean data\n",
        "- Identified performance patterns and opportunities\n",
        "- Generated prioritized recommendations\n",
        "\n",
        "### Business Value:\n",
        "- **Time Savings**: Hours of manual work → Minutes of AI processing\n",
        "- **Consistency**: Human variance → Standardized quality\n",
        "- **Scalability**: Handle datasets of any size\n",
        "- **Intelligence**: Not just cleaning, but understanding and insights\n",
        "\n",
        "### Traditional vs AI Approach:\n",
        "- **Traditional**: Manual rules, static processes, limited insights\n",
        "- **AI-Powered**: Adaptive intelligence, contextual understanding, business insights\n",
        "\n",
        "This demonstrates how AI transforms data pipelines from simple ETL to intelligent, self-adapting business intelligence systems."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}