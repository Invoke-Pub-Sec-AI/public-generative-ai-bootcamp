{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-colab-cell",
        "colab": {
          "base_uri": "https://localhost/"
        }
      },
      "source": "print('Setup complete.')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI System Monitoring & Observability\n",
        "\n",
        "## Learning Objectives\n",
        "- See real-time AI system monitoring in action\n",
        "- Watch intelligent alerting and anomaly detection\n",
        "- Understand AI performance metrics and optimization\n",
        "- Learn about cost monitoring and resource management\n",
        "\n",
        "## The Demo: Intelligent AI Operations\n",
        "\n",
        "We'll demonstrate:\n",
        "1. **Real-time Monitoring** - Track AI system performance\n",
        "2. **Intelligent Alerting** - AI-powered anomaly detection\n",
        "3. **Cost Optimization** - Monitor and optimize AI spending\n",
        "4. **Performance Analytics** - Understand usage patterns\n",
        "5. **Predictive Scaling** - AI-driven resource management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Any\n",
        "import pandas as pd\n",
        "\n",
        "# Import our AskSage client\n",
        "import sys\n",
        "sys.path.append('../../../bootcamp_common')\n",
        "from ask_sage import AskSageClient\n",
        "\n",
        "# Initialize client\n",
        "client = AskSageClient()\n",
        "print(\"AskSage client initialized successfully\")\n",
        "print(\"Ready to demonstrate AI system monitoring...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulated AI System Metrics\n",
        "\n",
        "Let's create realistic AI system performance data to monitor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate realistic AI system metrics\n",
        "def generate_ai_metrics(hours=24):\n",
        "    \"\"\"Generate realistic AI system metrics for monitoring demo\"\"\"\n",
        "    metrics = []\n",
        "    base_time = datetime.now() - timedelta(hours=hours)\n",
        "    \n",
        "    for i in range(hours * 4):  # 15-minute intervals\n",
        "        timestamp = base_time + timedelta(minutes=i * 15)\n",
        "        \n",
        "        # Simulate realistic patterns with some anomalies\n",
        "        hour = timestamp.hour\n",
        "        is_business_hours = 9 <= hour <= 17\n",
        "        is_anomaly = random.random() < 0.05  # 5% chance of anomaly\n",
        "        \n",
        "        # Base metrics with realistic patterns\n",
        "        base_requests = 100 if is_business_hours else 20\n",
        "        base_latency = 200 if is_business_hours else 150\n",
        "        base_cost = 0.05 if is_business_hours else 0.01\n",
        "        \n",
        "        # Add noise and anomalies\n",
        "        requests_per_min = base_requests + random.randint(-20, 20)\n",
        "        avg_latency_ms = base_latency + random.randint(-50, 50)\n",
        "        cost_per_request = base_cost + random.uniform(-0.01, 0.01)\n",
        "        \n",
        "        if is_anomaly:\n",
        "            requests_per_min *= random.uniform(0.1, 3.0)  # Spike or drop\n",
        "            avg_latency_ms *= random.uniform(1.5, 5.0)   # Latency spike\n",
        "            cost_per_request *= random.uniform(2.0, 4.0) # Cost spike\n",
        "        \n",
        "        # Calculate derived metrics\n",
        "        success_rate = random.uniform(0.95, 1.0) if not is_anomaly else random.uniform(0.7, 0.95)\n",
        "        token_usage = requests_per_min * random.randint(50, 500)\n",
        "        total_cost = requests_per_min * cost_per_request\n",
        "        \n",
        "        metrics.append({\n",
        "            \"timestamp\": timestamp.isoformat(),\n",
        "            \"requests_per_minute\": max(0, int(requests_per_min)),\n",
        "            \"avg_latency_ms\": max(50, int(avg_latency_ms)),\n",
        "            \"success_rate\": round(success_rate, 3),\n",
        "            \"token_usage\": int(token_usage),\n",
        "            \"cost_per_request\": round(cost_per_request, 4),\n",
        "            \"total_cost_usd\": round(total_cost, 2),\n",
        "            \"is_anomaly\": is_anomaly,\n",
        "            \"business_hours\": is_business_hours\n",
        "        })\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Generate 24 hours of metrics\n",
        "ai_metrics = generate_ai_metrics(24)\n",
        "print(f\"Generated {len(ai_metrics)} metric data points\")\n",
        "print(f\"Time range: {ai_metrics[0]['timestamp']} to {ai_metrics[-1]['timestamp']}\")\n",
        "print(f\"Anomalies included: {sum(1 for m in ai_metrics if m['is_anomaly'])}\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\nSample metrics:\")\n",
        "df_sample = pd.DataFrame(ai_metrics[-5:])\n",
        "print(df_sample[['timestamp', 'requests_per_minute', 'avg_latency_ms', 'success_rate', 'total_cost_usd']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AI-Powered Anomaly Detection\n",
        "\n",
        "Let's have AI analyze the metrics and detect anomalies automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI analyzes metrics for anomalies\n",
        "recent_metrics = ai_metrics[-20:]  # Last 5 hours\n",
        "\n",
        "anomaly_prompt = f\"\"\"\n",
        "Analyze these AI system metrics and detect anomalies, performance issues, and optimization opportunities.\n",
        "\n",
        "Metrics Data (last 5 hours):\n",
        "{json.dumps(recent_metrics, indent=2)}\n",
        "\n",
        "Provide analysis in JSON format:\n",
        "{{\n",
        "  \"system_health\": {{\n",
        "    \"overall_status\": \"Healthy|Warning|Critical\",\n",
        "    \"health_score\": \"number 1-10\",\n",
        "    \"summary\": \"string\"\n",
        "  }},\n",
        "  \"anomalies_detected\": [\n",
        "    {{\n",
        "      \"timestamp\": \"string\",\n",
        "      \"metric\": \"string\",\n",
        "      \"anomaly_type\": \"Spike|Drop|Trend|Outlier\",\n",
        "      \"severity\": \"High|Medium|Low\",\n",
        "      \"description\": \"string\",\n",
        "      \"impact\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"performance_insights\": [\n",
        "    {{\n",
        "      \"metric\": \"string\",\n",
        "      \"trend\": \"Improving|Stable|Degrading\",\n",
        "      \"insight\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"cost_analysis\": {{\n",
        "    \"total_cost_period\": \"number\",\n",
        "    \"cost_trend\": \"string\",\n",
        "    \"optimization_potential\": \"string\"\n",
        "  }},\n",
        "  \"alerts\": [\n",
        "    {{\n",
        "      \"priority\": \"Critical|High|Medium|Low\",\n",
        "      \"message\": \"string\",\n",
        "      \"action_required\": \"string\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== AI ANOMALY DETECTION ===\")\n",
        "anomaly_response = client.query({\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": anomaly_prompt}],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 1500\n",
        "})\n",
        "\n",
        "anomaly_result = anomaly_response['choices'][0]['message']['content']\n",
        "print(anomaly_result)\n",
        "\n",
        "# Parse anomaly analysis\n",
        "import re\n",
        "json_match = re.search(r'\\{.*\\}', anomaly_result, re.DOTALL)\n",
        "if json_match:\n",
        "    anomaly_data = json.loads(json_match.group())\n",
        "    \n",
        "    health = anomaly_data.get('system_health', {})\n",
        "    anomalies = anomaly_data.get('anomalies_detected', [])\n",
        "    alerts = anomaly_data.get('alerts', [])\n",
        "    \n",
        "    print(f\"\\nâœ“ System Health: {health.get('overall_status', 'Unknown')}\")\n",
        "    print(f\"âœ“ Health Score: {health.get('health_score', 0)}/10\")\n",
        "    print(f\"âœ“ Anomalies Detected: {len(anomalies)}\")\n",
        "    print(f\"âœ“ Active Alerts: {len(alerts)}\")\n",
        "    \n",
        "    # Show critical alerts\n",
        "    critical_alerts = [a for a in alerts if a.get('priority') == 'Critical']\n",
        "    if critical_alerts:\n",
        "        print(f\"\\nðŸš¨ CRITICAL ALERTS:\")\n",
        "        for alert in critical_alerts:\n",
        "            print(f\"  - {alert.get('message', 'Unknown alert')}\")\n",
        "            print(f\"    Action: {alert.get('action_required', 'No action specified')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intelligent Cost Optimization\n",
        "\n",
        "Let's have AI analyze costs and suggest optimizations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI analyzes costs and suggests optimizations\n",
        "cost_optimization_prompt = f\"\"\"\n",
        "Analyze AI system costs and provide optimization recommendations.\n",
        "\n",
        "24-Hour Metrics Summary:\n",
        "- Total requests: {sum(m['requests_per_minute'] for m in ai_metrics)}\n",
        "- Total cost: ${sum(m['total_cost_usd'] for m in ai_metrics):.2f}\n",
        "- Average latency: {sum(m['avg_latency_ms'] for m in ai_metrics) / len(ai_metrics):.1f}ms\n",
        "- Average success rate: {sum(m['success_rate'] for m in ai_metrics) / len(ai_metrics):.3f}\n",
        "- Total tokens: {sum(m['token_usage'] for m in ai_metrics):,}\n",
        "\n",
        "Business Hours vs Off-Hours:\n",
        "Business Hours Metrics:\n",
        "{json.dumps([m for m in ai_metrics if m['business_hours']][-5:], indent=2)}\n",
        "\n",
        "Off-Hours Metrics:\n",
        "{json.dumps([m for m in ai_metrics if not m['business_hours']][-5:], indent=2)}\n",
        "\n",
        "Provide cost optimization analysis:\n",
        "{{\n",
        "  \"cost_breakdown\": {{\n",
        "    \"total_daily_cost\": \"number\",\n",
        "    \"business_hours_cost\": \"number\",\n",
        "    \"off_hours_cost\": \"number\",\n",
        "    \"cost_per_successful_request\": \"number\"\n",
        "  }},\n",
        "  \"usage_patterns\": [\n",
        "    {{\n",
        "      \"pattern\": \"string\",\n",
        "      \"cost_impact\": \"High|Medium|Low\",\n",
        "      \"description\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"optimization_opportunities\": [\n",
        "    {{\n",
        "      \"strategy\": \"string\",\n",
        "      \"potential_savings\": \"string\",\n",
        "      \"implementation_effort\": \"Low|Medium|High\",\n",
        "      \"risk_level\": \"Low|Medium|High\",\n",
        "      \"description\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"recommendations\": [\n",
        "    {{\n",
        "      \"action\": \"string\",\n",
        "      \"priority\": \"High|Medium|Low\",\n",
        "      \"expected_savings\": \"string\",\n",
        "      \"timeline\": \"string\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== AI COST OPTIMIZATION ===\")\n",
        "cost_response = client.query({\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": cost_optimization_prompt}],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 1500\n",
        "})\n",
        "\n",
        "cost_result = cost_response['choices'][0]['message']['content']\n",
        "print(cost_result)\n",
        "\n",
        "# Parse cost analysis\n",
        "json_match = re.search(r'\\{.*\\}', cost_result, re.DOTALL)\n",
        "if json_match:\n",
        "    cost_data = json.loads(json_match.group())\n",
        "    \n",
        "    breakdown = cost_data.get('cost_breakdown', {})\n",
        "    opportunities = cost_data.get('optimization_opportunities', [])\n",
        "    recommendations = cost_data.get('recommendations', [])\n",
        "    \n",
        "    print(f\"\\nâœ“ Daily Cost Analysis: ${breakdown.get('total_daily_cost', 0):.2f}\")\n",
        "    print(f\"âœ“ Optimization Opportunities: {len(opportunities)}\")\n",
        "    print(f\"âœ“ Actionable Recommendations: {len(recommendations)}\")\n",
        "    \n",
        "    # Show high-priority recommendations\n",
        "    high_priority = [r for r in recommendations if r.get('priority') == 'High']\n",
        "    if high_priority:\n",
        "        print(f\"\\nðŸ’° HIGH-PRIORITY COST OPTIMIZATIONS:\")\n",
        "        for rec in high_priority:\n",
        "            print(f\"  - {rec.get('action', 'Unknown action')}\")\n",
        "            print(f\"    Savings: {rec.get('expected_savings', 'Not specified')}\")\n",
        "            print(f\"    Timeline: {rec.get('timeline', 'Not specified')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictive Scaling Analysis\n",
        "\n",
        "Let's have AI predict future resource needs and scaling requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI predicts scaling needs\n",
        "scaling_prompt = f\"\"\"\n",
        "Based on the AI system metrics, predict future scaling needs and resource requirements.\n",
        "\n",
        "Current Performance Trends:\n",
        "- Peak requests/min: {max(m['requests_per_minute'] for m in ai_metrics)}\n",
        "- Average requests/min: {sum(m['requests_per_minute'] for m in ai_metrics) / len(ai_metrics):.1f}\n",
        "- Peak latency: {max(m['avg_latency_ms'] for m in ai_metrics)}ms\n",
        "- Success rate range: {min(m['success_rate'] for m in ai_metrics):.3f} - {max(m['success_rate'] for m in ai_metrics):.3f}\n",
        "\n",
        "Recent 6-hour trend:\n",
        "{json.dumps(ai_metrics[-24:], indent=2)}\n",
        "\n",
        "Provide scaling analysis:\n",
        "{{\n",
        "  \"current_capacity\": {{\n",
        "    \"utilization_level\": \"Low|Medium|High|Critical\",\n",
        "    \"capacity_percentage\": \"number\",\n",
        "    \"bottlenecks\": [\"list of bottlenecks\"]\n",
        "  }},\n",
        "  \"growth_predictions\": {{\n",
        "    \"next_7_days\": \"string\",\n",
        "    \"next_30_days\": \"string\",\n",
        "    \"growth_rate\": \"string\"\n",
        "  }},\n",
        "  \"scaling_recommendations\": [\n",
        "    {{\n",
        "      \"timeframe\": \"Immediate|Short-term|Long-term\",\n",
        "      \"action\": \"string\",\n",
        "      \"resource_type\": \"Compute|Memory|Network|Storage\",\n",
        "      \"justification\": \"string\",\n",
        "      \"estimated_cost\": \"string\"\n",
        "    }}\n",
        "  ],\n",
        "  \"risk_assessment\": [\n",
        "    {{\n",
        "      \"risk\": \"string\",\n",
        "      \"probability\": \"High|Medium|Low\",\n",
        "      \"impact\": \"High|Medium|Low\",\n",
        "      \"mitigation\": \"string\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== AI PREDICTIVE SCALING ===\")\n",
        "scaling_response = client.query({\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": scaling_prompt}],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 1500\n",
        "})\n",
        "\n",
        "scaling_result = scaling_response['choices'][0]['message']['content']\n",
        "print(scaling_result)\n",
        "\n",
        "# Parse scaling analysis\n",
        "json_match = re.search(r'\\{.*\\}', scaling_result, re.DOTALL)\n",
        "if json_match:\n",
        "    scaling_data = json.loads(json_match.group())\n",
        "    \n",
        "    capacity = scaling_data.get('current_capacity', {})\n",
        "    predictions = scaling_data.get('growth_predictions', {})\n",
        "    scaling_recs = scaling_data.get('scaling_recommendations', [])\n",
        "    risks = scaling_data.get('risk_assessment', [])\n",
        "    \n",
        "    print(f\"\\nâœ“ Current Utilization: {capacity.get('utilization_level', 'Unknown')}\")\n",
        "    print(f\"âœ“ Capacity Usage: {capacity.get('capacity_percentage', 0)}%\")\n",
        "    print(f\"âœ“ Scaling Actions: {len(scaling_recs)}\")\n",
        "    print(f\"âœ“ Risk Factors: {len(risks)}\")\n",
        "    \n",
        "    # Show immediate actions\n",
        "    immediate_actions = [r for r in scaling_recs if r.get('timeframe') == 'Immediate']\n",
        "    if immediate_actions:\n",
        "        print(f\"\\nâš¡ IMMEDIATE SCALING ACTIONS:\")\n",
        "        for action in immediate_actions:\n",
        "            print(f\"  - {action.get('action', 'Unknown action')}\")\n",
        "            print(f\"    Resource: {action.get('resource_type', 'Not specified')}\")\n",
        "            print(f\"    Cost: {action.get('estimated_cost', 'Not specified')}\")\n",
        "    \n",
        "    # Show high-risk items\n",
        "    high_risks = [r for r in risks if r.get('probability') == 'High' or r.get('impact') == 'High']\n",
        "    if high_risks:\n",
        "        print(f\"\\nâš ï¸ HIGH-RISK FACTORS:\")\n",
        "        for risk in high_risks:\n",
        "            print(f\"  - {risk.get('risk', 'Unknown risk')}\")\n",
        "            print(f\"    Probability: {risk.get('probability', 'Unknown')} | Impact: {risk.get('impact', 'Unknown')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitoring Dashboard Summary\n",
        "\n",
        "### What We Demonstrated:\n",
        "\n",
        "**1. Intelligent Anomaly Detection**\n",
        "- AI automatically identified performance anomalies\n",
        "- Classified anomaly types and severity levels\n",
        "- Generated actionable alerts with context\n",
        "\n",
        "**2. Cost Intelligence**\n",
        "- Analyzed spending patterns across business hours\n",
        "- Identified optimization opportunities\n",
        "- Provided specific cost-saving recommendations\n",
        "\n",
        "**3. Predictive Scaling**\n",
        "- Analyzed current capacity utilization\n",
        "- Predicted future resource needs\n",
        "- Recommended proactive scaling actions\n",
        "\n",
        "**4. Risk Assessment**\n",
        "- Identified potential system risks\n",
        "- Assessed probability and impact\n",
        "- Suggested mitigation strategies\n",
        "\n",
        "### Business Value:\n",
        "\n",
        "**Proactive Operations**\n",
        "- Prevent issues before they impact users\n",
        "- Optimize costs automatically\n",
        "- Scale resources predictively\n",
        "\n",
        "**Intelligent Insights**\n",
        "- Context-aware alerting reduces noise\n",
        "- Business-impact focused recommendations\n",
        "- Data-driven decision making\n",
        "\n",
        "**Operational Efficiency**\n",
        "- Reduce manual monitoring overhead\n",
        "- Faster incident response\n",
        "- Optimized resource utilization\n",
        "\n",
        "### Traditional vs AI Monitoring:\n",
        "\n",
        "**Traditional Monitoring:**\n",
        "- Static thresholds and rules\n",
        "- Reactive alerting\n",
        "- Manual analysis required\n",
        "- Limited context understanding\n",
        "\n",
        "**AI-Powered Monitoring:**\n",
        "- Dynamic anomaly detection\n",
        "- Predictive insights\n",
        "- Automated root cause analysis\n",
        "- Business context awareness\n",
        "\n",
        "This demonstrates how AI transforms system monitoring from reactive alerting to proactive, intelligent operations management."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}